{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "#from obspy import Stream, Trace, UTCDateTime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Activation, Conv1D, Dense, Dropout, Flatten, MaxPooling3D, BatchNormalization, InputLayer, LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "classes_dir = 'D:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(classes_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "from Classes import Tf_shutup\n",
    "Tf_shutup.Tf_shutup()\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "base_dir = 'D:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-rating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print(f'Found GPU at: {device_name}')\n",
    "\n",
    "#tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reasonable-problem",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "### Data conditions: ###\n",
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.3,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.3,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "\n",
    "loadData = LoadData(**load_args)\n",
    "\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "dataGen = DataGenerator(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "absolute-daisy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25390 5078 3386\n",
      "All data:\n",
      "Total: 33854, earthquake: 11311, explosion: 10968, noise: 11575\n",
      "Train set:\n",
      "Total: 25390, earthquake: 8483, explosion: 8177, noise: 8730\n",
      "Validation set:\n",
      "Total: 5078, earthquake: 1697, explosion: 1684, noise: 1697\n",
      "Test set:\n",
      "Total: 3386, earthquake: 1131, explosion: 1107, noise: 1148\n",
      "Nr noise samples 8730\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "print(\"All data:\")\n",
    "classes, counts = handler.get_class_distribution_from_ds(full_ds)\n",
    "print(\"Train set:\")\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "print(\"Validation set:\")\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Test set:\")\n",
    "classes, counts = handler.get_class_distribution_from_ds(test_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "several-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Model picker #############\n",
    "model_nr_type = \"CNN\"\n",
    "is_lstm = False\n",
    "num_layers = 2\n",
    "decay_sequence = [1,1]\n",
    "use_layerwise_dropout_batchnorm = True\n",
    "\n",
    "########### Hyperparameters ###########\n",
    "batch_size = 512\n",
    "epochs = 30\n",
    "learning_rate = 0.05\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=1.0, clipvalue=0.5)\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "activation = 'tanh'\n",
    "output_layer_activation = 'sigmoid'\n",
    "dropout_rate = 0.3\n",
    "filters = 17\n",
    "kernel_size = 5\n",
    "l1_r = 0.00001\n",
    "l2_r = 0.0001\n",
    "padding = 'same'\n",
    "start_neurons = 64\n",
    "num_channels = 3\n",
    "\n",
    "########### Preprocessing ###########\n",
    "use_noise_augmentor = True\n",
    "use_time_augmentor = True\n",
    "detrend = False\n",
    "use_scaler = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.2\n",
    "\n",
    "use_tensorboard = False\n",
    "use_livelossplot = True\n",
    "use_custom = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "silver-fleet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit process completed after 464.31471133232117 seconds. Total datapoints fitted: 33854.\n",
      "Average time per datapoint: 0.013715209763464322\n",
      "Fitting noise progress: [------------------> ] 99 %%\r"
     ]
    }
   ],
   "source": [
    "scaler = None\n",
    "noiseAug = None\n",
    "if use_time_augmentor:\n",
    "    timeAug.fit()\n",
    "if use_scaler:\n",
    "    scaler = StandardScalerFitter(train_ds, timeAug).fit_scaler(detrend = detrend)\n",
    "if use_noise_augmentor:\n",
    "    noiseAug = NoiseAugmentor(train_ds, use_scaler, scaler, loadData, timeAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "royal-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RamLoader:\n",
    "    def __init__(self,handler, timeAug = None, scaler = None, noiseAug = None):\n",
    "        self.handler = handler\n",
    "        self.timeAug = timeAug\n",
    "        self.noiseAug = noiseAug\n",
    "        self.use_time_augmentor = False\n",
    "        self.num_classes = len(set(handler.loadData.label_dict.values()))\n",
    "        if self.timeAug != None:\n",
    "            self.use_time_augmentor = True\n",
    "    \n",
    "    def load_to_ram(self, ds, is_lstm):\n",
    "        loaded_label = np.empty((len(ds), 1))\n",
    "        loaded_trace = np.empty((self.handler.get_trace_shape_no_cast(ds, self.use_time_augmentor)))\n",
    "        print(\"Starting loading\")\n",
    "        if timeAug != None and scaler != None:\n",
    "            for i in range(len(ds)):\n",
    "                loaded_trace[i] = timeAug.augment_event(ds[i][0], ds[i][2])\n",
    "                loaded_trace[i] = scaler.transform(loaded_trace[i])\n",
    "                loaded_label[i] = self.handler.label_dict.get(ds[i][1])\n",
    "        elif timeAug != None:\n",
    "            for i in range(len(ds)):\n",
    "                loaded_trace[i] = timeAug.augment_event(ds[i][0], ds[i][2])\n",
    "                loaded_label[i] = self.handler.label_dict.get(ds[i][1])\n",
    "        elif scaler != None:\n",
    "            for i in range(len(ds)):\n",
    "                loaded_trace[i] = self.handler.path_to_trace(ds[i][0])\n",
    "                loaded_trace[i] = scaler.transform(loaded_trace[i])\n",
    "                loaded_label[i] = self.handler.label_dict.get(ds[i][1])\n",
    "        else:\n",
    "            for i in range(len(ds)):\n",
    "                loaded_trace[i] = self.handler.path_to_trace(ds[i][0])\n",
    "                loaded_label[i] = self.handler.label_dict.get(ds[i][1])\n",
    "        if noiseAug != None:\n",
    "            loaded_trace = self.noiseAug.batch_augment_noise(loaded_trace, 0, self.noiseAug.noise_std/10)\n",
    "        \n",
    "        if is_lstm:\n",
    "            loaded_trace = np.reshape(loaded_trace, (loaded_trace.shape[0], loaded_trace.shape[2], loaded_trace.shape[1]))\n",
    "        loaded_label = np_utils.to_categorical(loaded_label, self.num_classes, dtype=np.int8)\n",
    "        if self.num_classes == 2:\n",
    "            loaded_label = loaded_label[:,1]\n",
    "        return loaded_trace, loaded_label\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "insured-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramLoader = RamLoader(handler, timeAug, scaler, noiseAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weekly-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loading\n",
      "27.591693878173828\n",
      "Starting loading\n",
      "Starting loading\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "x_train, y_train = ramLoader.load_to_ram(train_ds, is_lstm)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "x_val, y_val = ramLoader.load_to_ram(val_ds, is_lstm)\n",
    "x_test, y_test = ramLoader.load_to_ram(test_ds, is_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "enhanced-relief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25390,), (5078,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "involved-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "\n",
    "callbacks = []\n",
    "if use_tensorboard:\n",
    "    import datetime\n",
    "    clear_tensorboard_dir()\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir tensorboard_dir/fit\n",
    "    log_dir = f\"{base_dir}/tensorboard_dir/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_ballback)\n",
    "\n",
    "if use_custom:\n",
    "    custom_callback = CustomCallback(data_gen)\n",
    "    callbacks.append(custom_callback)\n",
    "elif use_livelossplot:\n",
    "    callbacks.append(PlotLossesKeras())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "animated-duplicate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 3, 17)             510017    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3, 17)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3, 17)             68        \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 3, 17)             1462      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 17)             0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 3, 17)             68        \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 3, 17)             68        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 52        \n",
      "=================================================================\n",
      "Total params: 511,735\n",
      "Trainable params: 511,633\n",
      "Non-trainable params: 102\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_ds, channels, timesteps = dataGen.get_trace_shape_no_cast(train_ds, use_time_augmentor)\n",
    "input_shape = (batch_size, channels, timesteps)\n",
    "\n",
    "build_model_args ={\"model_type\" : model_nr_type,\n",
    "                    \"num_layers\": num_layers,\n",
    "                    \"input_shape\" : (channels, timesteps),\n",
    "                    \"num_classes\" : len(set(loadData.label_dict.values())),\n",
    "                    \"dropout_rate\" : dropout_rate,\n",
    "                    \"activation\" : activation,\n",
    "                    \"output_layer_activation\" : output_layer_activation,\n",
    "                    \"l2_r\" : l2_r,\n",
    "                    \"l1_r\" : l1_r,\n",
    "                    \"full_regularizer\" : True,\n",
    "                    \"start_neurons\" : start_neurons,\n",
    "                    \"decay_sequence\" : decay_sequence,\n",
    "                    \"filters\" : filters,\n",
    "                    \"kernel_size\" : kernel_size,\n",
    "                    \"padding\" : padding,\n",
    "                    \"use_layerwise_dropout_batchnorm\" : use_layerwise_dropout_batchnorm}\n",
    "model = DynamicModels(**build_model_args).model\n",
    "\n",
    "\n",
    "model_args = {'loss' : \"binary_crossentropy\",\n",
    "              'optimizer' : opt,\n",
    "              'metrics' : [tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None), \n",
    "                           tf.keras.metrics.BinaryAccuracy(name=\"binary_accuracy\", dtype=None, threshold=0.5),\n",
    "                           tf.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)]}\n",
    "                           \n",
    "\n",
    "model.compile(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25390 samples, validate on 5078 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = {'x' : x_train,\n",
    "        'y' : y_train,\n",
    "        'batch_size' : batch_size,\n",
    "        'epochs' : epochs,\n",
    "        'steps_per_epoch' : len(train_ds) // batch_size\n",
    "        'callbacks' : callbacks,\n",
    "        'validation_data' : (x_val, y_val),\n",
    "        'validation_steps' : len(val_ds) // batch_size\n",
    "        'shuffle' : True,\n",
    "        'use_multiprocessing' : False,\n",
    "        'workers' : 1}\n",
    "model_fit = model.fit(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RamGenerator(DataHandler):\n",
    "    \n",
    "    def __init__(self, loadData):\n",
    "        super().__init__(loadData)\n",
    "        self.num_classes = len(set(loadData.label_dict.values()))\n",
    "        \n",
    "    def data_generator(self, traces, labels, batch_size):\n",
    "        num_samples = len(labels)\n",
    "        while True:\n",
    "            for offset in range(0, num_samples, batch_size):\n",
    "                batch_traces = np.empty((batch_size, traces.shape[1], traces.shape[2]))\n",
    "                batch_labels = np.empty((batch_size, 1))\n",
    "                if offset + batch_size > num_samples:\n",
    "                    overflow = offset + batch_size - num_samples\n",
    "                    \n",
    "                    batch_traces[0:batch_size-overflow] = traces[offset:(offset+batch_size)-overflow]\n",
    "                    batch_labels[0:batch_size-overflow] = labels[offset:(offset+batch_size)-overflow]\n",
    "                    \n",
    "                    i_start = random.randint(0, num_samples-overflow)\n",
    "                    batch_traces[batch_size-overflow:batch_size] = traces[i_start:i_start+overflow]\n",
    "                    batch_labels[batch_size-overflow:batch_size] = labels[i_start:i_start+overflow]\n",
    "                else:\n",
    "                    batch_traces = traces[offset:offset + batch_size]\n",
    "                    batch_labels = labels[offset:offset+batch_size]\n",
    "                \n",
    "                yield batch_traces, batch_labels\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = RamGenerator(loadData)\n",
    "train_gen = gen.data_generator(x_train, y_train, batch_size)\n",
    "val_gen = gen.data_generator(x_val, y_val, batch_size)\n",
    "test_gen = gen.data_generator(x_test, y_test, batch_size)\n",
    "\n",
    "args = {'steps_per_epoch' : helper.get_steps_per_epoch(train_ds, batch_size),\n",
    "        'epochs' : epochs,\n",
    "        'validation_data' : val_gen,\n",
    "        'validation_steps' : helper.get_steps_per_epoch(val_ds, batch_size),\n",
    "        'verbose' : 1,\n",
    "        'use_multiprocessing' : True, \n",
    "        'workers' : 1,\n",
    "        'callbacks' : callbacks\n",
    "}\n",
    "\n",
    "model_fit = model.fit(train_gen, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-warrant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
