{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "base_dir = 'F:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.Modeling.Models import Models\n",
    "from Classes.Modeling.RandomGridSearch import RandomGridSearch\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "\n",
    "helper = HelperFunctions()\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [---------------------------->           ] 72 %\r"
     ]
    }
   ],
   "source": [
    "### Data conditions: ###\n",
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.4,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.3,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "\n",
    "loadData = LoadData(**load_args)\n",
    "\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "dataGen = DataGenerator(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(full_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_grid = {\n",
    "        \"batch_size\" : [64, 128, 256, 512],\n",
    "        \"epochs\" : [30, 33, 35],\n",
    "        \"learning_rate\" : [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "        \"optimizer\" : [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "    }\n",
    "model_grid = {\n",
    "    \"start_neurons\" : [16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"dropout_rate\" : [0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.001, 0],\n",
    "    \"filters\" : [11, 13, 15, 17, 19, 21, 23, 25, 27],\n",
    "    \"kernel_size\" : [3, 5, 7, 9, 11, 13, 15],\n",
    "    \"padding\" : [\"same\"],\n",
    "    \"l2_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"l1_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"activation\" : [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"output_layer_activation\" : [\"sigmoid\"]\n",
    "}\n",
    "\n",
    "\n",
    "model_nr = 8\n",
    "\n",
    "use_time_augmentor = True\n",
    "use_scaler = True\n",
    "use_noise_augmentor = True\n",
    "detrend = False\n",
    "use_minmax = False\n",
    "use_highpass = False\n",
    "highpass_freq = 0.1\n",
    "\n",
    "n_picks = 30\n",
    "\n",
    "use_tensorboard = False\n",
    "use_liveplots = True\n",
    "use_custom_callback = False\n",
    "use_early_stopping = True\n",
    "start_from_scratch = True\n",
    "\n",
    "randomGridSearch = RandomGridSearch(loadData, train_ds, val_ds, test_ds, model_nr, detrend, use_scaler, use_time_augmentor, \n",
    "                                    use_noise_augmentor, use_minmax, use_highpass, n_picks, hyper_grid = hyper_grid, \n",
    "                                    model_grid = model_grid, use_tensorboard = use_tensorboard,\n",
    "                                    use_liveplots = use_liveplots, use_custom_callback = use_custom_callback,\n",
    "                                    use_early_stopping = use_early_stopping, highpass_freq = highpass_freq,\n",
    "                                    start_from_scratch = start_from_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "    %tensorboard --logdir tensorboard_dir/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df, min_loss, max_accuracy, max_precision, max_recall = randomGridSearch.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_accuracy_i = max_accuracy['val_index']\n",
    "max_accuracy_i_train = max_accuracy['train_index']\n",
    "max_precision_i = max_precision['val_index']\n",
    "max_recall_i = max_recall['val_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df[results_df.columns[0:13]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max val accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy = randomGridSearch.fit_from_index(results_df, max_accuracy_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max train accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy_train = randomGridSearch.fit_from_index(results_df, max_accuracy_train_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Max val precision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy_train = randomGridSearch.fit_from_index(results_df, max_precision_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highest recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_recall = randomGridSearch.fit_from_index(results_df, max_recall_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
