{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:From /home/tord/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/util/module_wrapper.py:49: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.Modeling.TrainSingleModelRam import TrainSingleModelRam\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "\n",
    "utils = GlobalUtils()\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "parallel-department",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You are using the true test set.\n",
      "If this is an error, please set use_true_test_set = False and reload the kernel\n",
      "Mapping train redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping test redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded true test set, accompanied by a train set for preprocessing fitting.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Train ds:\n",
      "earthquake: (26527, 0.3371)  |  explosion: (25650, 0.3259)  |  noise: (26524, 0.337)  \n",
      "Test ds:\n",
      "earthquake: (698, 0.0396)  |  explosion: (8365, 0.4742)  |  noise: (8579, 0.4863)  \n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.25,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : True,\n",
    "    'even_balance' : False\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "train_ds = loadData.train\n",
    "test_ds = loadData.test\n",
    "handler = DataHandler(loadData)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "foreign-construction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing loading of the test set\n",
      "Step 1: Fit augmentors and scalers on training data\n",
      "Fitting train time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 449.16010069847107 seconds. Total datapoints fitted: 78701.\n",
      "Average time per datapoint: 0.005707171455235271\n",
      "\n",
      "\n",
      "Stage one loading training set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process of normalizer skipped as unecessary\n",
      "\n",
      "\n",
      "Stage two loading training set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Fitting noise augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Step 2: Load and transform the test set, using the previously fitted scaler and augmentors\n",
      "Fitting test time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 158.3871157169342 seconds. Total datapoints fitted: 17642.\n",
      "Average time per datapoint: 0.008977843539107482\n",
      "\n",
      "\n",
      "Stage one loading test set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading test set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Time elapsed to completion of load_to_ram_final_evaluation: 0:12:30.661189\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0e161d9b9e83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                       meier_load = False)\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoiseAug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mramLoader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_to_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "use_time_augmentor = True\n",
    "scaler_name = \"normalize\"\n",
    "use_noise_augmentor = True\n",
    "filter_name = None\n",
    "band_min = 2.0\n",
    "band_max = 4.0\n",
    "highpass_freq = 0.075\n",
    "\n",
    "\n",
    "shutdown = False\n",
    "\n",
    "num_classes = len(list(set(loadData.label_dict.values())))\n",
    "\n",
    "\n",
    "\n",
    "ramLoader = RamLoader(loadData, \n",
    "                      handler, \n",
    "                      use_time_augmentor = use_time_augmentor, \n",
    "                      use_noise_augmentor = use_noise_augmentor, \n",
    "                      scaler_name = scaler_name,\n",
    "                      filter_name = filter_name, \n",
    "                      band_min = band_min,\n",
    "                      band_max = band_max,\n",
    "                      highpass_freq = highpass_freq, \n",
    "                      load_test_set = True, \n",
    "                      meier_load = False)\n",
    "\n",
    "x_test, y_test, noiseAug = ramLoader.load_to_ram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the best performing model on Noise vs Not-Noise\n",
    "save_dir_3n = '/media/tord/T7/Thesis_ssd/SavedModels/CNN'\n",
    "model_name_3n = 'CNN_3N_final'\n",
    "model_3n_path = f'{save_dir_3n}/{model_name_3n}'\n",
    "# This model is trained with batch_size of 256\n",
    "# Batch size will be 1 for predictions\n",
    "print(model_3n_path)\n",
    "model_3n = helper.load_model(model_3n_path)\n",
    "\n",
    "\n",
    "\n",
    "# Path of the best performing model on Explosion vs Earthquake\n",
    "save_dir_ee = '/media/tord/T7/Thesis_ssd/SavedModels/CNN'\n",
    "model_name_ee = 'CNN_EE_final'\n",
    "model_ee_path = f'{save_dir_ee}/{model_name_ee}'\n",
    "# This model is trained with batch_size of 256\n",
    "# Batch size will be 1 for predictions\n",
    "model_ee = helper.load_model(model_ee_path)\n",
    "\n",
    "def predict_final_model(loadData, model_3n, model_ee, test_traces, test_labels, noiseAug, num_channels_3n, num_channels_ee):\n",
    "    # This function assumes data to be preprocessed with normalize scaler\n",
    "    noise_not_noise_dict = loadData.noise_not_noise_dict()\n",
    "    predictions_3n = helper.predict_RamGenerator(model_3n, test_traces, test_labels, 1, True, noiseAug, None, num_channels_3n)\n",
    "    rounded_3n_predictions = np.rint(predictions_3n)\n",
    "    \n",
    "    earth_explo_dict = loadData.earth_explo_dict()\n",
    "    not_noise_traces, not_noise_labels = get_not_noise_traces(predictions_3n, test_traces, noise_not_noise_dict)\n",
    "    predictions_ee = helper.predict_RamGenerator(model_ee, not_noise_traces, not_noise_labels, 1, True, noiseAug, None, num_channels_ee)\n",
    "    rounded_ee_predictions = np.rint(predictions_ee)\n",
    "    \n",
    "    # Converting the 3N predictions into string\n",
    "    sconverted_3n_predictions = convert_predictions(rounded_3n_predictions, noise_not_noise_dict)\n",
    "    # We now have a list of [\"noise\", \"not-noise\"...]\n",
    "    # We want to take the not-noise elements and convert them into their final predictions.\n",
    "    # We can do this by using a list of indexes of the not noise elements, and replace them by their true final predictions.\n",
    "    not_noise_indexes = get_not_noise_indexes(rounded_3n_predictions, noise_not_noise_dict)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    At this point in the function we have two lists of predictions.\n",
    "    The first list corresponds to the order of the test set, and is in the form [0,1,...0,1]\n",
    "    Where 3N predictions are 1, we predict these traces in the EE model.\n",
    "    This creates the second list, of 0 and 1. \n",
    "    \n",
    "    Our test_traces contains the actual labels in string format. \n",
    "    I need to convert all of the predictions into strings, and recreate the order of the true label list.\n",
    "    \n",
    "    \n",
    "    UNFORSEEN PROBLEM: Due to everything being run batchwise, we will not be predicting every sample.\n",
    "    POSSIBLE FIX: Set batch_size to 1, and pray I do not use batch normalization. I DONT, lets try it.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    return convert_ee_predictions(rounded_ee_predictions, earth_explo_only_dict, converted_3n_predictions, not_noise_indexes)\n",
    "    \n",
    "def get_not_noise_indexes(rounded_predictions, noise_not_noise_dict):\n",
    "    not_noise_indexes = np.where(rounded_predictions == noise_not_noise_dict[\"not-noise\"])\n",
    "    return not_noise_indexes\n",
    "    \n",
    "def get_not_noise_traces(predictions, test_traces, noise_not_noise_dict):\n",
    "    rounded_predictions = np.rint(predictions)\n",
    "    not_noise_indexes = get_not_noise_indexes(rounded_predictions, noise_not_noise_dict)\n",
    "    not_noise_traces = test_traces[not_noise_indexes]\n",
    "    not_noise_labels = test_traces[not_noise_indexes]\n",
    "    return not_noise_traces, not_noise_labels\n",
    "\n",
    "def convert_ee_predictions(rounded_predictions, label_dict, converted_3n_predictions, not_noise_indexes):\n",
    "    final_predictions = converted_3n_predictions.copy()\n",
    "    converted_ee_predictions = convert_predictions(rounded_predictions, label_dict)\n",
    "    for idx, nn_index in enumerate(not_noise_indexes):\n",
    "        final_predictions[nn_index] = converted_ee_predictions[idx]\n",
    "    return final_predictions\n",
    "    \n",
    "def convert_predictions(rounded_predictions, label_dict):\n",
    "    transformed_predictions = np.empty((rounded_predictions.shape))\n",
    "    for idx, pred in enumerate(rounded_predictions):\n",
    "        transformed_predictions[idx] =  label_dict.get(pred)\n",
    "    return transformed_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_pipeline(loadData, model_3n, model_ee, test_traces, test_labels, noiseAug, num_channels_3n, num_channels_ee):\n",
    "    test_pred = predict_final_model(loadData, model_3n, model_ee, test_traces, test_labels, noiseAug, num_channels_3n, num_channels_ee)\n",
    "    final_dict = {\"noise\" : 0, \"explosion\" : 1, \"earthquake\" : 2}\n",
    "    # Let us now convert the final predictions and true labels into one-hot encoding\n",
    "    final_preds = [final_dict[x] for x in test_pred]\n",
    "    final_true = [final_dixt[x] for x in test_labels]\n",
    "    assert len(final_preds) == len(final_true), f\"preds len {len(final_preds)}, true len {len(final_true)}\"\n",
    "    # We now have equal length predictions and true labels in number format [0,1,2].\n",
    "    # Should be able to attempt to print a confusion matrix now\n",
    "    conf=tf.math.confusion_matrix(final_true, final_preds, num_classes = 3)\n",
    "    precision, recall, fscore = precision_recall_fscore_support(final_true, final_preds, beta = 2, average = \"macro\", zero_division = 0.0)\n",
    "    report = classification_report(final_true, final_preds, target_names = final_dict)\n",
    "    return conf, report, precision, recall, fscore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf, report, precision, recall, fscore = evaluate_full_pipeline(loadData, model_3n, model_ee, x_test, y_test, noiseAug, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-stake",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
