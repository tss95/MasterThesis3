{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identified-boutique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:From /home/tord/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/util/module_wrapper.py:49: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n",
      "2 3\n",
      "Balancing due to disguised labels.\n",
      "This functions barely works, and is a piece of shit that should not be trusted. Only works because noise has id: 0\n",
      "{'noise': 84799, 'earthquake': 84799, 'explosion': 82246}\n",
      "Mapping train redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping validation redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping test redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded noise non-noise dataset.\n",
      "Evenly balanced among classes in the train set.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Train ds:\n",
      "earthquake: (10631, 0.2526)  |  explosion: (10106, 0.2402)  |  noise: (21343, 0.5072)  \n",
      "Val ds:\n",
      "earthquake: (242, 0.0371)  |  explosion: (3125, 0.4788)  |  noise: (3160, 0.4841)  \n",
      "Test ds:\n",
      "earthquake: (184, 0.0423)  |  explosion: (2062, 0.4739)  |  noise: (2105, 0.4838)  \n",
      "Fitting train time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 189.98623156547546 seconds. Total datapoints fitted: 42080.\n",
      "Average time per datapoint: 0.004514881928837345\n",
      "\n",
      "\n",
      "Fitting validation time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 50.284245014190674 seconds. Total datapoints fitted: 6527.\n",
      "Average time per datapoint: 0.007704036312883511\n",
      "\n",
      "\n",
      "Fitting test time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 32.962807178497314 seconds. Total datapoints fitted: 4351.\n",
      "Average time per datapoint: 0.00757591523293434\n",
      "\n",
      "\n",
      "Stage one loading training set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading validation set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading test set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Fitting scaler progress: [------------------->] 100 %\n",
      "\n",
      "Stage two loading training set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading validation set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading test set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Time elapsed to completion of load_to_ram_noise_not_noise: 0:05:45.953476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.TrainSingleModel import TrainSingleModel\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.ts_RamGenerator import modified_data_generator\n",
    "import json\n",
    "\n",
    "import gc\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "from livelossplot import PlotLossesKeras\n",
    "from GlobalUtils import GlobalUtils\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "utils = GlobalUtils()\n",
    "\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "tf.config.optimizer.set_jit(True)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.25,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "\n",
    "\n",
    "\n",
    "model_type = \"LSTM_baseline\"\n",
    "is_lstm = True\n",
    "num_channels = 3    \n",
    "\n",
    "use_time_augmentor = True\n",
    "scaler_name = \"standard\"\n",
    "use_noise_augmentor = False\n",
    "filter_name = None\n",
    "band_min = 2.0\n",
    "band_max = 4.0\n",
    "highpass_freq = 0.075\n",
    "\n",
    "\n",
    "shutdown = False\n",
    "\n",
    "num_classes = len(list(set(loadData.label_dict.values())))\n",
    "\n",
    "\n",
    "\n",
    "ramLoader = RamLoader(loadData, \n",
    "                      handler, \n",
    "                      use_time_augmentor = use_time_augmentor, \n",
    "                      use_noise_augmentor = use_noise_augmentor, \n",
    "                      scaler_name = scaler_name,\n",
    "                      filter_name = filter_name, \n",
    "                      band_min = band_min,\n",
    "                      band_max = band_max,\n",
    "                      highpass_freq = highpass_freq, \n",
    "                      load_test_set = True, \n",
    "                      meier_load = False)\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, noiseAug = ramLoader.load_to_ram()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "searching-texture",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20210514-102558']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'log_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-63c965e7f22c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m modelTrain = TrainSingleModel(x_train, y_train, x_val, y_val, x_test, y_test, noiseAug, helper,\n\u001b[0m\u001b[1;32m     22\u001b[0m                               \u001b[0mloadData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tensorboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_liveplots\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                               \u001b[0muse_custom_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_early_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_reduced_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mramLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'log_data'"
     ]
    }
   ],
   "source": [
    "use_tensorboard = True\n",
    "use_liveplots = False\n",
    "use_custom_callback = True\n",
    "use_early_stopping = False\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = False\n",
    "log_data = True\n",
    "\n",
    "def clear_tensorboard_dir():\n",
    "        import os\n",
    "        import shutil\n",
    "        path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "        files = os.listdir(path)\n",
    "        print(files)\n",
    "        for f in files:\n",
    "            shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "\n",
    "\n",
    "modelTrain = TrainSingleModel(x_train, y_train, x_val, y_val, x_test, y_test, noiseAug, helper,\n",
    "                              loadData, model_type, num_channels, use_tensorboard, use_liveplots,\n",
    "                              use_custom_callback, use_early_stopping, use_reduced_lr, ramLoader,\n",
    "                              log_data = log_data, results_df = None, results_file_name = None, index = None, \n",
    "                              start_from_scratch = start_from_scratch)\n",
    "params = {\n",
    "        \"batch_size\" : 128,\n",
    "        \"epochs\" : 50,\n",
    "        \"learning_rate\" : 0.01,\n",
    "        \"optimizer\" : \"sgd\",\n",
    "        \"units\" : 1,\n",
    "        \"output_layer_activation\" : \"sigmoid\"\n",
    "    }\n",
    "\n",
    "model, _ = modelTrain.run(16, 15, evaluate_train = True, evaluate_val = True, evaluate_test = True, meier_mode = False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nutritional-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf, report, precision, recall, fscore, accuracy = helper.evaluate_generator(model, x_val, y_val, params[\"batch_size\"],\n",
    "                                                                             loadData.label_dict, num_channels, noiseAug,\n",
    "                                                                             scaler_name, plot_conf_matrix = True, plot_p_r_curve = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
