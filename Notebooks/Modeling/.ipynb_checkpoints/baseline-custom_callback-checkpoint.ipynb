{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from obspy import Stream, Trace, UTCDateTime\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.layers import Activation, Conv1D, Dense, Dropout, Flatten, MaxPooling3D, BatchNormalization, InputLayer, LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os\n",
    "import sys\n",
    "classes_dir = 'C:\\Documents\\Thesis_ssd\\MasterThesis-2.0'\n",
    "os.chdir(classes_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.BaselineHelperFunctions import BaselineHelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.Modeling.Models import Models\n",
    "from Classes.Modeling.RandomGridSearch import RandomGridSearch\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "from Classes import Tf_shutup\n",
    "Tf_shutup.Tf_shutup()\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = BaselineHelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "base_dir = 'C:\\Documents\\Thesis_ssd\\MasterThesis-2.0'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "loadData = LoadData(num_classes = num_classes, isBalanced = True)\n",
    "shuffle = True\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.getDatasets(shuffle = shuffle)\n",
    "data_gen = DataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'model_nr': 5, 'index': 10}\n",
    "{'batch_size': 32, 'epochs': 35, 'learning_rate': 1e-05, 'optimizer': 'rmsprop'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.4, 'filters': 21, 'kernel_size': 7, 'l1_r': 0.001, 'l2_r': 0.2, \n",
    "'output_layer_activation': 'sigmoid', 'padding': 'same', 'start_neurons': 16}\n",
    "\n",
    "{'model_nr': 4, 'index': 10}\n",
    "{'batch_size': 8, 'epochs': 30, 'learning_rate': 0.0001, 'optimizer': 'adam'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.3, 'filters': 17, 'kernel_size': 3, 'l1_r': 0.01, 'l2_r': 0.1, \n",
    "'output_layer_activation': 'softmax', 'padding': 'same', 'start_neurons': 128}\n",
    "\n",
    "Crashing model:\n",
    "Test_mode: False, use_scaler: True, use_minmax: False, use_noise_augmentor: True, detrend: False\n",
    "{'model_nr': 4, 'index': 21}\n",
    "{'batch_size': 16, 'epochs': 35, 'learning_rate': 0.001, 'optimizer': 'sgd'}\n",
    "{'activation': 'relu', 'dropout_rate': 0.01, 'filters': 15, 'kernel_size': 13, 'l1_r': 0.0001, \n",
    "'l2_r': 0.2, 'output_layer_activation': 'sigmoid', 'padding': 'same', 'start_neurons': 64}\n",
    "\n",
    "{'model_nr': 7, 'index': 38}\n",
    "Test_mode: False, use_scaler: True, use_minmax: False, use_noise_augmentor: True detrend: False. \n",
    "{'batch_size': 64, 'epochs': 40, 'learning_rate': 0.001, 'optimizer': 'rmsprop'}\n",
    "{'activation': 'tanh', 'dropout_rate': 0, 'filters': 13, 'kernel_size': 5, 'l1_r': 0.0001,\n",
    "'l2_r': 0.01, 'output_layer_activation': 'softmax', 'padding': 'same', 'start_neurons': 32}\n",
    "{'test_loss': 1.4299607276916504, 'test_accuracy': 0.7728365659713745, \n",
    "'test_precision': 0.7728365659713745, 'test_recall': 0.7728365659713745}\n",
    "{'train_loss': 0.9629267454147339, 'train_accuracy': 0.8897058963775635, \n",
    "'train_precision': 0.8897058963775635, 'train_recall': 0.8897058963775635}\n",
    "\n",
    "\"\"\"\n",
    "############ Model picker #############\n",
    "model_nr = 8\n",
    "\n",
    "########### Hyperparameters ###########\n",
    "batch_size = 64\n",
    "epochs = 80\n",
    "learning_rate = 0.001\n",
    "#opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=1.0, clipvalue=0.5)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "activation = 'tanh'\n",
    "output_layer_activation = 'softmax'\n",
    "dropout_rate = 0\n",
    "filters = 13\n",
    "kernel_size = 5\n",
    "l1_r = 0.0001\n",
    "l2_r = 0.01\n",
    "padding = 'same'\n",
    "start_neurons = 32\n",
    "\n",
    "########### Preprocessing ###########\n",
    "test = False\n",
    "use_noise_augmentor = True\n",
    "detrend = False\n",
    "use_scaler = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.2\n",
    "\n",
    "use_tensorboard = True\n",
    "use_livelossplot = False\n",
    "use_custom = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20201019-143056']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13444), started 1:27:17 ago. (Use '!kill 13444' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-562cf20334b5543\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-562cf20334b5543\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "        \n",
    "if use_tensorboard:\n",
    "    import datetime\n",
    "    clear_tensorboard_dir()\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir tensorboard_dir/fit\n",
    "    log_dir = f\"{base_dir}/tensorboard_dir/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks = [tensorboard_callback]\n",
    "\n",
    "if use_custom:\n",
    "    custom_callback = CustomCallback(data_gen)\n",
    "    callbacks = custom_callback\n",
    "elif use_livelossplot:\n",
    "    callbacks = PlotLossesKeras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (64, 3, 32)               772352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (64, 3, 32)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (64, 3, 32)               128       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, 3, 16)               3136      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, 3, 16)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (64, 3, 16)               64        \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, 3, 8)                136       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (64, 3, 8)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (64, 3, 8)                32        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (64, 24)                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, 2)                   50        \n",
      "=================================================================\n",
      "Total params: 775,898\n",
      "Trainable params: 775,786\n",
      "Non-trainable params: 112\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_ds, channels, timesteps = data_gen.get_trace_shape_no_cast(train_ds)\n",
    "input_shape = (batch_size, channels, timesteps)\n",
    "\n",
    "build_model_args = {'model_nr' : model_nr,\n",
    "                    'input_shape' : input_shape,\n",
    "                    'num_classes' : num_classes,\n",
    "                    'dropout_rate' : dropout_rate,\n",
    "                    'activation' : activation,\n",
    "                    'output_layer_activation' : output_layer_activation,\n",
    "                    'l2_r' : l2_r,\n",
    "                    'l1_r' : l1_r,\n",
    "                    'full_regularizer' : True,\n",
    "                    'start_neurons' : start_neurons,\n",
    "                    'filters' : filters,\n",
    "                    'kernel_size' : kernel_size,\n",
    "                    'padding' : 'same'}\n",
    "model = Models(**build_model_args).model\n",
    "\n",
    "model_args = {'loss' : \"binary_crossentropy\",\n",
    "              'optimizer' : opt,\n",
    "              'metrics' : [\"accuracy\",\"MSE\",\n",
    "                           tf.keras.metrics.Precision(thresholds=None, top_k=None, class_id=None, name=None, dtype=None),\n",
    "                           tf.keras.metrics.Recall(thresholds=None, top_k=None, class_id=None, name=None, dtype=None)]}\n",
    "model.compile(**model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "  1/171 [..............................] - ETA: 0s - loss: 5.9093 - accuracy: 0.4531 - MSE: 0.3827 - precision: 0.4531 - recall: 0.4531WARNING:tensorflow:From C:\\Users\\tss_9\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "  2/171 [..............................] - ETA: 13s - loss: 5.7663 - accuracy: 0.4766 - MSE: 0.3536 - precision: 0.4766 - recall: 0.4766WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0270s vs `on_train_batch_end` time: 0.1371s). Check your callbacks.\n",
      "172/171 [==============================] - 31s 180ms/step - loss: 3.2411 - accuracy: 0.5821 - MSE: 0.2538 - precision: 0.5821 - recall: 0.5821 - val_loss: 2.1737 - val_accuracy: 0.5609 - val_MSE: 0.2445 - val_precision: 0.5609 - val_recall: 0.5609\n",
      "Epoch 2/80\n",
      "172/171 [==============================] - 30s 174ms/step - loss: 1.7000 - accuracy: 0.7087 - MSE: 0.1897 - precision: 0.7087 - recall: 0.7087 - val_loss: 1.5681 - val_accuracy: 0.6240 - val_MSE: 0.2243 - val_precision: 0.6240 - val_recall: 0.6240\n",
      "Epoch 3/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.3570 - accuracy: 0.7520 - MSE: 0.1657 - precision: 0.7520 - recall: 0.7520 - val_loss: 1.3506 - val_accuracy: 0.6792 - val_MSE: 0.1983 - val_precision: 0.6792 - val_recall: 0.6792\n",
      "Epoch 4/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.2107 - accuracy: 0.7878 - MSE: 0.1477 - precision: 0.7878 - recall: 0.7878 - val_loss: 1.3063 - val_accuracy: 0.6911 - val_MSE: 0.1932 - val_precision: 0.6911 - val_recall: 0.6911\n",
      "Epoch 5/80\n",
      "172/171 [==============================] - 29s 171ms/step - loss: 1.1621 - accuracy: 0.8056 - MSE: 0.1336 - precision: 0.8056 - recall: 0.8056 - val_loss: 1.3085 - val_accuracy: 0.7073 - val_MSE: 0.1905 - val_precision: 0.7073 - val_recall: 0.7073\n",
      "Epoch 6/80\n",
      "172/171 [==============================] - 30s 175ms/step - loss: 1.1428 - accuracy: 0.8257 - MSE: 0.1222 - precision: 0.8257 - recall: 0.8257 - val_loss: 1.3523 - val_accuracy: 0.7068 - val_MSE: 0.1922 - val_precision: 0.7068 - val_recall: 0.7068\n",
      "Epoch 7/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.1542 - accuracy: 0.8397 - MSE: 0.1136 - precision: 0.8397 - recall: 0.8397 - val_loss: 1.3688 - val_accuracy: 0.6969 - val_MSE: 0.1971 - val_precision: 0.6969 - val_recall: 0.6969\n",
      "Epoch 8/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.1455 - accuracy: 0.8518 - MSE: 0.1063 - precision: 0.8518 - recall: 0.8518 - val_loss: 1.4223 - val_accuracy: 0.6984 - val_MSE: 0.2011 - val_precision: 0.6984 - val_recall: 0.6984\n",
      "Epoch 9/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.1523 - accuracy: 0.8500 - MSE: 0.1052 - precision: 0.8500 - recall: 0.8500 - val_loss: 1.4556 - val_accuracy: 0.7026 - val_MSE: 0.2031 - val_precision: 0.7026 - val_recall: 0.7026\n",
      "Epoch 10/80\n",
      "172/171 [==============================] - 30s 174ms/step - loss: 1.1345 - accuracy: 0.8582 - MSE: 0.0988 - precision: 0.8582 - recall: 0.8582 - val_loss: 1.4518 - val_accuracy: 0.7089 - val_MSE: 0.2007 - val_precision: 0.7089 - val_recall: 0.7089\n",
      "Epoch 11/80\n",
      "172/171 [==============================] - 31s 178ms/step - loss: 1.1520 - accuracy: 0.8660 - MSE: 0.0965 - precision: 0.8660 - recall: 0.8660 - val_loss: 1.4428 - val_accuracy: 0.7115 - val_MSE: 0.1992 - val_precision: 0.7115 - val_recall: 0.7115\n",
      "Epoch 12/80\n",
      "172/171 [==============================] - 30s 172ms/step - loss: 1.1307 - accuracy: 0.8660 - MSE: 0.0949 - precision: 0.8660 - recall: 0.8660 - val_loss: 1.4865 - val_accuracy: 0.7078 - val_MSE: 0.2067 - val_precision: 0.7078 - val_recall: 0.7078\n",
      "Epoch 13/80\n",
      "172/171 [==============================] - 30s 177ms/step - loss: 1.1444 - accuracy: 0.8751 - MSE: 0.0899 - precision: 0.8751 - recall: 0.8751 - val_loss: 1.5022 - val_accuracy: 0.7063 - val_MSE: 0.2041 - val_precision: 0.7063 - val_recall: 0.7063\n",
      "Epoch 14/80\n",
      "172/171 [==============================] - 30s 173ms/step - loss: 1.1411 - accuracy: 0.8758 - MSE: 0.0865 - precision: 0.8758 - recall: 0.8758 - val_loss: 1.5144 - val_accuracy: 0.7141 - val_MSE: 0.2053 - val_precision: 0.7141 - val_recall: 0.7141\n",
      "Epoch 15/80\n",
      "172/171 [==============================] - 30s 176ms/step - loss: 1.1270 - accuracy: 0.8813 - MSE: 0.0856 - precision: 0.8813 - recall: 0.8813 - val_loss: 1.4311 - val_accuracy: 0.7177 - val_MSE: 0.1969 - val_precision: 0.7177 - val_recall: 0.7177\n",
      "Epoch 16/80\n",
      "172/171 [==============================] - 30s 176ms/step - loss: 1.1333 - accuracy: 0.8806 - MSE: 0.0871 - precision: 0.8806 - recall: 0.8806 - val_loss: 1.4463 - val_accuracy: 0.7245 - val_MSE: 0.1941 - val_precision: 0.7245 - val_recall: 0.7245\n",
      "Epoch 17/80\n",
      "172/171 [==============================] - 30s 172ms/step - loss: 1.1167 - accuracy: 0.8871 - MSE: 0.0842 - precision: 0.8871 - recall: 0.8871 - val_loss: 1.4856 - val_accuracy: 0.7203 - val_MSE: 0.2005 - val_precision: 0.7203 - val_recall: 0.7203\n",
      "Epoch 18/80\n",
      "172/171 [==============================] - 31s 179ms/step - loss: 1.1061 - accuracy: 0.8930 - MSE: 0.0795 - precision: 0.8930 - recall: 0.8930 - val_loss: 1.4913 - val_accuracy: 0.7115 - val_MSE: 0.2020 - val_precision: 0.7115 - val_recall: 0.7115\n",
      "Epoch 19/80\n",
      "172/171 [==============================] - ETA: 0s - loss: 1.1004 - accuracy: 0.8902 - MSE: 0.0807 - precision: 0.8902 - recall: 0.8902"
     ]
    }
   ],
   "source": [
    "scaler = None\n",
    "if use_scaler:\n",
    "    scaler = StandardScalerFitter(train_ds).fit_scaler(test = test, detrend = detrend)\n",
    "aug = None\n",
    "if use_noise_augmentor:\n",
    "    aug = NoiseAugmentor(train_ds, use_scaler, scaler)\n",
    "    \n",
    "    \n",
    "\n",
    "gen_args = {\n",
    "    'batch_size' : batch_size,\n",
    "    'test' : test,\n",
    "    'detrend' : detrend,\n",
    "    'use_scaler' : use_scaler,\n",
    "    'scaler' : scaler,\n",
    "    'use_noise_augmentor' : use_noise_augmentor,\n",
    "    'augmentor' : aug,\n",
    "    'num_classes' : num_classes,\n",
    "    'use_highpass' : use_highpass,\n",
    "    'highpass_freq' : highpass_freq\n",
    "}\n",
    "\n",
    "\n",
    "train_gen = data_gen.data_generator(train_ds, **gen_args)\n",
    "val_gen = data_gen.data_generator(val_ds, **gen_args)\n",
    "test_gen = data_gen.data_generator(test_ds, **gen_args)\n",
    "\n",
    "\n",
    "\n",
    "args = {'steps_per_epoch' : helper.get_steps_per_epoch(train_ds, batch_size, test),\n",
    "        'epochs' : epochs,\n",
    "        'validation_data' : val_gen,\n",
    "        'validation_steps' : helper.get_steps_per_epoch(val_ds, batch_size, test),\n",
    "        'verbose' : 1,\n",
    "        'use_multiprocessing' : False, \n",
    "        'workers' : 1,\n",
    "        'callbacks' : [callbacks]\n",
    "}\n",
    "\n",
    "model_fit = model.fit(train_gen, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_logs = custom_callback.full_training_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.get_n_points_with_highest_training_loss(train_ds, 100, full_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_points_with_highest_training_loss(full_logs, train_ds, n):\n",
    "    train_ds_dict = {}\n",
    "    for path, label in train_ds:\n",
    "        train_ds_dict[path] = {'label' : label,\n",
    "                               'loss': 0,\n",
    "                               'average_loss' : 0,\n",
    "                               'occurances' : 0}\n",
    "    counter = 0\n",
    "    for batch in full_logs:\n",
    "        loss = batch['loss']\n",
    "        for path_class in batch['batch_samples']:\n",
    "            train_ds_dict[path_class[0]]['loss'] += loss\n",
    "            train_ds_dict[path_class[0]]['occurances'] += 1\n",
    "    \n",
    "    train_ds_list = []\n",
    "    for sample in np.array(train_ds[:,0]):\n",
    "        if train_ds_dict[sample]['occurances'] == 0:\n",
    "            continue\n",
    "        train_ds_dict[sample]['average_loss'] = train_ds_dict[sample]['loss'] / train_ds_dict[sample]['occurances']\n",
    "        train_ds_list.append((sample, train_ds_dict[sample]['label'],train_ds_dict[sample]['average_loss']))\n",
    "    \n",
    "    sorted_train_ds_list = sorted(train_ds_list, key=lambda x: x[2], reverse = True)\n",
    "        \n",
    "    \n",
    "    return sorted_train_ds_list[0:n]\n",
    "        \n",
    "#get_n_points_with_highest_loss(full_logs, train_ds, 100)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(generator=test_gen, steps=helper.get_steps_per_epoch(test_ds, batch_size, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true_categorical.argmax(axis=1), predictions[0:1234].argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_confusion_matrix(model, test_gen, test_ds, batch_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_generator(val_gen, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_confusion_matrix(test_ds, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
