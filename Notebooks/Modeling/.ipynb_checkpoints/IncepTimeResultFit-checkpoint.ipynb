{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3.0'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.NarrowSearchRam import NarrowSearchRam\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutup()\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n",
      "{'noise': 105999, 'earthquake': 105999, 'explosion': 102808}\n",
      "Mapping redundancy: [------------------------------->        ] 80 %\r"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.4,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Preprocessing ###########\n",
    "use_noise_augmentor = True\n",
    "use_time_augmentor = True\n",
    "detrend = False\n",
    "use_scaler = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.2\n",
    "\n",
    "use_tensorboard = True\n",
    "use_livelossplot = False\n",
    "use_custom = False\n",
    "use_reduce_lr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResultModelGetter:\n",
    "\n",
    "    def __init__(self, loadData, handler, helper, timeAug = None, scaler = None, noiseAug = None):\n",
    "        self.loadData = loadData\n",
    "        self.handler = handler\n",
    "        self.helper = helper\n",
    "        self.timeAug = timeAug\n",
    "        self.scaler = scaler\n",
    "        self.noiseAug = noiseAug\n",
    "\n",
    "    def model_from_result(self, result_name, input_shape, num_classes = 2, by_index = False, index = 0, top_index = 0, sort_by = ['val_f1', 'train_f1'], early_stop = False):\n",
    "        df = self.load_df(result_name, num_classes)\n",
    "        df_f1 = df.copy()\n",
    "        df_f1 = self.add_f1(df_f1)\n",
    "        df_f1.convert_dtypes().dtypes\n",
    "\n",
    "        if by_index:\n",
    "            model_df = df_f1.iloc[index]\n",
    "        else:\n",
    "            ascending = False\n",
    "            if sort_by == ['val_loss', 'train_loss']:\n",
    "                ascending = True\n",
    "            df_f1 = df_f1.sort_values(by=sort_by, axis = 0, ascending = ascending).reset_index()\n",
    "            model_df = df_f1.iloc[top_index]\n",
    "\n",
    "        model_dict = self.row_to_dict(model_df)\n",
    "        batch_size = model_dict['batch_size']\n",
    "        epochs = model_dict['epochs']\n",
    "        learning_rate = model_dict['learning_rate']\n",
    "        num_channels = model_dict['num_channels']\n",
    "        fit_args = {\"batch_size\" : batch_size, \"epochs\" : epochs, \"learning_rate\" : learning_rate, \"num_channels\" : num_channels}\n",
    "\n",
    "        del model_dict['batch_size']\n",
    "        del model_dict['epochs']\n",
    "        del model_dict['learning_rate']\n",
    "        del model_dict['num_channels']\n",
    "        \n",
    "        return self.build_model_by_name(result_name, model_dict, input_shape, num_classes, fit_args)\n",
    "\n",
    "    def row_to_dict(self, model_df):\n",
    "        keys = list(model_df.keys())\n",
    "        # Assumes 10 columns dedicated to results and the rest to hyperparams\n",
    "        hyper_keys = keys[:len(keys) - 10]\n",
    "        model_dict = model_df[:len(hyper_keys)].to_dict()\n",
    "        del model_dict['index']\n",
    "        return model_dict\n",
    "\n",
    "    def build_model_by_name(self, result_name, model_dict, input_shape, num_classes, fit_args):\n",
    "        model_name = result_name.split('_')[1]\n",
    "        if model_name == 'InceptionTime':\n",
    "            return self.build_incep_model(model_dict, input_shape, num_classes, fit_args)\n",
    "        else:\n",
    "            raise Exception(f\"Not implemented {model_name} yet\")\n",
    "\n",
    "    def build_incep_model(self, model_dict, input_shape, num_classes, fit_args):\n",
    "        optimizer = self.helper.get_optimizer(model_dict['optimizer'], fit_args[\"learning_rate\"])\n",
    "        compile_args = self.helper.generate_model_compile_args(optimizer, num_classes)\n",
    "        input = {\"input_shape\" : input_shape, \"nr_classes\" : num_classes}\n",
    "        build_dict = {**input, **model_dict}\n",
    "        build_dict[\"optimizer\"] = optimizer\n",
    "\n",
    "        inceptionTime = InceptionTimeModel(**build_dict)\n",
    "        model = inceptionTime.build_model(input_shape, num_classes)\n",
    "        print(model.summary())\n",
    "        return model, fit_args\n",
    "\n",
    "        \n",
    "    def load_df(self, result_name, num_classes):\n",
    "        return GridSearchResultProcessor().get_results_df_by_name(result_name, num_classes)\n",
    "\n",
    "    def add_f1(self, df):\n",
    "        df_f1 = df\n",
    "        df_f1.columns=df_f1.columns.str.strip()\n",
    "        all_train_precision = df_f1['train_precision']\n",
    "        all_train_recall = df_f1['train_recall']\n",
    "        all_val_precision = df_f1['val_precision']\n",
    "        all_val_recall = df_f1['val_recall']\n",
    "        f1_train = self.create_f1_list(all_train_precision, all_train_recall)\n",
    "        f1_val = self.create_f1_list(all_val_precision, all_val_recall)\n",
    "        df_f1['train_f1'] = f1_train\n",
    "        df_f1['val_f1'] = f1_val\n",
    "        return df_f1\n",
    "\n",
    "    def f1_score(self, precision, recall):\n",
    "        f1 = 2*((precision*recall)/(precision + recall))\n",
    "        return f1\n",
    "\n",
    "    def create_f1_list(self, precision_df, recall_df):\n",
    "        f1 = []\n",
    "        for i in range(len(precision_df)):\n",
    "            f1.append(self.f1_score(precision_df.loc[i], recall_df.loc[i]))\n",
    "        return f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultModel = ResultModelGetter(loadData, handler, HelperFunctions(), None, None, None)\n",
    "model, fit_args = resultModel.model_from_result('results_InceptionTime_noiseNotNoise_timeAug_sscale_noiseAug_earlyS.csv', (3, 6000), by_index = False, index = 10, top_index = 0, sort_by = ['val_accuracy', 'train_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Preprocessing ###########\n",
    "use_noise_augmentor = True\n",
    "use_time_augmentor = True\n",
    "detrend = False\n",
    "use_scaler = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.2\n",
    "\n",
    "use_tensorboard = True\n",
    "use_livelossplot = False\n",
    "use_custom = False\n",
    "use_reduce_lr = True\n",
    "\n",
    "scaler = None\n",
    "noiseAug = None\n",
    "if use_time_augmentor:\n",
    "    timeAug.fit()\n",
    "if use_scaler:\n",
    "    scaler = StandardScalerFitter(train_ds, timeAug).fit_scaler(detrend = detrend)\n",
    "if use_noise_augmentor:\n",
    "    noiseAug = NoiseAugmentor(train_ds, use_scaler, scaler, loadData, timeAug)\n",
    "num_ds, channels, timesteps = handler.get_trace_shape_no_cast(train_ds, use_time_augmentor)\n",
    "input_shape = (channels, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ramLoader = RamLoader(handler, timeAug, scaler)\n",
    "x_train, y_train = ramLoader.load_to_ram(train_ds, False)\n",
    "x_val, y_val = ramLoader.load_to_ram(val_ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "\n",
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "\n",
    "if use_tensorboard:\n",
    "    import datetime\n",
    "    clear_tensorboard_dir()\n",
    "    #%tensorboard --logdir tensorboard_dir/fit\n",
    "    log_dir = f\"{base_dir}/tensorboard_dir/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    callbacks.append(tensorboard_callback)\n",
    "    # tensorboard --logdir='/media/tord/T7/Thesis_ssd/MasterThesis3.0/tensorboard_dir/fit'\n",
    "\n",
    "if use_custom:\n",
    "    custom_callback = CustomCallback(data_gen)\n",
    "    callbacks.append(custom_callback)\n",
    "elif use_livelossplot:\n",
    "    callbacks.append(PlotLossesKeras())\n",
    "elif use_reduce_lr:\n",
    "    callbacks.append(tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=50,\n",
    "                                                          min_lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "gen = RamGenerator(loadData, handler)\n",
    "\n",
    "batch_size = fit_args[\"batch_size\"]\n",
    "\n",
    "train_gen = gen.data_generator(x_train, y_train, batch_size)\n",
    "val_gen = gen.data_generator(x_val, y_val, batch_size)\n",
    "\n",
    "args = {'steps_per_epoch' : helper.get_steps_per_epoch(train_ds, batch_size),\n",
    "        'epochs' : 100,\n",
    "        'validation_data' : val_gen,\n",
    "        'validation_steps' : helper.get_steps_per_epoch(val_ds, batch_size),\n",
    "        'verbose' : 1,\n",
    "        'use_multiprocessing' : False, \n",
    "        'workers' : 1,\n",
    "        'callbacks' : callbacks\n",
    "}\n",
    "\n",
    "model_fit = model.fit(train_gen, **args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
