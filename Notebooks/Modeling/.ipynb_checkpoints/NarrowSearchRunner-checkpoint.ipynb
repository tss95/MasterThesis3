{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import pprint\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "base_dir = 'F:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "\n",
    "helper = HelperFunctions()\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data conditions: ###\n",
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.05,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "\n",
    "loadData = LoadData(**load_args)\n",
    "\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "dataGen = DataGenerator(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NarrowSearch(GridSearchResultProcessor):\n",
    "    \n",
    "    def __init__(self, loadData, train_ds, val_ds, test_ds, model_type, detrend, use_scaler, use_time_augmentor, use_noise_augmentor,\n",
    "                 use_minmax, use_highpass, n_picks, main_grid, hyper_grid, model_grid, use_tensorboard = False, \n",
    "                 use_liveplots = True, use_custom_callback = False, use_early_stopping = False, highpass_freq = 0.1, \n",
    "                 start_from_scratch = True):\n",
    "        \n",
    "        self.loadData = loadData\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.model_nr_type = model_type\n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "        self.detrend = detrend\n",
    "        self.use_scaler = use_scaler\n",
    "        self.use_noise_augmentor = use_noise_augmentor\n",
    "        self.use_time_augmentor = use_time_augmentor\n",
    "        self.use_minmax = use_minmax\n",
    "        self.use_highpass = use_highpass\n",
    "        self.n_picks = n_picks\n",
    "        self.main_grid = main_grid\n",
    "        self.hyper_grid = hyper_grid\n",
    "        self.model_grid = model_grid\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.highpass_freq = highpass_freq\n",
    "        self.start_from_scratch = start_from_scratch\n",
    "        self.helper = HelperFunctions()\n",
    "        self.dataGen = DataGenerator(self.loadData)\n",
    "        self.handler = DataHandler(self.loadData)\n",
    "        if self.loadData.earth_explo_only:\n",
    "            self.full_ds = np.concatenate((self.loadData.noise_ds, self.loadData.full_ds))\n",
    "        else:\n",
    "            self.full_ds = self.loadData.full_ds\n",
    "        \n",
    "    def fit(self):\n",
    "        self.timeAug, self.scaler, self.noiseAug = self.init_preprocessing(self.use_time_augmentor, \n",
    "                                                                           self.use_scaler, \n",
    "                                                                           self.use_noise_augmentor)\n",
    "        self.hyper_picks, self.model_picks = self.create_search_space(self.main_grid, self.hyper_grid, self.model_grid)\n",
    "        self.results_df = self.initiate_results_df(self.results_file_name, self.num_classes, self.start_from_scratch, self.hyper_picks[0], self.model_picks[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def create_search_space(self, main_grid, hyper_grid, model_grid):\n",
    "        hypermodel_grid = {**hyper_grid, **model_grid}\n",
    "        key_list = list(hypermodel_grid.keys())\n",
    "        np.random.shuffle(key_list)\n",
    "        search_list = []\n",
    "        for key in key_list:\n",
    "            if len(hypermodel_grid[key]) > 1:\n",
    "                one_model = main_grid.copy()\n",
    "                one_model[key] = hypermodel_grid[key]\n",
    "                key_grid = list(ParameterGrid(one_model))\n",
    "                search_list.append(key_grid)\n",
    "            else:\n",
    "                continue\n",
    "        search_list = list(chain.from_iterable(search_list))\n",
    "        hyper_search, model_search = self.unmerge_search_space(search_list, hyper_grid, model_grid)\n",
    "        return hyper_search, model_search\n",
    "    \n",
    "    def unmerge_search_space(self, search_space, hyper_grid, model_grid):\n",
    "        hyper_keys = list(hyper_grid.keys())\n",
    "        model_keys = list(model_grid.keys())\n",
    "        hyper_search_grid = []\n",
    "        model_search_grid = []\n",
    "        for space in search_space:\n",
    "            hyper_search_grid.append({key:value for (key,value) in space.items() if key in hyper_keys})\n",
    "            model_search_grid.append({key:value for (key,value) in space.items() if key in model_keys})\n",
    "        return hyper_search_grid, model_search_grid\n",
    "    \n",
    "    def init_preprocessing(self, use_time_augmentor, use_scaler, use_noise_augmentor):\n",
    "        if use_time_augmentor:\n",
    "            timeAug = TimeAugmentor(self.handler, self.full_ds, seed = self.loadData.seed)\n",
    "            timeAug.fit()\n",
    "        else:\n",
    "            timeAug = None\n",
    "        if use_scaler:\n",
    "            if self.use_minmax:\n",
    "                scaler = MinMaxScalerFitter(self.train_ds, timeAug).fit_scaler(detrend = self.detrend)\n",
    "            else:\n",
    "                scaler = StandardScalerFitter(self.train_ds, timeAug).fit_scaler(detrend = self.detrend)\n",
    "        else:\n",
    "            scaler = None\n",
    "        if use_noise_augmentor:\n",
    "            noiseAug = NoiseAugmentor(self.loadData.noise_ds, use_scaler, scaler, self.loadData, timeAug)\n",
    "        else:\n",
    "            noiseAug = None\n",
    "        return timeAug, scaler, noiseAug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_grid ={\"batch_size\" : [64],\n",
    "            \"epochs\" : [50],\n",
    "            \"learning_rate\" : [0.01],\n",
    "            \"optimizer\" : [\"sgd\"],\n",
    "            \"start_neurons\" : [32],\n",
    "            \"dropout_rate\" : [0.3],\n",
    "            \"filters\" : [17],\n",
    "            \"kernel_size\" : [5],\n",
    "            \"padding\" : [\"same\"],\n",
    "            \"l2_r\" : [0.001],\n",
    "            \"l1_r\" : [0.0001],\n",
    "            \"activation\" : [\"tanh\"],\n",
    "            \"output_layer_activation\" : [\"sigmoid\"]\n",
    "           }\n",
    "\n",
    "hyper_grid = {\n",
    "        \"batch_size\" : [64, 128, 256, 512],\n",
    "        \"epochs\" : [30, 33, 35],\n",
    "        \"learning_rate\" : [0.1, 0.01, 0.001, 0.0001, 0.00001],\n",
    "        \"optimizer\" : [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "    }\n",
    "model_grid = {\n",
    "    \"start_neurons\" : [16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"dropout_rate\" : [0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.001, 0],\n",
    "    \"filters\" : [11, 13, 15, 17, 19, 21, 23, 25, 27],\n",
    "    \"kernel_size\" : [3, 5, 7, 9, 11, 13, 15],\n",
    "    \"padding\" : [\"same\"],\n",
    "    \"l2_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"l1_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"activation\" : [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"output_layer_activation\" : [\"sigmoid\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = create_search_space(main_grid, hyper_grid, model_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'batch_size': 64,\n",
       " 'dropout_rate': 0.3,\n",
       " 'epochs': 50,\n",
       " 'filters': 17,\n",
       " 'kernel_size': 5,\n",
       " 'l1_r': 0.0001,\n",
       " 'l2_r': 0.001,\n",
       " 'learning_rate': 0.01,\n",
       " 'optimizer': 'sgd',\n",
       " 'output_layer_activation': 'sigmoid',\n",
       " 'padding': 'same',\n",
       " 'start_neurons': 32}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unmerge_search_space(search_space, hyper_grid, model_grid):\n",
    "        hyper_keys = list(hyper_grid.keys())\n",
    "        model_keys = list(model_grid.keys())\n",
    "        hyper_search_grid = []\n",
    "        model_search_grid = []\n",
    "        for space in search_space:\n",
    "            hyper_search_grid.append({key:value for (key,value) in space.items() if key in hyper_keys})\n",
    "            model_search_grid.append({key:value for (key,value) in space.items() if key in model_keys})\n",
    "            \n",
    "        return hyper_search_grid, model_search_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper, model = unmerge_search_space(search_space, hyper_grid, model_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_space)):\n",
    "    assert {**hyper[i], **model[i]} == search_space[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('a', 1), ('b', 2), ('c', 3)}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = ['a', 'b', 'c']\n",
    "old ={'a':1,\n",
    "    'b':2,\n",
    "    'c':3,\n",
    "    'd':4}\n",
    "\n",
    "{(k, old.get(k)) for k in old if k in keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'batch_size': 64,\n",
       " 'dropout_rate': 0.3,\n",
       " 'epochs': 50,\n",
       " 'filters': 17,\n",
       " 'kernel_size': 5,\n",
       " 'l1_r': 0.0001,\n",
       " 'l2_r': 0.001,\n",
       " 'learning_rate': 0.01,\n",
       " 'optimizer': 'sgd',\n",
       " 'output_layer_activation': 'sigmoid',\n",
       " 'padding': 'same',\n",
       " 'start_neurons': 32}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
