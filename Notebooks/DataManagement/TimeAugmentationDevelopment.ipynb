{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "classes_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3.0'\n",
    "os.chdir(classes_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.NarrowSearchRam import NarrowSearchRam\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutup()\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.3,\n",
    "    'seed' : 2,\n",
    "    'subsample_size' : 0.3,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25390 5078 3386\n",
      "Total: 25390, earthquake: 8446, explosion: 8337, noise: 8607\n",
      "Nr noise samples 8607\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2005-02-14T17.37.06.000000Z.h5'\n",
      "  'noise' 0]\n",
      " [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2017-02-13T13.55.42.000000Z.h5'\n",
      "  'noise' 0]\n",
      " [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/earthquakes/2016-02-11T23.43.25.442000Z.h5'\n",
      "  'earthquake' 0]\n",
      " ..., \n",
      " [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/explosions/2016-06-16T12.22.17.857000Z.h5'\n",
      "  'explosion' 0]\n",
      " [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/explosions/2011-11-07T00.23.15.537000Z.h5'\n",
      "  'explosion' 0]\n",
      " [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/explosions/1996-09-08T21.00.07.474000Z.h5'\n",
      "  'explosion' 0]]\n"
     ]
    }
   ],
   "source": [
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import utils\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "from dateutil import parser\n",
    "import time\n",
    "import h5py\n",
    "import json\n",
    "\n",
    "\n",
    "class TimeAugmentor():\n",
    "    \n",
    "    def __init__(self, DataHandler, ds, seed = None):\n",
    "        self.handler = DataHandler\n",
    "        self.ds = ds\n",
    "        self.fitted_dict = {}\n",
    "        self.seed = seed\n",
    "    \n",
    "    def fit(self):\n",
    "        time_start = time.time()\n",
    "        path_red_ds = self.ds[:,[0,2]]\n",
    "        len_ds = len(path_red_ds)\n",
    "        _,_,pre_length = self.handler.get_trace_shape_no_cast(self.ds, False)\n",
    "        post_length = 6000\n",
    "        np.random.seed(0)\n",
    "        print(\"Issues will occur if upsampling of explosions or noise is implemented\")\n",
    "        explo_ds = self.ds[self.ds[:,1] == \"explosion\"]\n",
    "        earth_ds = self.ds[self.ds[:,1] == \"earthquake\"]\n",
    "        noise_ds = self.ds[self.ds[:,1] == \"noise\"]\n",
    "        max_redundancy = max(earth_ds[:,2])\n",
    "        #fitted_dict = dict.fromkeys(set(self.ds[:,0]))\n",
    "        fitted_dict = {}\n",
    "        if len(explo_ds) > 0:\n",
    "            i = 0\n",
    "            explo_len = len(set(explo_ds[:,0]))\n",
    "            for path in set(explo_ds[:,0]):\n",
    "                random_start_index = np.random.randint(0,4500, 1)\n",
    "                initial_index, info = self.find_initial_event_index(path)\n",
    "                fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                     'random_start_index' : random_start_index}\n",
    "                self.progress_bar(i + 1, explo_len)\n",
    "                i += 1\n",
    "        print(\"Finished explosions\")\n",
    "        if len(noise_ds) > 0:\n",
    "            i = 0\n",
    "            noise_len = len(set(noise_ds[:,0]))\n",
    "            for path in set(noise_ds[:,0]):\n",
    "                random_start_index = np.random.randint(0,4500, 1)\n",
    "                initial_index, info = self.find_initial_event_index(path)\n",
    "                fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                     'random_start_index' : random_start_index}\n",
    "                self.progress_bar(i + 1, noise_len)\n",
    "                i += 1\n",
    "        print(\"Finished noise\")\n",
    "        if len(earth_ds) > 0:\n",
    "            i = 0\n",
    "            earth_len = len(set(earth_ds[:,0]))\n",
    "            for path in set(earth_ds[:,0]):\n",
    "                random_start_index = np.random.randint(0,4500, max_redundancy)\n",
    "                initial_index, info = self.find_initial_event_index(path)\n",
    "                fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                     'random_start_index' : random_start_index}\n",
    "                self.progress_bar(i + 1, earth_len)\n",
    "                i += 1\n",
    "\n",
    "        print(\"Finished earthquakes\")\n",
    "        self.fitted_dict = fitted_dict\n",
    "        time_end = time.time()\n",
    "        print(f\"Fit process completed after {time_end - time_start} seconds. Total datapoints fitted: {len(path_red_ds)}.\")\n",
    "        print(f\"Average time per datapoint: {(time_end - time_start) / len(path_red_ds)}\")\n",
    "           \n",
    "    def np_generator(self, path_red_ds):\n",
    "        for row in path_red_ds:\n",
    "            yield row\n",
    "            \n",
    "            \n",
    "    def augment_event(self, path, redundancy_index):\n",
    "        trace, info = self.handler.path_to_trace(path)\n",
    "        fit = self.fitted_dict[path]\n",
    "        augmented_trace = np.empty((3, 6000))\n",
    "        \n",
    "        random_start_index = fit['random_start_index'][int(redundancy_index)]\n",
    "        initial_index = fit['initial_index']\n",
    "        interesting_part_length = trace.shape[1] - initial_index\n",
    "        missing_length = (augmented_trace.shape[1] - random_start_index) - interesting_part_length\n",
    "        \n",
    "        for i in range(augmented_trace.shape[0]):\n",
    "            augmented_trace[i] = self.fill_start(trace, augmented_trace, random_start_index, initial_index, i)\n",
    "            augmented_trace[i] = self.fill_interesting_part(trace, augmented_trace, random_start_index, interesting_part_length, initial_index, i)\n",
    "            if missing_length > 0:\n",
    "                # missing_length was intereting_part_length. Why? Error?\n",
    "                augmented_trace[i] = self.fill_lacking_ends(trace, augmented_trace, random_start_index, missing_length, i)\n",
    "        return augmented_trace\n",
    "    \n",
    "    def fill_start(self, trace, augmented_trace, random_start_index, initial_index, i_channel):\n",
    "        if random_start_index < initial_index:\n",
    "            augmented_trace[i_channel][0:random_start_index] = trace[i_channel][0:random_start_index]\n",
    "            return augmented_trace[i_channel]\n",
    "        else:\n",
    "            augmented_trace[i_channel][0:initial_index] = trace[i_channel][0:initial_index]\n",
    "            trace_interval_start = trace.shape[1] - (random_start_index - initial_index)\n",
    "            trace_interval_end = trace.shape[1]\n",
    "            augmented_trace[i_channel][initial_index:random_start_index] = trace[i_channel][trace_interval_start:trace_interval_end]\n",
    "            return augmented_trace[i_channel]\n",
    "\n",
    "    def fill_interesting_part(self, trace, augmented_trace, random_start_index, interesting_length, initial_index, i_channel):\n",
    "        aug_interval_end = min(random_start_index + interesting_length, augmented_trace.shape[1])\n",
    "        trace_interval_end = min(initial_index + interesting_length, initial_index + (augmented_trace.shape[1] - random_start_index))\n",
    "        augmented_trace[i_channel][random_start_index:aug_interval_end] = trace[i_channel][initial_index:trace_interval_end]\n",
    "        return augmented_trace[i_channel]\n",
    "        \n",
    "    def fill_lacking_ends(self, trace, augmented_trace, random_start_index, missing_length, i_channel):\n",
    "        fill_interval_start = random_start_index\n",
    "        fill_interval_end = random_start_index + missing_length\n",
    "        augmented_trace[i_channel][augmented_trace.shape[1] - missing_length:augmented_trace.shape[1]] = trace[i_channel][fill_interval_start:fill_interval_end]\n",
    "        return augmented_trace[i_channel]\n",
    "    \n",
    "\n",
    "    def find_initial_event_index(self, path):\n",
    "        info = self.path_to_info(path)\n",
    "        start_time = parser.isoparse(info['trace_stats']['starttime']).replace(tzinfo=None)\n",
    "        if info['analyst_pick_time'] != None:\n",
    "            event_time = parser.isoparse(info['analyst_pick_time']).replace(tzinfo=None)\n",
    "            uncertainty = 0\n",
    "        else:\n",
    "            event_time = parser.isoparse(info['est_arrivaltime_arces']).replace(tzinfo=None)\n",
    "            uncertainty = 0\n",
    "            if 'origins' in info:\n",
    "                if 'time_errors' in info['origins'][0]:\n",
    "                    uncertainty = min(float(info['origins'][0]['time_errors']['uncertainty']), 15)\n",
    "        sampling_rate = info['trace_stats']['sampling_rate']\n",
    "        relative_seconds = (event_time - start_time).total_seconds()\n",
    "        # Problem with uncertainty: Some events have very large uncertainty.\n",
    "        # This can be so high that the interesting event could have potentially occured prior to the recording.          \n",
    "        initial_index = max(math.floor((relative_seconds-uncertainty)*sampling_rate),0)\n",
    "        return initial_index, info\n",
    "    \n",
    "    def path_to_info(self, path):\n",
    "        with h5py.File(path, 'r') as dp:\n",
    "            info = np.array(dp.get('event_info'))\n",
    "            # This is a mess, but for some reason it works with this shitty code.\n",
    "            info = str(info)\n",
    "            info = info[2:len(info)-1]\n",
    "            info = json.loads(info)\n",
    "        return info\n",
    "        \n",
    "    \n",
    "    def progress_bar(self, current, total, barLength = 20):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Fitting time augmentor: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "timeAug = TimeAugmentor(handler, full_and_noise_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues will occur if upsampling of explosions or noise is implemented\n",
      "Finished explosionstor: [------------------->] 100 %\n",
      "Finished noiseugmentor: [------------------->] 100 %\n",
      "Finished earthquakesor: [------------------->] 100 %\n",
      "Fit process completed after 13.833172798156738 seconds. Total datapoints fitted: 42461.\n",
      "Average time per datapoint: 0.000325785374771125\n"
     ]
    }
   ],
   "source": [
    "timeAug.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 49.9425354 ,  36.26490021,  21.7385788 , ...,  30.713974  ,\n",
       "         37.65123367,  40.28333664],\n",
       "       [  1.04194868,  -8.6063633 , -28.35959053, ...,  68.97608185,\n",
       "         54.63550949,  47.34148788],\n",
       "       [ 58.01567459,  35.01927185,  30.62397385, ...,  24.47158623,\n",
       "         19.21693802,  15.27928448]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeAug.augment_event(full_ds[0][0], full_ds[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'initial_index': 2400, 'random_start_index': array([1980])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeAug.fitted_dict[full_ds[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "for i in range(len(full_and_noise_ds)):\n",
    "    next(gen)\n",
    "    i += 1\n",
    "print(i)\n",
    "print(len(full_and_noise_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_set = set(full_ds[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = dict.fromkeys(path_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explo_ds = full_ds[full_ds[:,1] == \"explosion\"]\n",
    "earth_ds = full_ds[full_ds[:,1] == \"earthquake\"]\n",
    "noise_ds = full_ds[full_ds[:,1] == \"noise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def fit(self):\n",
    "    time_start = time.time()\n",
    "    path_red_ds = self.ds[:,[0,2]]\n",
    "    len_ds = len(path_red_ds)\n",
    "    _,_,pre_length = self.handler.get_trace_shape_no_cast(ds, False)\n",
    "    post_length = 6000\n",
    "    np.random.seed(0)\n",
    "    print(\"Issues will occur if upsampling of explosions or noise is implemented\")\n",
    "    explo_ds = self.ds[self.ds[:,1] == \"explosion\"]\n",
    "    earth_ds = self.ds[self.ds[:,1] == \"earthquake\"]\n",
    "    noise_ds = self.ds[self.ds[:,1] == \"noise\"]\n",
    "    max_redundancy = max(earth_ds[:,2])\n",
    "    fitted_dict = dict.fromkeys(set(self.ds[:,0]))\n",
    "    if len(explo_ds) > 0:\n",
    "        i = 0\n",
    "        explo_len = len(set(earth_ds[:,0]))\n",
    "        for path in set(explo_ds[:,0]):\n",
    "            random_start_index = np.random.randint(0,4500, 1)\n",
    "            initial_index, info = self.find_initial_event_index(path)\n",
    "            fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                 'random_start_index' : random_start_index}\n",
    "            self.progress_bar(i + 1, explo_len)\n",
    "            i += 1\n",
    "    print(\"Finished explosions\")\n",
    "    if len(noise_ds) > 0:\n",
    "        i = 0\n",
    "        noise_len = len(set(noise_ds[:,0]))\n",
    "        for path in set(noise_ds[:,0]):\n",
    "            random_start_index = np.random.randint(0,4500, 1)\n",
    "            initial_index, info = self.find_initial_event_index(path)\n",
    "            fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                 'random_start_index' : random_start_index}\n",
    "            self.progress_bar(i + 1, noise_len)\n",
    "            i += 1\n",
    "    print(\"Finished noise\")\n",
    "    if len(earth_ds) > 0:\n",
    "        i = 0\n",
    "        earth_len = len(set(earth_ds[:,0]))\n",
    "        for path in set(earth_ds[:,0]):\n",
    "            random_start_index = np.random.randint(0,4500, max_redundancy)\n",
    "            initial_index, info = self.find_initial_event_index(path)\n",
    "            fitted_dict[path] = {'initial_index': initial_index,\n",
    "                                 'random_start_index' : random_start_index}\n",
    "            self.progress_bar(i + 1, earth_len)\n",
    "            i += 1\n",
    "            \n",
    "    print(\"Finished earthquakes\")\n",
    "    self.fitted_dict = fitted_dict\n",
    "    time_end = time.time()\n",
    "    print(f\"Fit process completed after {time_end - time_start} seconds. Total datapoints fitted: {len(path_red_ds)}.\")\n",
    "    print(f\"Average time per datapoint: {(time_end - time_start) / len(path_red_ds)}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = fit(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(ds_fitted_to):\n",
    "    paths = ds_fitted_to[:,0]\n",
    "    reds = ds_fitted_to[:,2]\n",
    "    path_max = np.empty((len(set(paths)),2), dtype=object)\n",
    "    for idx, path in enumerate(set(paths)):\n",
    "        path_max[idx][0] = path\n",
    "        path_max[idx][1] = max(np.array(ds_fitted_to[ds_fitted_to[:,0] == path][:,2], dtype=int))\n",
    "    for i, p_m in enumerate(path_max):\n",
    "        path = p_m[0]\n",
    "        m = int(p_m[1])\n",
    "        assert len(timeAug.fitted_dict[path]['random_start_index']) == m + 1, f\"{timeAug.fitted_dict[path]['random_start_index']} should have length {m +1}, where entry is {ds_fitted_to[i]}\"\n",
    "\n",
    "sanity_check(full_and_noise_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(timeAug.fitted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "i = 0\n",
    "for path, label, redundency_index in full_and_noise_ds:\n",
    "    _ = timeAug.augment_event(path, redundency_index)\n",
    "    i += 1\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_and_noise_ds[np.where(full_and_noise_ds[:,2] == max(full_and_noise_ds[:,2]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeAug.fitted_dict[full_and_noise_ds[np.where(full_and_noise_ds[:,2] == max(full_and_noise_ds[:,2]))][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_initial_event_index(path):\n",
    "    trace, info = handler.path_to_trace(path)\n",
    "    start_time = parser.isoparse(info['trace_stats']['starttime']).replace(tzinfo=None)\n",
    "    if info['analyst_pick_time'] != None:\n",
    "        event_time = parser.isoparse(info['analyst_pick_time']).replace(tzinfo=None)\n",
    "    else:\n",
    "        event_time = parser.isoparse(info['est_arrivaltime_arces']).replace(tzinfo=None)\n",
    "    sampling_rate = info['trace_stats']['sampling_rate']\n",
    "    relative_seconds = (event_time - start_time).total_seconds()\n",
    "    # Problem with uncertainty: Some events have very large uncertainty.\n",
    "    # This can be so high that the interesting event could have potentially occured prior to the recording.\n",
    "    if 'time_errors' in info['origins'][0]:\n",
    "        uncertainty = float(info['origins'][0]['time_errors']['uncertainty'])\n",
    "    else:\n",
    "        uncertainty = 0\n",
    "    initial_index = max(math.floor((relative_seconds-uncertainty)*sampling_rate),0)\n",
    "\n",
    "    return initial_index, trace, info\n",
    "\n",
    "def shift_event(path):\n",
    "    initial_index, trace, info = find_initial_event_index(path)\n",
    "    pre_length = trace.shape[1]\n",
    "    random_start_index = np.random.randint(0, 5000)\n",
    "    augmented_trace = np.empty((3, 6000))\n",
    "    interesting_part_length = pre_length - initial_index\n",
    "    # Handling what happens when the duration of the interesting event is shorter than what is needed to fill the array:\n",
    "    ideal_length = augmented_trace.shape[1] - random_start_index\n",
    "    missing_length = ideal_length - interesting_part_length\n",
    "    if missing_length > 0:\n",
    "        filler_index_start = np.random.randint(0, (initial_index - missing_length))\n",
    "        filler_index_end = filler_index_start + missing_length\n",
    "        # First index of what requires more filling\n",
    "        required_fill_index_start = augmented_trace.shape[1] - missing_length\n",
    "    \n",
    "    for i in range(augmented_trace.shape[0]):\n",
    "        augmented_trace[i][0:random_start_index] = trace[i][0:random_start_index]\n",
    "        augmented_trace[i][random_start_index:random_start_index + interesting_part_length] = trace[i][initial_index: initial_index + (augmented_trace.shape[1] - random_start_index)]\n",
    "        if missing_length > 0:\n",
    "            augmented_trace[i][required_fill_index_start:augmented_trace.shape[1]] = trace[i][filler_index_start:filler_index_end]\n",
    "\n",
    "    return augmented_trace, info\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    train_aug.augment_event(train_aug.ds[i][0])\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "_, info = handler.path_to_trace(train_ds[i][0])\n",
    "trace = timeAug.augment_event(train_ds[i][0])\n",
    "trace = scaler.transform(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_event(trace, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(no_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_trace, shift_info = shift_event(broken_eq)\n",
    "print(shift_trace.shape)\n",
    "plot_event(shift_trace, shift_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_index, trace, info = find_initial_event_index(broken_eq)\n",
    "print(initial_index)\n",
    "pprint.pprint(info)\n",
    "start_time = parser.isoparse(info['trace_stats']['starttime']).replace(tzinfo=None)\n",
    "event_time = parser.isoparse(info['est_arrivaltime_arces']).replace(tzinfo=None)\n",
    "\n",
    "relative_event_time = event_time - start_time\n",
    "relative_seconds = relative_event_time.total_seconds()\n",
    "initial_index = math.floor(relative_seconds*sampling_rate)\n",
    "print(initial_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace, info = handler.path_to_trace(some_eq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_event(trace, info):\n",
    "    start_time = info['origins'][0]['time']\n",
    "    channels = info['trace_stats']['channels']\n",
    "    sampl_rate = info['trace_stats']['sampling_rate']\n",
    "    station = info['trace_stats']['station']\n",
    "    \n",
    "    trace_BHE = Trace(\n",
    "    data=trace[0],\n",
    "    header={\n",
    "        'station': station,\n",
    "        'channel': channels[0],\n",
    "        'sampling_rate': sampl_rate,\n",
    "        'starttime': start_time})\n",
    "    trace_BHN = Trace(\n",
    "        data=trace[1],\n",
    "        header={\n",
    "            'station': station,\n",
    "            'channel': channels[1],\n",
    "            'sampling_rate': sampl_rate, \n",
    "            'starttime': start_time})\n",
    "    trace_BHZ = Trace(\n",
    "        data=trace[2],\n",
    "        header={\n",
    "            'station': station,\n",
    "            'channel': channels[2],\n",
    "            'sampling_rate': sampl_rate,\n",
    "            'starttime': start_time})\n",
    "    stream = Stream([trace_BHE, trace_BHN, trace_BHZ])\n",
    "    stream.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_trace, eq_info = handler.path_to_trace(broken_eq)\n",
    "plot_event(eq_trace, eq_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "#pprint.pprint(eq_info)\n",
    "\n",
    "start_time = parser.isoparse(eq_info['origins'][0]['time']).replace(tzinfo=None)\n",
    "print(start_time)\n",
    "event_time = parser.isoparse(eq_info['est_arrivaltime_arces'])\n",
    "print(event_time)\n",
    "sampling_rate = eq_info['trace_stats']['sampling_rate']\n",
    "\n",
    "relative_event_time = event_time - start_time\n",
    "relative_seconds = relative_event_time.total_seconds()\n",
    "initial_index = relative_seconds*sampling_rate\n",
    "print(initial_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Which time should i look at?\n",
    " - The time uncertainty is in seconds, right?\n",
    " - How long does earthquakes recorded by ARCES normally last?\n",
    " - Some uncertainties are so high that the event could have occured prior to recording. For events such as this I assume that the event starts at index = 0.\n",
    " - Some events lack uncertainty measure.\n",
    " - What does est_arrivaltime_arces actually mean? Assumed it meant event_start_time but then I saw noise has the same statistic.\n",
    " - Can I use noise augmentation when training a model which does not classify noise?\n",
    " - How do I draw vertical lines in Obspy?\n",
    " \n",
    "For Pekka:\n",
    "  - Can I use noise augmentation when training a model which does not classify noise?\n",
    "  - Is noise augmentation even necessary anymore now that the time augmentation is here? \n",
    "  - Should I keep the test/validation set clear of events that are in the train set, even though their augmentation makes them different?\n",
    "  - Do I need to make my upsampling/downsampling more sophisticated in order to balance the MSRD balanced?\n",
    "  - Is using the large dataset worth it anymore?\n",
    "      - Downsides:\n",
    "          - Time consuming\n",
    "          - Samples are very different from ACRES\n",
    "              - No beamforming\n",
    "              - Repeating channels\n",
    "              - No explosion samples!\n",
    "              - Destroys Noise Augmentor\n",
    "          - Limited ROI\n",
    "      - Upsides:\n",
    "          - More real data\n",
    "          - Metrics more reflective of reality, not impacted by the time augmentation process.\n",
    "  - Do I need to write about why this work is important?\n",
    "      - If so, why do you think it is important?\n",
    "  - Making sure: Can report performances of the models prior to this new dataset and compare them to performance after the time augmentation implementation.\n",
    "  - Do you have any suggestions of areas of improvement beyond what is already implemented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "     Likely depreciated due to new fitting method.\n",
    "         \n",
    "    def find_initial_event_index(self, path):\n",
    "        trace, info = self.handler.path_to_trace(path)\n",
    "        start_time = parser.isoparse(info['trace_stats']['starttime']).replace(tzinfo=None)\n",
    "        if info['analyst_pick_time'] != None:\n",
    "            event_time = parser.isoparse(info['analyst_pick_time']).replace(tzinfo=None)\n",
    "        else:\n",
    "            event_time = parser.isoparse(info['est_arrivaltime_arces']).replace(tzinfo=None)\n",
    "        sampling_rate = info['trace_stats']['sampling_rate']\n",
    "        relative_seconds = (event_time - start_time).total_seconds()\n",
    "        # Problem with uncertainty: Some events have very large uncertainty.\n",
    "        # This can be so high that the interesting event could have potentially occured prior to the recording.\n",
    "        if 'time_errors' in info['origins'][0]:\n",
    "            uncertainty = float(info['origins'][0]['time_errors']['uncertainty'])\n",
    "        else:\n",
    "            uncertainty = 0\n",
    "        initial_index = max(math.floor((relative_seconds-uncertainty)*sampling_rate),0)\n",
    "\n",
    "        return initial_index, trace, info\n",
    "\n",
    "    def shift_event(self, path):\n",
    "        initial_index, trace, info = self.find_initial_event_index(path)\n",
    "        pre_length = trace.shape[1]\n",
    "        random_start_index = np.random.randint(0, 5000)\n",
    "        augmented_trace = np.empty((3, 6000))\n",
    "        interesting_part_length = pre_length - initial_index\n",
    "        # Handling what happens when the duration of the interesting event is shorter than what is needed to fill the array:\n",
    "        ideal_length = augmented_trace.shape[1] - random_start_index\n",
    "        missing_length = ideal_length - interesting_part_length\n",
    "        if missing_length > 0:\n",
    "            filler_index_start = np.random.randint(0, (initial_index - missing_length))\n",
    "            filler_index_end = filler_index_start + missing_length\n",
    "            # First index of what requires more filling\n",
    "            required_fill_index_start = augmented_trace.shape[1] - missing_length\n",
    "\n",
    "        for i in range(augmented_trace.shape[0]):\n",
    "            augmented_trace[i][0:random_start_index] = trace[i][0:random_start_index]\n",
    "            augmented_trace[i][random_start_index:random_start_index + interesting_part_length] = trace[i][initial_index: initial_index + (augmented_trace.shape[1] - random_start_index)]\n",
    "            if missing_length > 0:\n",
    "                augmented_trace[i][required_fill_index_start:augmented_trace.shape[1]] = trace[i][filler_index_start:filler_index_end]\n",
    "        return augmented_trace\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1,1,1,1,1]\n",
    "b = [2,2,2]\n",
    "a[0:len(b)] = b\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['a',3],['b',6],['b',1],['b',5], ['b', 0],['a',1]]\n",
    "\n",
    "\n",
    "\n",
    "def fit(a):\n",
    "    temp = {}\n",
    "    for path, red in a:\n",
    "        if path in temp:\n",
    "            if red+1 < len(temp[path]):\n",
    "                continue\n",
    "            else:\n",
    "                temp[path] = np.random.randint(0,5, red+1)\n",
    "        else:\n",
    "            temp[path] = np.random.randint(0,5,red+1)\n",
    "    return temp\n",
    "fit(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
