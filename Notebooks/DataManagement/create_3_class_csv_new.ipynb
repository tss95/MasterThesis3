{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py as h5\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = 'F:'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.path.join('F:\\\\', 'Thesis_ssd', 'norsar_data_nov')\n",
    "csv_folder = os.path.join('F:\\\\', 'Thesis_ssd','MasterThesis3.0','csv_folder')\n",
    "explo_path = os.path.join(root, 'explosions')\n",
    "earth_path = os.path.join(root, 'earthquakes')\n",
    "noise_path = os.path.join(root, 'noise')\n",
    "induced_path = os.path.join(root, 'induced')\n",
    "data_csv = 'event_paths_no_nan_no_induced.csv'\n",
    "balanced_csv = 'balanced_csv_3_class.csv'\n",
    "test_set_csv = 'DO_NOT_TOUCH_test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balanced_csv_2_class.csv',\n",
       " 'balanced_csv_3_class.csv',\n",
       " 'event_paths_no_nan_no_induced.csv',\n",
       " 'event_paths_no_nan_no_induced_no_explosions.csv',\n",
       " '.ipynb_checkpoints',\n",
       " '2_classes',\n",
       " '3_classes',\n",
       " '4_classes']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = os.path.join('F:\\\\', 'Thesis_ssd', 'norsar_data_nov')\n",
    "os.listdir(csv_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bypassing issues\n",
    " - Due to either dataset issues or ignorance, I was forced to label the dataset by their file names, rather than event info.\n",
    " - The dataset contains a lot of NaN values, and therefore need to have a method for omitting those datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_event_csv(root, csv_name, output_folder = csv_folder):\n",
    "    with open(os.path.join(output_folder, csv_name), 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ' ', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        for idx, event_type in enumerate(os.listdir(root)):\n",
    "            if event_type == 'induced':\n",
    "                print(f\"Skipped {event_type}\")\n",
    "                continue\n",
    "            event_list = os.listdir(os.path.join(root,event_type))\n",
    "            event_type_total_nr = len(event_list)\n",
    "            for idx_by_event, event in enumerate(event_list):\n",
    "                path = os.path.join(root, event_type, event)\n",
    "                trace_array, label, info = path_to_trace(path)\n",
    "                progress_bar(idx_by_event + 1, event_type_total_nr, label)\n",
    "                if np.any(pd.isnull(trace_array)):\n",
    "                    print(f\"NaN values in (label, file name) {label, event}\")\n",
    "                    continue\n",
    "                if label == \"induced or triggered event\":\n",
    "                    print(\"If you're reading this, someone has a lot of work to do.\")\n",
    "                    continue\n",
    "                else:\n",
    "                    writer.writerow([f'{path},{label}'])\n",
    "                \n",
    "            print(f'Completed {event_type}                                            ')\n",
    "    print(\"Completed \" + csv_name)\n",
    "    \n",
    "def csv_to_numpy(data_csv, csv_folder = csv_folder):\n",
    "    with open(csv_folder + '/' + data_csv) as file:\n",
    "        file_list = np.array(list(file))\n",
    "        dataset = np.empty((len(file_list), 2), dtype=object)\n",
    "        for idx, event in enumerate(file_list):\n",
    "            path, label = event.split(',')\n",
    "            dataset[idx][0] = path.rstrip()\n",
    "            dataset[idx][1] = label.rstrip()\n",
    "        file.close()\n",
    "    return dataset\n",
    "    \n",
    "def path_to_trace(path):\n",
    "    with h5py.File(path, 'r') as dp:\n",
    "        trace_array = np.array(dp.get('traces'))\n",
    "        info = np.array(dp.get('event_info'))\n",
    "        info = json.loads(str(info))\n",
    "    # No event type for noise, so handling that below\n",
    "    if path.split('\\\\')[2] == 'noise':\n",
    "        label = 'noise'\n",
    "    else:\n",
    "        label = info['event_type']\n",
    "    # Since we consider induced earthquakes as earthquakes we need to handle that as well:\n",
    "    if label == \"induced or triggered event\":\n",
    "        label = \"earthquake\"\n",
    "    return trace_array, label, info\n",
    "\n",
    "def assert_true_labels(ds):\n",
    "    for path, label in ds:\n",
    "        _, label_from_trace, _ = path_to_trace(path)\n",
    "        if label_from_trace != 'noise':\n",
    "            label_from_trace = label_from_trace + 's'\n",
    "            label = label + 's'\n",
    "        assert path.split('/')[1] == label == label_from_trace, f'Mismatch between {path.split(\"/\")[1]} and {label} and {label_from_trace}, at path: {path}'\n",
    "\n",
    "\n",
    "def get_class_distribution_from_csv(data_csv, csv_folder = csv_folder):\n",
    "    with open(csv_folder + '/' + data_csv) as file:\n",
    "        nr_earthquakes = 0\n",
    "        nr_explosions = 0\n",
    "        nr_noise = 0\n",
    "        nr_total = 0\n",
    "        for row in file:\n",
    "            event_type = row.split(',')[1].rstrip()\n",
    "            if event_type == \"earthquake\":\n",
    "                nr_earthquakes += 1\n",
    "            elif event_type == \"explosion\":\n",
    "                nr_explosions += 1\n",
    "            elif event_type == \"noise\":            \n",
    "                nr_noise += 1\n",
    "            nr_total += 1\n",
    "        \n",
    "        return nr_earthquakes, nr_explosions, nr_noise, nr_total\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def even_classes_csv(data_csv, balanced_csv_name, output_folder = csv_folder):\n",
    "    nr_earthquakes, nr_explosions, nr_noise, nr_total = get_class_distribution_from_csv(data_csv)\n",
    "    min_class_nr = min([nr_earthquakes, nr_explosions, nr_noise])\n",
    "    print(min_class_nr)\n",
    "    csv_numpy = csv_to_numpy(data_csv)\n",
    "    class_count = [0,0,0,0]\n",
    "    earthquake_list = []\n",
    "    explosion_list = []\n",
    "    noise_list = []\n",
    "    final_class_count = []\n",
    "    for path, label in csv_numpy:\n",
    "        if label == \"explosion\":\n",
    "            explosion_list.append([path,label])\n",
    "            class_count[0] += 1\n",
    "            class_count[3] += 1\n",
    "        if label == \"earthquake\":\n",
    "            earthquake_list.append([path,label])\n",
    "            class_count[1] += 1\n",
    "            class_count[3] += 1\n",
    "        if label == \"noise\":\n",
    "            noise_list.append([path,label])\n",
    "            class_count[2] += 1\n",
    "            class_count[3] += 1\n",
    "    pure_classes = [earthquake_list, explosion_list, noise_list]\n",
    "    print(len(pure_classes[0]))\n",
    "    with open(output_folder + '/' + balanced_csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ' ', quotechar = '|', quoting = csv.QUOTE_MINIMAL)\n",
    "        for single_class in pure_classes:\n",
    "            print(f'Sample_size = {min_class_nr} single_class length: {len(single_class)}' )\n",
    "            random_samples = random.sample(single_class, min_class_nr)\n",
    "            final_class_count.append(len(random_samples))\n",
    "            for path, label in random_samples:\n",
    "                writer.writerow([f'{path},{label}'])\n",
    "        print(final_class_count)\n",
    "\n",
    "def create_true_test_set(full_ds):\n",
    "    ds = full_ds\n",
    "    np.random.shuffle(ds)\n",
    "    train_val, test = train_test_split(ds, test_size = 0.075)\n",
    "    return test\n",
    "        \n",
    "def generate_subset_csv(input_ds, output_csv_name, output_folder):\n",
    "    nr_rows = len(input_ds)\n",
    "    with open(output_folder + '/' + output_csv_name, 'w+', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter = ',', quotechar = ' ', quoting = csv.QUOTE_MINIMAL)\n",
    "        for row in input_ds:\n",
    "            writer.writerow(row)\n",
    "    print(f'Completed writing {nr_rows} rows to {output_folder}/{output_csv_name}.')\n",
    "        \n",
    "\n",
    "def downsample_label(target_label, ds, n_samples):\n",
    "    target_array = np.array([x for x in ds if x[1] == target_label], dtype = object)\n",
    "    down_ds = np.array([y for y in ds if y[1] != target_label], dtype = object)\n",
    "    down_ds = np.concatenate((down_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "    return np.array(down_ds)\n",
    "\n",
    "def upsample_label(target_label, ds, n_samples):\n",
    "    target_array = np.array([x for x in ds if x[1] == target_label])\n",
    "    up_ds = [y for y in ds if y[1] != target_label]\n",
    "    up_ds = np.concatenate((up_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "    return np.array(up_ds)\n",
    "\n",
    "def frac_diff_n_samples(frac_diff, min_counts, max_counts):\n",
    "    diff = max_counts - min_counts\n",
    "    n_samples = int(min_counts + diff*frac_diff)\n",
    "    return n_samples\n",
    "\n",
    "def balance_ds(ds, downsample, upsample, frac_diff = 0):\n",
    "    unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "    nr_classes = len(unique_labels)\n",
    "    if downsample:\n",
    "        # Downsamples by first reducing the largest class, then the second class.\n",
    "        for i in range(nr_classes-1):\n",
    "            print(i)\n",
    "            unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "            most_occuring_label = unique_labels[np.where(counts == max(counts))]\n",
    "            n_samples_frac_diff = frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "            ds = downsample_label(most_occuring_label, ds, n_samples_frac_diff)\n",
    "    if upsample:\n",
    "        #\n",
    "        unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "        least_occuring_label = unique_labels[np.where(counts == min(counts))]\n",
    "        n_samples_for_balance = max(counts)\n",
    "        ds = upsample_label(least_occuring_label, ds, n_samples_for_balance)\n",
    "    np.random.shuffle(ds)\n",
    "    return ds\n",
    "        \n",
    "        \n",
    "def progress_bar(current, total, event_type, barLength = 20):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Writing %ss [%s%s] %d %%' % (event_type, arrow, spaces, percent), end='\\r')\n",
    "                      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_event_csv(root, data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquakes: 9464, Explosions: 111173, Noise: 114578, Total: 235215\n"
     ]
    }
   ],
   "source": [
    "nr_eq, nr_ex, nr_noise, nr_total = get_class_distribution_from_csv(data_csv)\n",
    "print(f'Earthquakes: {nr_eq}, Explosions: {nr_ex}, Noise: {nr_noise}, Total: {nr_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = csv_to_numpy(data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = create_true_test_set(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['earthquake', 'explosion', 'noise'], dtype=object), array([ 698, 8365, 8579], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(test_set[:,1], return_counts = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing 217573 rows to F:\\Thesis_ssd\\MasterThesis3.0\\csv_folder/full_no_test.csv.\n"
     ]
    }
   ],
   "source": [
    "generate_subset_csv(full_ds_no_test, \"full_no_test.csv\", csv_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6852\n",
      "6852\n",
      "Sample_size = 6852 single_class length: 6852\n",
      "Sample_size = 6852 single_class length: 107786\n",
      "Sample_size = 6852 single_class length: 91612\n",
      "[6852, 6852, 6852]\n"
     ]
    }
   ],
   "source": [
    "# Generate balanced data csv using data_csv:\n",
    "even_classes_csv(data_csv, balanced_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = csv_to_numpy(balanced_csv)\n",
    "\n",
    "train_ds, test_val_ds = train_test_split(ds, test_size = 0.2)\n",
    "val_ds, test_ds = train_test_split(test_val_ds, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16444 2878 1234\n",
      "20556\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "print(len(train_ds) + len(val_ds) + len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_true_labels(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating balanced splitted csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed writing 1234 rows to C:\\Documents\\Thesis_ssd\\MasterThesis-2.0\\csv_folder/csv_folder_3_class/balanced/test_set.csv.\n",
      "Completed writing 2878 rows to C:\\Documents\\Thesis_ssd\\MasterThesis-2.0\\csv_folder/csv_folder_3_class/balanced/validation_set.csv.\n",
      "Completed writing 16444 rows to C:\\Documents\\Thesis_ssd\\MasterThesis-2.0\\csv_folder/csv_folder_3_class/balanced/train_set.csv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_sub = 'balanced'\n",
    "output_folder = f\"{csv_folder}/csv_folder_3_class/{root_sub}\"\n",
    "\n",
    "# Test set:\n",
    "output_csv_name = \"test_set.csv\"\n",
    "generate_subset_csv(test_ds, output_csv_name, output_folder)\n",
    "\n",
    "# Validation set:\n",
    "output_csv_name = \"validation_set.csv\"\n",
    "generate_subset_csv(val_ds, output_csv_name, output_folder)\n",
    "\n",
    "# Train set:\n",
    "output_csv_name = \"train_set.csv\"\n",
    "generate_subset_csv(train_ds, output_csv_name, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/test_set.csv'))\n",
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/validation_set.csv'))\n",
    "assert_true_labels(csv_to_numpy('csv_folder_3_class/balanced/train_set.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'event_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3cb166ffc9d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtime_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0meq_trace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meq_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meq_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath_to_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{os.path.join(noise_path, '2015-06-02T12.51.52.000000Z.h5')}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_end\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2ca8bfcb914e>\u001b[0m in \u001b[0;36mpath_to_trace\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'noise'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;31m# Since we consider induced earthquakes as earthquakes we need to handle that as well:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"induced or triggered event\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'event_type'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "time_start = time.time()\n",
    "eq_trace, eq_label, eq_info = path_to_trace(f\"{os.path.join(noise_path, '2015-06-02T12.51.52.000000Z.h5')}\")\n",
    "time_end = time.time()\n",
    "print(time_end - time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyst_pick_time': None,\n",
      " 'az_to_arces': 54.610377749207366,\n",
      " 'baz_to_arces': 253.12745291713466,\n",
      " 'comments': [{'creation_info': {'author': 'general'},\n",
      "               'resource_id': 'smi:local/563d926d-299d-4672-92c0-fc7232966ee4',\n",
      "               'text': 'PROBABLY EARTHQUAKE'}],\n",
      " 'dist_to_arces': 941.1384693251722,\n",
      " 'est_arrivaltime_arces': '1991-01-13 02:24:29.890000',\n",
      " 'event_type': 'earthquake',\n",
      " 'event_type_certainty': 'suspected',\n",
      " 'magnitude_dist_ratio': 0.0036126458654250036,\n",
      " 'magnitude_sqrtdist_ratio': 0.11082867833934054,\n",
      " 'magnitudes': [{'creation_info': {'agency_id': 'HEL'},\n",
      "                 'mag': 3.4,\n",
      "                 'magnitude_type': 'ML',\n",
      "                 'origin_id': 'smi:local/0dc10fdd-f584-4a02-81f8-42ab1875b841',\n",
      "                 'resource_id': 'smi:local/18b449b8-b1e1-4bdf-baed-431b5a7d3508',\n",
      "                 'station_count': 38}],\n",
      " 'origins': [{'creation_info': {'agency_id': 'HEL'},\n",
      "              'depth': 15.0,\n",
      "              'latitude': 65.759,\n",
      "              'longitude': 5.528,\n",
      "              'resource_id': 'smi:local/0dc10fdd-f584-4a02-81f8-42ab1875b841',\n",
      "              'time': '1991-01-13T02:22:05.100000Z'}],\n",
      " 'preferred_magnitude_id': 'smi:local/18b449b8-b1e1-4bdf-baed-431b5a7d3508',\n",
      " 'preferred_origin_id': 'smi:local/0dc10fdd-f584-4a02-81f8-42ab1875b841',\n",
      " 'resource_id': 'smi:local/923c524d-d995-4612-a091-5008fc4e72c6',\n",
      " 'trace_stats': {'channels': ['P-beam, vertical',\n",
      "                              'S-beam, transverse',\n",
      "                              'S-beam, radial'],\n",
      "                 'sampling_rate': 40.0,\n",
      "                 'starttime': '1991-01-13T02:23:29.897009Z',\n",
      "                 'station': 'ARCES beam'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(eq_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{os.path.join(noise_path, '2015-06-02T12.51.52.000000Z.h5')}\"\n",
    "\n",
    "def open_file(path):\n",
    "    with h5py.File(path, 'r') as dp:\n",
    "        trace_array = np.array(dp.get('traces'))\n",
    "        info = np.array(dp.get('event_info'))\n",
    "        info = json.loads(str(info))\n",
    "        print(path.split(\"\\\\\")[2])\n",
    "    return trace_array, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise\n"
     ]
    }
   ],
   "source": [
    "trace, info = open_file(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'est_arrivaltime_arces': '2015-06-02 12:52:52',\n",
       " 'baz_to_arces': 0.9887103208968245,\n",
       " 'analyst_pick_time': None,\n",
       " 'trace_stats': {'starttime': '2015-06-02T12:51:52.000000Z',\n",
       "  'sampling_rate': 40.0,\n",
       "  'station': 'ARCES beam',\n",
       "  'channels': ['P-beam, vertical', 'S-beam, transverse', 'S-beam, radial']}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235215 17642\n"
     ]
    }
   ],
   "source": [
    "print(len(full_ds), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_full = np.concatenate((full_ds[0:100], test_set))\n",
    "mock_test = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_mock = np.array([x for x in mock_full if x not in mock_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reduced_mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_test_set_from_full_ds(full_ds, test_set):\n",
    "    return [x for x in full_ds if x[0] not in test_set[:,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds_no_test = remove_test_set_from_full_ds(full_ds, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217573"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_ds_no_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['F:/Thesis_ssd\\\\norsar_data_nov\\\\explosions\\\\2007-02-25T00.20.28.832000Z.h5',\n",
       "       'explosion'], dtype=object)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217573\n"
     ]
    }
   ],
   "source": [
    "print(len(full_ds)- len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
