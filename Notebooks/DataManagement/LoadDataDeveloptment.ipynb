{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = 'F:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData():\n",
    "    \n",
    "    def __init__(self, earth_explo_only = False, noise_earth_only = False, downsample = False, upsample = False, frac_diff = 1, seed = None, subsample_size = 1):\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.earth_explo_only = earth_explo_only\n",
    "        self.noise_earth_only = noise_earth_only\n",
    "        self.downsample = downsample\n",
    "        self.upsample = upsample\n",
    "        self.frac_diff = frac_diff\n",
    "        self.subsample_size = subsample_size\n",
    "        \n",
    "        self.csv_folder = os.path.join('F:\\\\', 'Thesis_ssd','MasterThesis3.0','csv_folder')\n",
    "        self.data_csv_name = 'event_paths_no_nan_no_induced.csv'\n",
    "        self.full_ds = self.csv_to_numpy(self.data_csv_name, self.csv_folder)\n",
    "        \n",
    "        \n",
    "        # WARNING: If you ever want to do Noise + Explosion only then you CANNOT set downsample = True. \n",
    "        # This will not produce the desired results\n",
    "        \n",
    "        if downsample or upsample:\n",
    "            self.full_ds = self.balance_ds(self.full_ds, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        else:\n",
    "            np.random.seed(self.seed)\n",
    "            np.random.shuffle(self.full_ds)\n",
    "        #self.map_redundancy(self.full_ds)   \n",
    "        \n",
    "        # Reduce sample size if needed:\n",
    "        self.full_ds = self.full_ds[0:int(len(self.full_ds)*self.subsample_size)]\n",
    "        \n",
    "        # Remove uninteresting label from ds but keep noise seperately if removed\n",
    "        self.refine_full_ds()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Only need to map redundency if upsampling, as upsampling is the cause of redundancy\n",
    "        if self.upsample:\n",
    "            self.full_ds = self.map_redundancy(self.full_ds)\n",
    "        else:\n",
    "            zero_column = np.zeros((len(self.full_ds), 1))\n",
    "            self.full_ds = np.hstack((self.full_ds, zero_column))\n",
    "        \n",
    "        # The noise needs to be reduced in order to work properly in noise augmentor (creating training set for noise augmentor).\n",
    "        if self.earth_explo_only:\n",
    "            self.noise_ds, _ = train_test_split(self.noise_ds, test_size = 0.15, random_state = self.seed)\n",
    "        \n",
    "        \n",
    "        self.train, val_test = train_test_split(self.full_ds, test_size = 0.15, random_state = self.seed)\n",
    "        self.val, self.test = train_test_split(val_test, test_size = 0.5, random_state = self.seed)\n",
    "        \n",
    "        # Need to seperate noise for noise augmentor\n",
    "        if not self.earth_explo_only:\n",
    "            self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "        \n",
    "        # Make noise_ds match shape of the rest\n",
    "        zero_column = np.zeros((len(self.noise_ds), 1))\n",
    "        self.noise_ds = np.hstack((self.noise_ds, zero_column))\n",
    "        \n",
    "        if self.earth_explo_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'explosion' : 1}\n",
    "        elif self.noise_earth_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1}\n",
    "        else:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1, 'explosion' : 2, 'induced' : 3}\n",
    "    \n",
    "    def refine_full_ds(self):\n",
    "        if self.earth_explo_only or self.noise_earth_only:\n",
    "            if self.earth_explo_only:\n",
    "                self.noise_ds = np.array(self.full_ds[self.full_ds[:,1] == \"noise\"])\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"noise\"])\n",
    "            if self.noise_earth_only:\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"explosion\"])\n",
    "        if self.earth_explo_only and self.noise_earth_only:\n",
    "            raise Exception(\"Cannot have both earth_explo_only = True and noise_earth_only = True\")\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        return self.full_ds, self.train, self.val, self.test  \n",
    "        \n",
    "    def csv_to_numpy(self, data_csv, csv_folder):\n",
    "        with open(csv_folder + '/' + data_csv) as file:\n",
    "            file_list = np.array(list(file))\n",
    "            dataset = np.empty((len(file_list), 2), dtype=object)\n",
    "            for idx, event in enumerate(file_list):\n",
    "                path, label = event.split(',')\n",
    "                dataset[idx][0] = path.rstrip()\n",
    "                dataset[idx][1] = label.rstrip()\n",
    "            file.close()\n",
    "        return dataset\n",
    "    \n",
    "    def downsample_label(self, target_label, ds, n_samples):\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label], dtype = object)\n",
    "        down_ds = np.array([y for y in ds if y[1] != target_label], dtype = object)\n",
    "        np.random.seed(self.seed)\n",
    "        down_ds = np.concatenate((down_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(down_ds)\n",
    "\n",
    "    def upsample_label(self, target_label, ds, n_samples):\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label])\n",
    "        up_ds = [y for y in ds if y[1] != target_label]\n",
    "        np.random.seed(self.seed)\n",
    "        up_ds = np.concatenate((up_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(up_ds)\n",
    "\n",
    "    def frac_diff_n_samples(self, frac_diff, min_counts, max_counts):\n",
    "        diff = max_counts - min_counts\n",
    "        n_samples = int(min_counts + diff*frac_diff)\n",
    "        return n_samples\n",
    "\n",
    "    def balance_ds(self, ds, downsample, upsample, frac_diff = 0):\n",
    "        unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "        nr_classes = len(unique_labels)\n",
    "        if downsample:\n",
    "            # Downsamples by first reducing the largest class, then the second class.\n",
    "            for i in range(nr_classes-1):\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                most_occuring_label = unique_labels[np.where(counts == max(counts))]\n",
    "                n_samples_frac_diff = self.frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "                ds = self.downsample_label(most_occuring_label, ds, n_samples_frac_diff)\n",
    "        if upsample:\n",
    "            if frac_diff != 0:\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                least_occuring_label = unique_labels[np.where(counts == min(counts))]\n",
    "                n_samples_for_balance = max(counts)\n",
    "                ds = self.upsample_label(least_occuring_label, ds, n_samples_for_balance)\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(ds)\n",
    "        return ds\n",
    "    \n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return self.label_dict\n",
    "    \n",
    "    def map_redundancy(self, ds):\n",
    "        # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "        new_column = np.zeros((len(ds), 1), dtype = int)\n",
    "        mapped_ds = np.hstack((ds, new_column))\n",
    "        earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "        unique_earth_paths = set(earth_ds[:,0])\n",
    "        nr_unique_earth_paths = len(unique_earth_paths)\n",
    "        for idx, path in enumerate(unique_earth_paths):\n",
    "            progress_bar(idx + 1, nr_unique_earth_paths)\n",
    "            nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "            label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "            repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "            current_index = 0\n",
    "            if len(repeating_indexes) > 1:\n",
    "                for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                    mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                    mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                    mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                    current_index += 1\n",
    "        return mapped_ds\n",
    "\n",
    "    def progress_bar(self, current, total, barLength = 40):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Mapping redundancy: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "loadData = LoadData(earth_explo_only = True, downsample = True, upsample = True, frac_diff = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_redundancy(ds):\n",
    "    # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "    new_column = np.zeros((len(ds), 1), dtype = int)\n",
    "    mapped_ds = np.hstack((ds, new_column))\n",
    "    earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "    unique_earth_paths = set(earth_ds[:,0])\n",
    "    nr_unique_earth_paths = len(unique_earth_paths)\n",
    "    for idx, path in enumerate(unique_earth_paths):\n",
    "        progress_bar(idx + 1, nr_unique_earth_paths)\n",
    "        nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "        label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "        repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "        current_index = 0\n",
    "        if len(repeating_indexes) > 1:\n",
    "            for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                current_index += 1\n",
    "    return mapped_ds\n",
    "\n",
    "def progress_bar(current, total, barLength = 40):\n",
    "    percent = float(current) * 100 / total\n",
    "    arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "    spaces  = ' ' * (barLength - len(arrow))\n",
    "    print('Mapping redundancy: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "redundancy_ds = map_redundancy(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2003-08-13T22.26.18.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2017-03-14T22.58.39.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2012-03-02T06.10.23.000000Z.h5',\n",
       "        'noise'],\n",
       "       ...,\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2002-08-17T17.39.15.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2015-07-21T23.44.41.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2003-09-07T15.57.12.000000Z.h5',\n",
       "        'noise']], dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadData.noise_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 3], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "ds = np.array([['a', 1],['a',1],['b',0],['a',1],['c',3],['b',0]])\n",
    "print(np.where(ds[ds[:,0] == 'a'][:,0][0] == ds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'a'], dtype='<U1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[ds[:,0] == 'a'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'b', 'a', 'c', 'b'], dtype='<U1')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
