{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigDataLoader:\n",
    "    def __init__(self, upsample = False, downsample = False, frac_diff = 1, test = False, seed = None):\n",
    "        self.upsample = upsample\n",
    "        self.downsample = downsample\n",
    "        self.balance = self.upsample and self.downsample\n",
    "        self.seed = seed\n",
    "        self.test = test\n",
    "        self.source_path = \"F:\\Thesis_ssd\"\n",
    "        \n",
    "        #TODO: Make these two lines more generic\n",
    "        self.filename = f\"{self.source_path}\\LargeDataset\\merge.hdf5\"\n",
    "        self.csv_file = f\"{self.source_path}\\LargeDataset\\merge.csv\"\n",
    "        \n",
    "        self.info_file = self.parse_csv(self.csv_file)\n",
    "        self.data_file = h5py.File(self.filename, 'r',  rdcc_nbytes = 10**9).get('data')\n",
    "        \n",
    "        self.event_names = list(self.data_file.keys())\n",
    "        if self.test:\n",
    "            np.random.shuffle(self.event_names)\n",
    "            self.event_names = self.event_names[0:int(len(self.event_names)*0.02)]\n",
    "        self.name_label = self.create_name_label_array(self.event_names)\n",
    "        self.handler = BigDataHandler(self)\n",
    "        self.balancer = Balancer(self, self.handler)\n",
    "        \n",
    "        if upsample or downsample:\n",
    "            self.name_label = self.balancer.balance_dataset(self.name_label, downsample, upsample, frac_diff = frac_diff)\n",
    "    \n",
    "    def parse_csv(self, csv_file):\n",
    "        col_names = pd.read_csv(csv_file, nrows=0).columns\n",
    "        non_string_or_time = {\n",
    "                      'receiver_latititude' : float,\n",
    "                      'receiver_longitude' : float,\n",
    "                      'p_weight' : float,\n",
    "                      'p_travel_secs' : float,\n",
    "                      'source_latitude' : float,\n",
    "                      'source_longitude' : float,\n",
    "                      'source_magnitude' : float,\n",
    "                      'source_distance_deg' : float,\n",
    "                      'source_distance_km' : float,\n",
    "                      'back_azimuth_deg' : float,\n",
    "                      'snr_db' : object,\n",
    "                      'code_end_sample' : object}\n",
    "        non_string_or_time.update({col: str for col in col_names if col not in non_string_or_time})\n",
    "        df = pd.read_csv(self.csv_file, dtype = non_string_or_time)\n",
    "        return df\n",
    "           \n",
    "\n",
    "    def get_label_by_name(self, event_name):\n",
    "        # Fastest way to get label\n",
    "        labels = {'EV': 'earthquake' , 'NO' : 'noise'}\n",
    "        return labels[event_name.split('_')[-1]]\n",
    "\n",
    "\n",
    "    def create_name_label_array(self, name_list):\n",
    "        name_label = np.empty((len(name_list), 2), dtype='<U32')\n",
    "        for idx, name in enumerate(name_list):\n",
    "            name_label[idx] = [name, self.get_label_by_name(name)]\n",
    "        return name_label\n",
    "\n",
    "    def get_dataset_distribution(self, name_labels):\n",
    "        labels = [x[1] for x in name_labels]\n",
    "        uniques, counts = np.unique(labels, return_counts = True)\n",
    "        return uniques, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigDataHandler():\n",
    "    \n",
    "    def __init__(self, data_loader):\n",
    "        self.loader = data_loader\n",
    "        self.label_dict = {'earthquake':0, 'noise': 1}\n",
    "        self.source_path = self.loader.source_path\n",
    "        self.seed = self.loader.seed\n",
    "        self.data_file = self.loader.data_file\n",
    "    \n",
    "    def create_train_val_test(self, name_label, val_test_size = 0.1, val_test_prop = 0.5, seed = None, shuffle = False):\n",
    "        train, val_test = train_test_split(name_label, test_size = val_test_size, random_state = seed, shuffle = shuffle)\n",
    "        val, test = train_test_split(val_test, test_size = val_test_prop, random_state = seed, shuffle = shuffle)\n",
    "        return train, val, test\n",
    "    \n",
    "    def name_to_trace(self, name):\n",
    "        return self.data_file.get(name)[:]\n",
    "    \n",
    "    def csv_to_trace_label(self, data_file, info_file, index):\n",
    "        name = info_file['trace_name'][index]\n",
    "        event = data_file.get('data').get(name)[:]\n",
    "        label = self.get_label_by_name(name)\n",
    "        return event, label\n",
    "    \n",
    "    def get_csv_row_by_name(self, event_name):\n",
    "        return self.loader.df_csv[self.loader.df_csv['trace_name'] == event_name].values\n",
    "    \n",
    "    def get_trace_shape(self, df):\n",
    "        some_name = df[0][0]\n",
    "        trace = np.transpose(self.name_to_trace(some_name))\n",
    "        num_channels, num_timesteps = trace.shape\n",
    "        return num_channels, num_timesteps\n",
    "    \n",
    "    def batch_to_trace_binary_label(self, batch):\n",
    "        names = batch[:,0]\n",
    "        labels = batch[:,1]\n",
    "        batch_trace = np.empty((len(batch), 6000, 3))\n",
    "        batch_info = np.empty((len(batch), 1))\n",
    "        for idx, name in enumerate(names):\n",
    "            batch_trace[idx] = self.name_to_trace(name)\n",
    "            batch_info[idx] = self.label_dict.get(labels[idx])\n",
    "        batch_trace = np.reshape(batch_trace, (len(batch), 3, 6000))\n",
    "        return batch_trace, batch_info\n",
    "\n",
    "    def transform_batch(self, scaler, batch_X):\n",
    "        transformed_X = batch_X\n",
    "        for i in range(len(batch_X)):\n",
    "            transformed_X[i] = scaler.transform(batch_X[i])\n",
    "        return transformed_X      \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Balancer():\n",
    "    \n",
    "    def __init__(self, data_loader, handler):\n",
    "        self.loader = data_loader\n",
    "        self.handler = handler\n",
    "        self.seed = data_loader.seed\n",
    "        \n",
    "    def downsample_label(self, target_label, name_label_df, n_samples, seed):\n",
    "        # Downsamples target label\n",
    "        target_df = name_label_df.loc[name_label_df[\"label\"] == target_label]\n",
    "        downsampled_target_df = target_df.sample(n_samples, random_state = self.seed)\n",
    "        \n",
    "        non_target_df_len = len(name_label_df) - len(target_df) \n",
    "        downsampled_df = np.empty((non_target_df_len + len(downsampled_target_df), 2), dtype = '<U32')\n",
    "        downsampled_df[0:non_target_df_len] = name_label_df.loc[name_label_df[\"label\"] != target_label]\n",
    "        downsampled_df[non_target_df_len:non_target_df_len + len(downsampled_target_df)] = downsampled_target_df\n",
    "        downsampled_df = pd.DataFrame(downsampled_df, columns=[\"name\", \"label\"])\n",
    "        \n",
    "        return downsampled_df\n",
    "        \n",
    "    def upsample_label(self, target_label, name_label_df, n_samples, seed):\n",
    "        # Selects n_samples from the target label to include in the new df in addition to the non-target label dps.\n",
    "        target_df = name_label_df.loc[name_label_df[\"label\"] == target_label]\n",
    "        \n",
    "        random_selection = np.empty((len(name_label_df)-len(target_df)+n_samples, 2), dtype = '<U32')\n",
    "        random_selection[0:len(name_label_df)-len(target_df)] = name_label_df.loc[name_label_df[\"label\"] != target_label]\n",
    "        \n",
    "        current_len = len(name_label_df) - len(target_df)\n",
    "        random_selection[current_len:current_len + n_samples] = target_df.sample(n_samples, replace = True, random_state = seed)\n",
    "        random_selection = pd.DataFrame(random_selection, columns = [\"name\", \"label\"])\n",
    "        \n",
    "        return random_selection\n",
    "    \n",
    "    def frac_diff_n_samples(self, frac_diff, min_counts, max_counts):\n",
    "        diff = max_counts - min_counts\n",
    "        n_samples = int(min_counts + diff*frac_diff)\n",
    "        return n_samples\n",
    "        \n",
    "\n",
    "    def balance_dataset(self, name_label, downsample, upsample, frac_diff = 1):\n",
    "        \"\"\"\n",
    "        Balance the dataset. Downsample, upsample or both.\n",
    "        \n",
    "        PARAMETERS:\n",
    "        ------------------------------\n",
    "        name_label: np.array - array of all event names and their respective label.\n",
    "        downsample: bool -     True then will downsample\n",
    "        upsample:   bool -     True then will upsample such that the length of the least occuring label is \n",
    "                               equal to the most occuring\n",
    "        frac_diff: float -     Fraction of the most prominent label that will be downsampled. \n",
    "                               0 will mean that it will be downsampled so that its length is equal \n",
    "                               to that of the least occuring label\n",
    "        \n",
    "        \"\"\"\n",
    "        balancing = pd.DataFrame(name_label, columns = [\"name\", \"label\"], dtype='<U32')\n",
    "        if downsample:\n",
    "            uniques, counts = self.loader.get_dataset_distribution(np.array(balancing, dtype = '<U32'))\n",
    "            most_occuring_label = uniques[np.where(counts == max(counts))][0]\n",
    "            frac_diff_n_samples = self.frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "            balancing = self.downsample_label(most_occuring_label, balancing, frac_diff_n_samples, self.seed)\n",
    "                     \n",
    "        if upsample:\n",
    "            uniques, counts = self.loader.get_dataset_distribution(np.array(balancing,  dtype = '<U32'))\n",
    "            least_occuring_label = uniques[np.where(counts == min(counts))][0]\n",
    "            n_samples_for_balance = max(counts)\n",
    "            balancing = self.upsample_label(least_occuring_label, balancing, n_samples_for_balance, self.seed)\n",
    "        balancing = balancing.sample(frac = 1, random_state = self.seed).reset_index(drop=True)\n",
    "        return np.array(balancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "class BigDataGenerator():\n",
    "    \n",
    "    def __init__(self, data_loader):\n",
    "        self.loader = data_loader\n",
    "        self.handler = self.loader.handler\n",
    "   \n",
    "   \n",
    "    def data_generator(self, ds, batch_size, use_scaler = False, scaler = None):\n",
    "        channels, timesteps = self.handler.get_trace_shape(ds)\n",
    "        num_samples = len(ds)\n",
    "        while True:\n",
    "            for offset in range(0, num_samples, batch_size):\n",
    "                # Get the samples you'll use in this batch\n",
    "                self.batch_samples = np.empty((batch_size,2), dtype = np.ndarray)\n",
    "                \n",
    "                # Handle what happens when asking for a batch but theres no more new data\n",
    "                if offset+batch_size > num_samples:\n",
    "                    overflow = offset + batch_size - num_samples\n",
    "                    self.batch_samples[0:batch_size-overflow] = ds[offset:offset+batch_size]\n",
    "                    i_start = random.randint(0, num_samples-overflow)\n",
    "                    self.batch_samples[batch_size-overflow:batch_size] = ds[i_start:i_start+overflow]           \n",
    "                else:\n",
    "                    self.batch_samples = ds[offset:offset+batch_size]\n",
    "                # Preprocessinng\n",
    "                X, y = self.preprocessing(self.batch_samples, use_scaler, scaler)\n",
    "                try:\n",
    "                    y = np_utils.to_categorical(y, len(np.unique(y)), dtype=np.int64)\n",
    "                except:\n",
    "                    raise Exception(f'Error when doing to_categorical. Inputs are y: {y} and num_classes: {len(np.unique(y))}')               \n",
    "                yield X, y\n",
    "    \n",
    "    def preprocessing(self, batch_samples, use_scaler, scaler):\n",
    "        batch_trace, batch_label = self.handler.batch_to_trace_binary_label(batch_samples)\n",
    "        if use_scaler:\n",
    "            batch_trace = self.handler.transform_batch(scaler, batch_trace)\n",
    "        return batch_trace, batch_label\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "class BigScalerFitter():\n",
    "    \n",
    "    def __init__(self, train_ds, scaler, data_loader):\n",
    "        self.train_ds = train_ds\n",
    "        self.scaler = scaler\n",
    "        self.loader = data_loader\n",
    "\n",
    "    def subsample(self, ds, shuffle = False, subsample_rate = 0.2):\n",
    "        channels, timesteps = self.data_loader.handler.get_trace_shape(ds)\n",
    "        num_samples = len(ds)\n",
    "        num_samples = int(num_samples*subsample_rate)\n",
    "        if shuffle:\n",
    "            ds = ds.sample(frac = 1, random_state = self.loader.seed)\n",
    "        subsample_X = np.empty((num_samples, channels, timesteps))\n",
    "        subsample_y = np.empty((num_samples,1), dtype=np.dtype('<U10'))\n",
    "        for idx, name, label in enumerate(ds):\n",
    "            subsample_X[idx] = self.handler.name_to_trace(name)\n",
    "            subsample_y[idx] = label\n",
    "        return subsample_X, subsample_y\n",
    "\n",
    "    def transform_subsample(self, ds, subsample_rate = 0.2, shuffle = False):\n",
    "        subsamples_X, subsamples_y = self.subsample(ds, shuffle, subsample_rate)\n",
    "        for i in range(len(subsamples_X)):\n",
    "            subsamples_X[i] = self.scaler.transform(subsamples_X[i])\n",
    "        return subsamples_X, subsamples_y\n",
    "\n",
    "\n",
    "    def transform_sample(self, sample_X):\n",
    "        return self.scaler.transform(sample_X)\n",
    "    \n",
    "    def save_fit(self, scaler):\n",
    "        dump(scaler, f'{self.scaler_folder}\\{self.scaler_name}_{self.handler.seed}')\n",
    "    \n",
    "    def load_fit(self, scaler_type):\n",
    "        if path.exists(f'{self.scaler_folder}\\{scaler_type}_{self.handler.seed}'):\n",
    "            return load(f'{self.scaler_folder}\\{self.scaler_name}_{self.handler.seed}')\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def progress_bar(self, current, total, barLength = 20):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Fitting scaler progress: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "class BigStandardScalerFitter(BigScalerFitter):\n",
    "    \n",
    "    def __init__(self, train_ds, handler):\n",
    "        self.train_ds = train_ds\n",
    "        self.handler = handler\n",
    "        self.scaler_name = \"BigStandardScaler\"\n",
    "        self.scaler_folder = self.handler.source_path + \"\\MasterThesis\\Scalers\"\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit_scaler(self):\n",
    "        if self.load_fit(self.scaler_name) != None:\n",
    "            self.scaler = self.load_fit(self.scaler_name)\n",
    "            return self.scaler\n",
    "        else:\n",
    "            ds = self.train_ds\n",
    "            channels, timesteps = self.handler.get_trace_shape(ds)\n",
    "            num_samples = len(ds)\n",
    "            ds = np.array(ds)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                self.progress_bar(i, num_samples, \"Fitting scaler\")\n",
    "                X = self.handler.name_to_trace(ds[i][0])\n",
    "\n",
    "                self.scaler.partial_fit(X)\n",
    "            self.save_fit(self.scaler)\n",
    "            return self.scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = BigDataLoader(test = False, upsample = False, downsample = False, frac_diff = 0, seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = data_loader.handler.create_train_val_test(data_loader.name_label, seed = data_loader.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.handler.name_to_trace(train[9][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['earthquake', 'noise'], dtype='<U10'),\n",
       " array([1030231,  235426], dtype=int64))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.get_dataset_distribution(data_loader.name_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tss_9\\anaconda3\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator StandardScaler from version 0.23.1 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "scaler = BigStandardScalerFitter(train, data_loader.handler).fit_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = BigDataGenerator(data_loader)\n",
    "train_gen = data_gen.data_generator(train, 64, use_scaler = True, scaler = scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_n_iterations(gen, n_iter):\n",
    "    start = time.time()\n",
    "    for i in range(n_iter):\n",
    "        X, y = next(gen)\n",
    "    end = time.time()\n",
    "    print(f\"Exectuted {n_iter} iterations in {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exectuted 10 iterations in 24.51870083808899 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_n_iterations(train_gen, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_no_scaler = data_gen.data_generator(train, 64, use_scaler = False, scaler = scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exectuted 10 iterations in 20.894123792648315 seconds.\n"
     ]
    }
   ],
   "source": [
    "time_n_iterations(train_gen_no_scaler, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_simple_traces(names, n):\n",
    "    traces = np.empty((n, 6000, 3))\n",
    "    start = time.time()\n",
    "    for i in range(n):\n",
    "        traces[i] = data_loader.handler.name_to_trace(names[i])\n",
    "    end = time.time()\n",
    "    print(f\"Exectuted {n} iterations in {end - start} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exectuted 640 iterations in 20.94095802307129 seconds.\n"
     ]
    }
   ],
   "source": [
    "n_simple_traces(data_loader.name_label[:,0][0:64*10], 64*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B046.PB_201204121520_NO', 'GDXB.NC_201105111239_NO',\n",
       "       'YFT.SN_20140216034642_EV', ..., 'SPNN.AV_20180221012506_EV',\n",
       "       'ARSB.KR_20180115121130_NO', 'OMMB.NN_20140608093441_EV'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.name_label[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = data_loader.data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_events = len(list(data_file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = (num_events, 6000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1265657, 6000, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = data_loader.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Thesis_ssd\\\\LargeDataset\\\\merge.hdf5'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-9412b3c58d8d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-9412b3c58d8d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    loaded = .get('')\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "loaded = .get('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = h5py.File(filename, 'r',rdcc_nbytes = 10**11)[\"data\"]\n",
    "def name_to_trace(name):\n",
    "    return data_file[name][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lookups = 1000\n",
    "start = time.time()\n",
    "for i in range(n_lookups):   \n",
    "    dump = name_to_trace(data_loader.event_names[i])\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Exectuted {n_lookups} iterations in {total_time} seconds.\")\n",
    "print(f\"Each lookup took {total_time/n_lookups} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
