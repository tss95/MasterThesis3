{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\nINFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.NarrowSearchIncepTime import NarrowSearchIncepTime\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutup()\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 3\n",
      "{'noise': 105999, 'earthquake': 105999, 'explosion': 102808}\n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.1,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal as D\n",
    "\n",
    "class NarrowOptimizer(GridSearchResultProcessor):\n",
    "\n",
    "    \"\"\"\n",
    "    This class functions as an heuristic that will attempt to reach a local minima. The class can either start off an existing search, or can start its own. The process looks a little like this:\n",
    "    1. Select the best model from existing result file (if using a file)\n",
    "    2. Use the best model / start model as the foundation. Create a search space around this in terms of hyperparameters.\n",
    "    3. Do a narrow search on the generated search space. If quick_mode = True, then if a better model is found during the narrow search, replace the base model with this and return to step 2. \n",
    "    4. Repeat steps 1-3\n",
    "    \n",
    "\n",
    "    Notes:\n",
    "    \n",
    "    - Would like this to be as robust as possible, and not dependent on InceptionTime. Want to be able to use this class for any model really.\n",
    "        - This can be challenging when creating dictionaries, as annoyingly, the models use 2 seperate dictionaries for initilization.\n",
    "    \n",
    "    Drawbacks, potential points of failure:\n",
    "     - The filtering method is very simple, and assumes that less than half of the good models are buggy. This is definitely not necessarily the case, and will cause this class to potentially try to optimize a lost cause. HOW CAN THIS BE SOLVED!?!?!?!? BY FIXING THE ORIGINAL BUG YOU DUMB FUCK\n",
    "     - The way results are processed, requires a model_grid and a hyper_grid. This design choice is the root of sooo many problems, and may lead to a less than robust implementation of this class. This can lead to different versions. Potential soution: Use this class as a parent class, and have children objects that are specialized for each type of model. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, use_highpass,\n",
    "                 use_tensorboard, use_liveplots, use_custom_callback, use_early_stopping, highpass_freq,\n",
    "                 use_reduced_lr, num_channels, depth, quick_mode = False, continue_from_result_file = False, \n",
    "                 result_file_name = \"\", start_grid = []):\n",
    "        \n",
    "        self.loadData = loadData\n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "        self.detrend = detrend\n",
    "        self.use_scaler = use_scaler\n",
    "        self.use_time_augmentor = use_time_augmentor\n",
    "        self.use_noise_augmentor = use_noise_augmentor\n",
    "        self.use_minmax = use_minmax\n",
    "        self.use_highpass = use_highpass\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.highpass_freq = highpass_freq\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.depth = depth\n",
    "        self.quick_mode = quick_mode\n",
    "        self.continue_from_result_file = continue_from_result_file\n",
    "        self.result_file_name = result_file_name\n",
    "        self.start_grid = start_grid\n",
    "\n",
    "    def run(self, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10):\n",
    "        \"\"\"\n",
    "        Self explanatory\n",
    "\n",
    "        PARAMS:\n",
    "        --------------\n",
    "        result_file_name: (str) Name of the file to be used. If continue_from_result == False, then this will not be used\n",
    "        num_classes: (int)\n",
    "        optimize_metric: [string, string] Optimization criteria. First element will be most significant.\n",
    "        nr_candidates: (int) Number of model candidates that will be considered in the first step sort.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.quick_mode:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Quick mode, starting of result file: {self.result_file_name}\")\n",
    "                fit_class = self.determine_model(self.result_file_name)\n",
    "                fit_class.run_quick_mode(optimize_metric, nr_candidates)\n",
    "                \n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        else:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Exhaustive mode, starting of result file: {result_file_name}\")\n",
    "\n",
    "                fit_class = self.determine_model(result_file_name)\n",
    "                fit_class.run_exhaustive_mode(optimize_metric, nr_candidates)\n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        return\n",
    "\n",
    "    def determine_model(self, result_file_name):\n",
    "         name_list = result_file_name.split('_')\n",
    "         if \"InceptionTime\" in name_list:\n",
    "             return IncepTimeNarrowOptimizer(self.loadData, self.detrend, self.use_scaler, self.use_time_augmentor,\n",
    "                         self.use_noise_augmentor, self.use_minmax, self.use_highpass, self.use_tensorboard,\n",
    "                         self.use_liveplots, self.use_custom_callback, self.use_early_stopping, \n",
    "                         self.highpass_freq, self.use_reduced_lr, self.num_channels, self.depth, \n",
    "                         self.quick_mode, self.continue_from_result_file, self.result_file_name, self.start_grid)\n",
    "         else:\n",
    "            raise Excpetion(\"Other models have not yet been implemented in this class\")\n",
    "    \n",
    "    def quick_mode(self, result_file_name, num_classes, optimize_metric):\n",
    "        pass\n",
    "\n",
    "    def get_best_model(self, result_file_name, num_classes, optimize_metric, nr_candidates):\n",
    "        # Clear nan values\n",
    "        self.clear_nans(result_file_name, num_classes)\n",
    "        results_df = self.get_results_df_by_name(result_file_name, num_classes)\n",
    "        df_f1 = results_df.copy()\n",
    "        # Add f1 stats\n",
    "        df_f1 = self.add_f1_stats(df_f1)\n",
    "        # Sort by sort conditions\n",
    "        sorted_df = self.sort_df(df_f1, optimize_metric)\n",
    "        # Get the top nr_candidates\n",
    "        best_initial_candidates = sorted_df.copy().head(nr_candidates)\n",
    "        # Attempt to only select models which have the best f1 score, and first part of the sort conditions\n",
    "        # This is due to (likely) bug that has some models perform really well in one metric, but terrible in other metrics. The working assumption is that models with high f1, are good.\n",
    "        # TODO: Consider just switching the optimizer metrics here. Without the current BUG with strange training metrics (and inconsistent metrics wrt. the confusion matrix) this is a good opportunity to optimize with two metrics.\n",
    "        best_initial_sorted_by_f1 = self.sort_df(best_initial_candidates, ['val_f1', optimize_metric[0]])\n",
    "        # Select nr_candidates//2 of these models, and then resort them by their primary condition.\n",
    "        reduced_sorted_by_f1 = best_initial_sorted_by_f1.head(nr_candidates//2)\n",
    "        best_secondary_sorted_by_conditions = self.sort_df(reduced_sorted_by_f1, optimize_metric)\n",
    "        # At this point we should have filtered out bad outlier models, and be left with good candidates. \n",
    "        # We now select the best model according to the sort condidtions.\n",
    "        best_model = best_secondary_sorted_by_conditions.head(1)\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    \n",
    "    def add_f1_stats(self, df_f1):\n",
    "        df_f1.columns=df_f1.columns.str.strip()\n",
    "        all_train_precision = df_f1['train_precision']\n",
    "        all_train_recall = df_f1['train_recall']\n",
    "        all_val_precision = df_f1['val_precision']\n",
    "        all_val_recall = df_f1['val_recall']\n",
    "        f1_train = self.create_f1_list(all_train_precision, all_train_recall)\n",
    "        f1_val = self.create_f1_list(all_val_precision, all_val_recall)\n",
    "        df_f1['train_f1'] = f1_train\n",
    "        df_f1['val_f1'] = f1_val\n",
    "        return df_f1\n",
    "\n",
    "    \n",
    "\n",
    "    def f1_score(self, precision, recall):\n",
    "        f1 = 2*((precision*recall)/(precision + recall))\n",
    "        return f1\n",
    "\n",
    "    def create_f1_list(self, precision_df, recall_df):\n",
    "        f1 = []\n",
    "        for i in range(len(precision_df)):\n",
    "            f1.append(self.f1_score(precision_df.loc[i], recall_df.loc[i]))\n",
    "        return f1\n",
    "\n",
    "        \n",
    "    def sort_df(self, df, sort_conditions):\n",
    "        ascending = False\n",
    "        if sort_conditions == ['val_loss', 'train_loss'] or sort_conditions == ['train_loss', 'val_loss']:\n",
    "            ascending = True\n",
    "        if 'val_loss' in sort_conditions and 'train_loss' not in sort_conditions:\n",
    "            raise Exception(\"Problematic sorting criteria. Cannot determine if sorting should be ascending or descending. A solution for this needs to be implemented in order for this to work\")\n",
    "        return df.sort_values(by=sort_conditions, axis = 0, ascending = ascending)\n",
    "\n",
    "    \"\"\"\n",
    "    def convert_best_model_to_main_grid(self, best_model):\n",
    "        model_dict = self.row_to_dict(best_model)\n",
    "\n",
    "\n",
    "    def row_to_dict(self, model_df):\n",
    "        keys = list(model_df.keys())\n",
    "        # Assumes 10 columns dedicated to results and the rest to hyperparams\n",
    "        hyper_keys = keys[:len(keys) - 10]\n",
    "        model_dict = model_df[:len(hyper_keys)].to_dict()\n",
    "        #del model_dict['index']\n",
    "        return model_dict\n",
    "    \"\"\"\n",
    "\n",
    "    def delete_metrics(self, best_model_df):\n",
    "        best_model_df = best_model_df[best_model_df.columns[:len(best_model_df.columns) - 10]]\n",
    "        return best_model_df\n",
    "\n",
    "    def adapt_best_model_dict(self, best_model_dict):\n",
    "        print(best_model_dict)\n",
    "        return {key:[value] for (key,value) in best_model_dict.items()}\n",
    "\n",
    "    def create_search_grid(self, main_model_grid):\n",
    "        # Handle hyperparameters that are the same for all models\n",
    "        param_grid = main_model_grid.copy()\n",
    "        scaler = range(-4, 4, 2)\n",
    "    \n",
    "    \n",
    "    def create_batch_params(self, batch_center):\n",
    "        max_batch_size = 4096\n",
    "        new_params = [batch_center//4, batch_center//2, batch_center*2, batch_center*4]\n",
    "        for i, batch_size in enumerate(new_params):\n",
    "            new_params[i] = min(batch_size, max_batch_size)\n",
    "        return list(set(new_params))\n",
    "    \n",
    "    def create_learning_rate_params(self, learning_rate_center):\n",
    "        min_learning_rate = 0.00001\n",
    "        new_learning_params = [learning_rate_center*10**2, learning_rate_center*10**1, (learning_rate_center*10)/2, learning_rate_center / 2, learning_rate_center*10**(-1), learning_rate_center*10**(-2)]\n",
    "        for i, rate in enumerate(new_learning_params):\n",
    "            new_learning_params[i] = max(rate, min_learning_rate)\n",
    "        return list(set(new_learning_params))\n",
    "\n",
    "    def create_epochs_params(self, epoch_center):\n",
    "        max_epochs = 150\n",
    "        new_epochs = [epoch_center - 20, epoch_center -10, epoch_center + 10, epoch_center +20]\n",
    "        for i in range(len(new_epochs)):\n",
    "            new_epochs[i] = min(max(new_epochs[i], 10), max_epochs)\n",
    "        return list(set(new_epochs))\n",
    "    \n",
    "    def create_optimizer_params(self, current_optimizer):\n",
    "        options = [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "        del options[options.index(current_optimizer)]\n",
    "        return options\n",
    "\n",
    "    def create_activation_params(self, current_activation, include_linear):\n",
    "        if include_linear:\n",
    "            options = [\"linear\", \"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        else: \n",
    "            options = [\"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        del options[options.index(current_activation)]\n",
    "        return options\n",
    "\n",
    "    def create_reg_params(self, current_reg):\n",
    "        max_reg = 0.3\n",
    "        if current_reg == 0.0:\n",
    "            current_reg = 0.01\n",
    "        new_reg = [current_reg*10**2, current_reg*10, (current_reg*10)/2, current_reg/2, current_reg*10**(-1), current_reg*10**(-2)]\n",
    "        for i in range(len(new_reg)):\n",
    "            new_reg[i] = min(new_reg[i], max_reg)\n",
    "        return list(set(new_reg))\n",
    "\n",
    "    def create_boolean_params(self, current_bool):\n",
    "        if current_bool:\n",
    "            return [False, False]\n",
    "        else:\n",
    "            return [True, True]\n",
    "\n",
    "    def create_output_activation(self, current):\n",
    "        return [current]\n",
    "\n",
    "    def get_metrics(self, model, optimize_metric):\n",
    "        return model[optimize_metric[0]].iloc[0], model[optimize_metric[1]].iloc[0]\n",
    "\n",
    "    def create_search_space(self, main_grid, search_grid):\n",
    "        key_list = list(main_grid.keys())\n",
    "        np.random.shuffle(key_list)\n",
    "        search_list = []\n",
    "        for key in key_list:\n",
    "            if len(search_grid[key]) > 1:\n",
    "                one_model = main_grid.copy()\n",
    "                one_model[key] = hypermodel_grid[key]\n",
    "                key_grid = list(ParameterGrid(one_model))\n",
    "                search_list.append(key_grid)\n",
    "            else:\n",
    "                continue\n",
    "        search_list = list(chain.from_iterable(search_list))\n",
    "        pprint.pprint(search_list)\n",
    "        hyper_search, model_search = self.unmerge_search_space(search_list, hyper_grid, model_grid)\n",
    "        return hyper_search, model_search\n",
    "\n",
    "\n",
    "\n",
    "class IncepTimeNarrowOptimizer(NarrowOptimizer):\n",
    "\n",
    "    def __init__(self, loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, use_highpass, use_tensorboard, use_liveplots, use_custom_callback, use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, quick_mode = False, continue_from_result_file = False, \n",
    "                result_file_name = \"\", start_grid = []):\n",
    "        super().__init__(loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, \n",
    "                         use_highpass, use_tensorboard, use_liveplots, use_custom_callback, \n",
    "                         use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, \n",
    "                         quick_mode, continue_from_result_file, result_file_name, start_grid)\n",
    "        \n",
    "    \n",
    "    def run_exhaustive_mode(self, optimize_metric, nr_candidates):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        # To start of I will only implement what to do when we are continuing off existing file.\n",
    "        self.best_model = self.get_best_model(self.result_file_name, self.num_classes, optimize_metric, nr_candidates)\n",
    "        self.current_best_metrics = self.get_metrics(self.best_model, optimize_metric)\n",
    "        print(f\"Current best metrics: {optimize_metric[0]} = {self.current_best_metrics[0]}, {optimize_metric[1]} = {self.current_best_metrics[1]}\")\n",
    "        best_model_dict = self.delete_metrics(self.best_model).iloc[0].to_dict()\n",
    "        print(\"Gained with this model:\")\n",
    "        pp.pprint(best_model_dict)\n",
    "        search_grid = self.create_search_grid(best_model_dict)\n",
    "        print(\"Which will be explored with this search space:\")\n",
    "        pp.pprint(search_grid)\n",
    "        ramLoader = RamLoader(self.loadData, \n",
    "                              self.handler, \n",
    "                              use_time_augmentor = self.use_time_augmentor, \n",
    "                              use_noise_augmentor = self.use_noise_augmentor, \n",
    "                              use_scaler = self.use_scaler,\n",
    "                              use_minmax = self.use_minmax, \n",
    "                              use_highpass = self.use_highpass, \n",
    "                              highpass_freq = self.highpass_freq, \n",
    "                              detrend = self.detrend, \n",
    "                              load_test_set = False)\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val, self.timeAug, self.scaler, self.noiseAug = ramLoader.load_to_ram(False, self.num_channels)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def run_quick_mode(self, optimize_metric, nr_candidates):\n",
    "        raise Exception(\"Quick mode has not yet been implemented\")\n",
    "\n",
    "\n",
    "    def create_search_grid(self, main_grid):\n",
    "        # This is the least robust function in this class. \n",
    "        return {'batch_size' : self.create_batch_params(main_grid['batch_size']),\n",
    "                     'epochs' : self.create_epochs_params(main_grid['epochs']),\n",
    "                     'learning_rate' : self.create_learning_rate_params(main_grid['learning_rate']),\n",
    "                     'optimizer' : self.create_optimizer_params(main_grid['optimizer']),\n",
    "                     'bottleneck_size' : self.create_bottleneck_size(main_grid['bottleneck_size']),\n",
    "                     'kernel_size' : self.create_kernel_and_filter_params(main_grid['kernel_size']),\n",
    "                     'l1_r' : self.create_reg_params(main_grid['l1_r']),\n",
    "                     'l2_r' : self.create_reg_params(main_grid['l2_r']),\n",
    "                     'module_activation' : self.create_activation_params(main_grid['module_activation'], include_linear = True),\n",
    "                     'module_output_activation' : self.create_activation_params(main_grid['module_output_activation'], include_linear = True),\n",
    "                     'nr_modules' : self.create_nr_modules_params(main_grid['nr_modules']),\n",
    "                     'num_filters' : self.create_kernel_and_filter_params(main_grid['num_filters']),\n",
    "                     'output_activation' : self.create_output_activation(main_grid['output_activation']),\n",
    "                     'reg_module' : self.create_boolean_params(main_grid['reg_module']),\n",
    "                     'reg_shortcut' : self.create_boolean_params(main_grid['reg_shortcut']),\n",
    "                     'shortcut_activation' : self.create_activation_params(main_grid['shortcut_activation'], include_linear = False),\n",
    "                     'use_bottleneck' : self.create_boolean_params(main_grid['use_bottleneck']),\n",
    "                     'use_residuals' : self.create_boolean_params(main_grid['use_residuals'])}\n",
    "\n",
    "    def create_nr_modules_params(self, center):\n",
    "        max_modules = 30\n",
    "        new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "        for i in range(len(new_nr_modules)):\n",
    "            new_nr_modules[i] = min(max(new_nr_modules[i], 1), max_modules)\n",
    "        return list(set(new_nr_modules))\n",
    "    \n",
    "    def create_kernel_and_filter_params(self, current):\n",
    "        max_size = 120\n",
    "        new_kernels = [current - 20, current - 10, current - 2, current + 2, current + 10, current + 20]\n",
    "        for i, kern in enumerate(new_kernels):\n",
    "            new_kernels[i] = min(max(kern, 2), max_size)\n",
    "        return list(set(new_kernels))\n",
    "\n",
    "    def create_bottleneck_size(self, current_nr):\n",
    "        max_nr = 100\n",
    "        new_bottleneck = [current_nr - 4, current_nr - 2, current_nr + 2, current_nr + 4]\n",
    "        for i, neck in enumerate(new_bottleneck):\n",
    "            new_bottleneck[i] = min(max(neck, 2), max_nr)\n",
    "        return list(set(new_bottleneck))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exhaustive mode, starting of result file: results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv\nCurrent best metrics: val_accuracy = 0.970384478569, val_f1 = 0.8507676617429287\nGained with this model:\n{   'batch_size': 128,\n    'bottleneck_size': 28,\n    'epochs': 100,\n    'kernel_size': 60,\n    'l1_r': 0.0,\n    'l2_r': 0.0001,\n    'learning_rate': 0.001,\n    'module_activation': 'tanh',\n    'module_output_activation': 'sigmoid',\n    'nr_modules': 23,\n    'num_filters': 38,\n    'optimizer': 'adam',\n    'output_activation': 'sigmoid',\n    'reg_module': True,\n    'reg_shortcut': False,\n    'shortcut_activation': 'relu',\n    'use_bottleneck': True,\n    'use_residuals': True}\nWhich will be explored with this search space:\n{   'batch_size': [32, 256, 64, 512],\n    'bottleneck_size': [24, 26, 32, 30],\n    'epochs': [80, 90, 120, 110],\n    'kernel_size': [70, 40, 80, 50, 58, 62],\n    'l1_r': [0.3, 0.1, 0.05, 0.005, 0.0001, 0.001],\n    'l2_r': [   1.0000000000000001e-05,\n                0.01,\n                5.0000000000000002e-05,\n                1.0000000000000002e-06,\n                0.001,\n                0.00050000000000000001],\n    'learning_rate': [   0.10000000000000001,\n                         1.0000000000000001e-05,\n                         0.01,\n                         0.0050000000000000001,\n                         0.0001,\n                         0.00050000000000000001],\n    'module_activation': ['linear', 'relu', 'softmax', 'sigmoid'],\n    'module_output_activation': ['linear', 'relu', 'softmax', 'tanh'],\n    'nr_modules': [17, 26, 20, 29],\n    'num_filters': [36, 40, 48, 18, 58, 28],\n    'optimizer': ['rmsprop', 'sgd'],\n    'output_activation': ['sigmoid'],\n    'reg_module': [False, False],\n    'reg_shortcut': [True, True],\n    'shortcut_activation': ['softmax', 'tanh', 'sigmoid'],\n    'use_bottleneck': [False, False],\n    'use_residuals': [False, False]}\n"
     ]
    }
   ],
   "source": [
    "num_channels = 3\n",
    "\n",
    "use_time_augmentor = True\n",
    "use_scaler = True\n",
    "use_noise_augmentor = True\n",
    "detrend = True\n",
    "use_minmax = False\n",
    "use_highpass = True\n",
    "highpass_freq = 0.1\n",
    "\n",
    "use_tensorboard = True\n",
    "use_liveplots = False\n",
    "use_custom_callback = False\n",
    "use_early_stopping = True\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = True\n",
    "\n",
    "result_file_name = 'results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv'\n",
    "quick_mode = False\n",
    "continue_from_result_file = True\n",
    "start_grid = None\n",
    "\n",
    "depth = 5\n",
    "\n",
    "\n",
    "\n",
    "narrowOpt = NarrowOptimizer(loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, \n",
    "                            use_highpass, use_tensorboard, use_liveplots, use_custom_callback, \n",
    "                            use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, \n",
    "                            quick_mode, continue_from_result_file, result_file_name, start_grid)\n",
    "\n",
    "#top_10 = narrowOpt.get_best_model(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\n",
    "best_model_dict = narrowOpt.run(['val_accuracy', 'val_f1'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 14 required positional arguments: 'detrend', 'use_scaler', 'use_time_augmentor', 'use_noise_augmentor', 'use_minmax', 'use_highpass', 'use_tensorboard', 'use_liveplots', 'use_custom_callback', 'use_early_stopping', 'highpass_freq', 'use_reduced_lr', 'num_channels', and 'depth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-f616878cd507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mincepTimeOpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIncepTimeNarrowOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquick_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from_result_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincepTimeOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mincepTimeOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_search_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 14 required positional arguments: 'detrend', 'use_scaler', 'use_time_augmentor', 'use_noise_augmentor', 'use_minmax', 'use_highpass', 'use_tensorboard', 'use_liveplots', 'use_custom_callback', 'use_early_stopping', 'highpass_freq', 'use_reduced_lr', 'num_channels', and 'depth'"
     ]
    }
   ],
   "source": [
    "incepTimeOpt = IncepTimeNarrowOptimizer(0, quick_mode = True, continue_from_result_file = True)\n",
    "best_model_dict = incepTimeOpt.run(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\n",
    "incepTimeOpt.create_search_grid(best_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{   'batch_size': 128,\n    'bottleneck_size': 28,\n    'epochs': 100,\n    'kernel_size': 60,\n    'l1_r': 0.0,\n    'l2_r': 0.0001,\n    'learning_rate': 0.001,\n    'module_activation': 'tanh',\n    'module_output_activation': 'sigmoid',\n    'nr_modules': 23,\n    'num_filters': 38,\n    'optimizer': 'adam',\n    'output_activation': 'sigmoid',\n    'reg_module': True,\n    'reg_shortcut': False,\n    'shortcut_activation': 'relu',\n    'use_bottleneck': True,\n    'use_residuals': True}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(best_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{21, 24, 30}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "def dab(center):\n",
    "    max_modules = 30\n",
    "    new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "    for i in range(len(new_nr_modules)):\n",
    "        new_nr_modules[i] = min(new_nr_modules[i], max_modules)\n",
    "    return set(new_nr_modules)\n",
    "dab(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}