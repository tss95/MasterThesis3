{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.NarrowSearchIncepTime import NarrowSearchIncepTime\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutup()\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'LoadData' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e1e3854a93bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m'even_balance'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloadData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoadData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mload_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mfull_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnoise_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_ds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LoadData' is not defined"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.1,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-4ab252674ad4>, line 204)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-4ab252674ad4>\"\u001b[0;36m, line \u001b[0;32m204\u001b[0m\n\u001b[0;31m    def\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class NarrowOptimizer(GridSearchResultProcessor):\n",
    "\n",
    "    \"\"\"\n",
    "    This class functions as an heuristic that will attempt to reach a local minima. The class can either start off an existing search, or can start its own. The process looks a little like this:\n",
    "    1. Select the best model from existing result file (if using a file)\n",
    "    2. Use the best model / start model as the foundation. Create a search space around this in terms of hyperparameters.\n",
    "    3. Do a narrow search on the generated search space. If quick_mode = True, then if a better model is found during the narrow search, replace the base model with this and return to step 2. \n",
    "    4. Repeat steps 1-3\n",
    "    \n",
    "\n",
    "    Notes:\n",
    "    \n",
    "    - Would like this to be as robust as possible, and not dependent on InceptionTime. Want to be able to use this class for any model really.\n",
    "        - This can be challenging when creating dictionaries, as annoyingly, the models use 2 seperate dictionaries for initilization.\n",
    "    \n",
    "    Drawbacks, potential points of failure:\n",
    "     - The filtering method is very simple, and assumes that less than half of the good models are buggy. This is definitely not necessarily the case, and will cause this class to potentially try to optimize a lost cause. HOW CAN THIS BE SOLVED!?!?!?!? BY FIXING THE ORIGINAL BUG YOU DUMB FUCK\n",
    "     - The way results are processed, requires a model_grid and a hyper_grid. This design choice is the root of sooo many problems, and may lead to a less than robust implementation of this class. This can lead to different versions. Potential soution: Use this class as a parent class, and have children objects that are specialized for each type of model. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, quick_mode = False, continue_from_result_file = False, result_file_name = \"\", start_grid = []):\n",
    "        self.depth = depth\n",
    "        self.quick_mode = quick_mode\n",
    "        self.continue_from_result_file = continue_from_result_file\n",
    "        self.result_file_name = result_file_name\n",
    "        self.start_grid = start_grid\n",
    "\n",
    "    def run(self, result_file_name, num_classes, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10):\n",
    "        \"\"\"\n",
    "        Self explanatory\n",
    "\n",
    "        PARAMS:\n",
    "        --------------\n",
    "        result_file_name: (str) Name of the file to be used. If continue_from_result == False, then this will not be used\n",
    "        num_classes: (int)\n",
    "        optimize_metric: [string, string] Optimization criteria. First element will be most significant.\n",
    "        nr_candidates: (int) Number of model candidates that will be considered in the first step sort.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.quick_mode:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Quick mode, starting of result file: {result_file_name}\")\n",
    "                best_model = self.get_best_model(result_file_name, num_classes, optimize_metric, nr_candidates)\n",
    "\n",
    "                best_model_dict = self.delete_metrics(best_model).iloc[0].to_dict()\n",
    "                best_model_dict = self.adapt_best_model_dict(best_model_dict)\n",
    "                return best_model_dict\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        else:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Exhaustive mode, starting of result file: {result_file_name}\")\n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    def quick_mode(self, result_file_name, num_classes, optimize_metric):\n",
    "        pass\n",
    "\n",
    "    def get_best_model(self, result_file_name, num_classes, optimize_metric, nr_candidates):\n",
    "        # Clear nan values\n",
    "        self.clear_nans(result_file_name, num_classes)\n",
    "        results_df = self.get_results_df_by_name(result_file_name, num_classes)\n",
    "        df_f1 = results_df.copy()\n",
    "        # Add f1 stats\n",
    "        df_f1 = self.add_f1_stats(df_f1)\n",
    "        # Sort by sort conditions\n",
    "        sorted_df = self.sort_df(df_f1, optimize_metric)\n",
    "        # Get the top nr_candidates\n",
    "        best_initial_candidates = sorted_df.copy().head(nr_candidates)\n",
    "        # Attempt to only select models which have the best f1 score, and first part of the sort conditions\n",
    "        # This is due to (likely) bug that has some models perform really well in one metric, but terrible in other metrics. The working assumption is that models with high f1, are good.\n",
    "        # TODO: Consider just switching the optimizer metrics here. Without the current BUG with strange training metrics (and inconsistent metrics wrt. the confusion matrix) this is a good opportunity to optimize with two metrics.\n",
    "        best_initial_sorted_by_f1 = self.sort_df(best_initial_candidates, ['val_f1', optimize_metric[0]])\n",
    "        # Select nr_candidates//2 of these models, and then resort them by their primary condition.\n",
    "        reduced_sorted_by_f1 = best_initial_sorted_by_f1.head(nr_candidates//2)\n",
    "        best_secondary_sorted_by_conditions = self.sort_df(reduced_sorted_by_f1, optimize_metric)\n",
    "        # At this point we should have filtered out bad outlier models, and be left with good candidates. \n",
    "        # We now select the best model according to the sort condidtions.\n",
    "        best_model = best_secondary_sorted_by_conditions.head(1)\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    \n",
    "    def add_f1_stats(self, df_f1):\n",
    "        df_f1.columns=df_f1.columns.str.strip()\n",
    "        all_train_precision = df_f1['train_precision']\n",
    "        all_train_recall = df_f1['train_recall']\n",
    "        all_val_precision = df_f1['val_precision']\n",
    "        all_val_recall = df_f1['val_recall']\n",
    "        f1_train = self.create_f1_list(all_train_precision, all_train_recall)\n",
    "        f1_val = self.create_f1_list(all_val_precision, all_val_recall)\n",
    "        df_f1['train_f1'] = f1_train\n",
    "        df_f1['val_f1'] = f1_val\n",
    "        return df_f1\n",
    "\n",
    "    \n",
    "\n",
    "    def f1_score(self, precision, recall):\n",
    "        f1 = 2*((precision*recall)/(precision + recall))\n",
    "        return f1\n",
    "\n",
    "    def create_f1_list(self, precision_df, recall_df):\n",
    "        f1 = []\n",
    "        for i in range(len(precision_df)):\n",
    "            f1.append(self.f1_score(precision_df.loc[i], recall_df.loc[i]))\n",
    "        return f1\n",
    "\n",
    "        \n",
    "    def sort_df(self, df, sort_conditions):\n",
    "        ascending = False\n",
    "        if sort_conditions == ['val_loss', 'train_loss'] or sort_conditions == ['train_loss', 'val_loss']:\n",
    "            ascending = True\n",
    "        if 'val_loss' in sort_conditions and 'train_loss' not in sort_conditions:\n",
    "            raise Exception(\"Problematic sorting criteria. Cannot determine if sorting should be ascending or descending. A solution for this needs to be implemented in order for this to work\")\n",
    "        return df.sort_values(by=sort_conditions, axis = 0, ascending = ascending)\n",
    "\n",
    "    \"\"\"\n",
    "    def convert_best_model_to_main_grid(self, best_model):\n",
    "        model_dict = self.row_to_dict(best_model)\n",
    "\n",
    "\n",
    "    def row_to_dict(self, model_df):\n",
    "        keys = list(model_df.keys())\n",
    "        # Assumes 10 columns dedicated to results and the rest to hyperparams\n",
    "        hyper_keys = keys[:len(keys) - 10]\n",
    "        model_dict = model_df[:len(hyper_keys)].to_dict()\n",
    "        #del model_dict['index']\n",
    "        return model_dict\n",
    "    \"\"\"\n",
    "\n",
    "    def delete_metrics(self, best_model_df):\n",
    "        best_model_df = best_model_df[best_model_df.columns[:len(best_model_df.columns) - 10]]\n",
    "        return best_model_df\n",
    "\n",
    "    def adapt_best_model_dict(self, best_model_dict):\n",
    "        print(best_model_dict)\n",
    "        return {key:[value] for (key,value) in best_model_dict.items()}\n",
    "\n",
    "    def create_search_grid(self, main_model_grid):\n",
    "        # Handle hyperparameters that are the same for all models\n",
    "        param_grid = main_model_grid.copy()\n",
    "        scaler = range(-4, 4, 2)\n",
    "    \n",
    "    \n",
    "    def create_batch_params(self, batch_center):\n",
    "        max_batch_size = 4096\n",
    "        new_params = [batch_center//4, batch_center//2, batch_center*2, batch_center*4]\n",
    "        for i, batch_size in enumerate(new_params):\n",
    "            new_params[i] = min(batch_size, max_batch_size)\n",
    "        return set(new_params)\n",
    "    \n",
    "    def create_learning_rate_params(self, learning_rate_center):\n",
    "        min_learning_rate = 0.00001\n",
    "        new_learning_params = [learning_rate_center*10**2, learning_rate_center*10**1, (learning_rate_center*10)/2, learning_rate_center / 2, learning_rate_center*10**(-1), learning_rate_center*10**(-2)]\n",
    "        for i, rate in enumerate(new_learning_params):\n",
    "            new_learning_params[i] = max(rate, min_learning_rate)\n",
    "        return set(new_learning_params)\n",
    "\n",
    "    def create_epochs_params(self, epoch_center):\n",
    "        max_epochs = 150\n",
    "        new_epochs = [epoch_center - 20, epoch_center -10, epoch_center + 10, epoch_center +20]\n",
    "        for i in range(len(new_epochs)):\n",
    "            new_epochs[i] = min(max(new_epochs[i], 10), max_epochs)\n",
    "        return set(new_epochs)\n",
    "    \n",
    "    def create_optimizer_params(self, current_optimizer):\n",
    "        options = [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "        return options[options != current_optimizer]\n",
    "\n",
    "    def create_activation_params(self, current_activation, include_linear):\n",
    "        if include_linear:\n",
    "            options = [\"linear\", \"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        else: \n",
    "            options = [\"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        return options[options != current_activation]\n",
    "\n",
    "    def create_reg_params(self, current_reg):\n",
    "        max_reg = 0.3\n",
    "        new_reg = [current_reg*10**2, current_reg*10, (current_reg*10)/2, current_reg/2, current_reg*10**(-1), current_reg*10**(-2)]\n",
    "        for i in range(len(new_reg)):\n",
    "            new_reg[i] = min(new_reg[i], max_reg)\n",
    "        return set(new_reg)\n",
    "\n",
    "    def create_boolean_params(self, current_bool):\n",
    "        if current_bool:\n",
    "            return [False, False]\n",
    "        else:\n",
    "            return [True, True]\n",
    "\n",
    "class IncepTimeNarrowOptimizer(NarrowOptimizer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super.__init__()\n",
    "\n",
    "    def create_search_grid(self, main_model_dict):\n",
    "        # Create grid that is to be serched\n",
    "        pass\n",
    "\n",
    "    def create_nr_modules_modules(self, current_nr_modules):\n",
    "        max_modules = 30\n",
    "        new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "        for i in range(len(new_nr_modules)):\n",
    "            new_nr_modules[i] = min(max(new_nr_modules[i], 1), max_modules)\n",
    "        return set(new_nr_modules)\n",
    "    \n",
    "    def create_kernel_size(self, current_size):\n",
    "        max_size = 100\n",
    "        new_kernels = [current - 4, current -2, current +2, current +4]\n",
    "        for i, kern in enumerate(new_kernels):\n",
    "            new_kernels[i] = min(max(kern, 2), max_size)\n",
    "        return set(new_kernels)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Quick mode, starting of result file: results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv\n{'batch_size': 128, 'epochs': 100, 'learning_rate': 0.001, 'optimizer': 'adam', 'bottleneck_size': 28, 'kernel_size': 60, 'l1_r': 0.0, 'l2_r': 0.0001, 'module_activation': 'tanh', 'module_output_activation': 'sigmoid', 'nr_modules': 23, 'num_filters': 38, 'output_activation': 'sigmoid', 'reg_module': True, 'reg_shortcut': False, 'shortcut_activation': 'relu', 'use_bottleneck': True, 'use_residuals': True}\n"
     ]
    }
   ],
   "source": [
    "result_file_name = 'results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv'\n",
    "\n",
    "narrowOpt = NarrowOptimizer(0, quick_mode = True, continue_from_result_file = True)\n",
    "#top_10 = narrowOpt.get_best_model(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\n",
    "best_model_dict = narrowOpt.run(result_file_name, 2, ['val_accuracy', 'val_f1'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{   'batch_size': [128],\n    'bottleneck_size': [28],\n    'epochs': [100],\n    'kernel_size': [60],\n    'l1_r': [0.0],\n    'l2_r': [0.0001],\n    'learning_rate': [0.001],\n    'module_activation': ['tanh'],\n    'module_output_activation': ['sigmoid'],\n    'nr_modules': [23],\n    'num_filters': [38],\n    'optimizer': ['adam'],\n    'output_activation': ['sigmoid'],\n    'reg_module': [True],\n    'reg_shortcut': [False],\n    'shortcut_activation': ['relu'],\n    'use_bottleneck': [True],\n    'use_residuals': [True]}\n"
     ]
    }
   ],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(best_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{21, 24, 30}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "def dab(center):\n",
    "    max_modules = 30\n",
    "    new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "    for i in range(len(new_nr_modules)):\n",
    "        new_nr_modules[i] = min(new_nr_modules[i], max_modules)\n",
    "    return set(new_nr_modules)\n",
    "dab(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}