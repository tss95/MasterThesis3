{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\nINFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\nYour GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.NarrowSearchIncepTime import NarrowSearchIncepTime\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutup()\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 3\n",
      "{'noise': 105999, 'earthquake': 105999, 'explosion': 102808}\n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.1,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from itertools import chain\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "class NarrowOptimizer(GridSearchResultProcessor):\n",
    "\n",
    "    \"\"\"\n",
    "    This class functions as an heuristic that will attempt to reach a local minima. The class can either start off an existing search, or can start its own. The process looks a little like this:\n",
    "    1. Select the best model from existing result file (if using a file)\n",
    "    2. Use the best model / start model as the foundation. Create a search space around this in terms of hyperparameters.\n",
    "    3. Do a narrow search on the generated search space. If quick_mode = True, then if a better model is found during the narrow search, replace the base model with this and return to step 2. \n",
    "    4. Repeat steps 1-3\n",
    "    \n",
    "\n",
    "    Notes:\n",
    "    \n",
    "    - Would like this to be as robust as possible, and not dependent on InceptionTime. Want to be able to use this class for any model really.\n",
    "        - This can be challenging when creating dictionaries, as annoyingly, the models use 2 seperate dictionaries for initilization.\n",
    "    \n",
    "    Drawbacks, potential points of failure:\n",
    "     - The filtering method is very simple, and assumes that less than half of the good models are buggy. This is definitely not necessarily the case, and will cause this class to potentially try to optimize a lost cause. HOW CAN THIS BE SOLVED!?!?!?!? BY FIXING THE ORIGINAL BUG YOU DUMB FUCK\n",
    "     - The way results are processed, requires a model_grid and a hyper_grid. This design choice is the root of sooo many problems, and may lead to a less than robust implementation of this class. This can lead to different versions. Potential soution: Use this class as a parent class, and have children objects that are specialized for each type of model. \n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, use_highpass,\n",
    "                 use_tensorboard, use_liveplots, use_custom_callback, use_early_stopping, highpass_freq,\n",
    "                 use_reduced_lr, num_channels, depth, quick_mode = False, continue_from_result_file = False, \n",
    "                 result_file_name = \"\", start_grid = []):\n",
    "        \n",
    "        self.loadData = loadData\n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "        self.detrend = detrend\n",
    "        self.use_scaler = use_scaler\n",
    "        self.use_time_augmentor = use_time_augmentor\n",
    "        self.use_noise_augmentor = use_noise_augmentor\n",
    "        self.use_minmax = use_minmax\n",
    "        self.use_highpass = use_highpass\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.highpass_freq = highpass_freq\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.depth = depth\n",
    "        self.quick_mode = quick_mode\n",
    "        self.continue_from_result_file = continue_from_result_file\n",
    "        self.result_file_name = result_file_name\n",
    "        self.start_grid = start_grid\n",
    "\n",
    "        self.helper = HelperFunctions()\n",
    "        self.handler = DataHandler(self.loadData)\n",
    "\n",
    "        self.current_best_model = None\n",
    "        self.current_best_metrics = None\n",
    "\n",
    "    def run(self, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10):\n",
    "        \"\"\"\n",
    "        Self explanatory\n",
    "\n",
    "        PARAMS:\n",
    "        --------------\n",
    "        result_file_name: (str) Name of the file to be used. If continue_from_result == False, then this will not be used\n",
    "        num_classes: (int)\n",
    "        optimize_metric: [string, string] Optimization criteria. First element will be most significant.\n",
    "        nr_candidates: (int) Number of model candidates that will be considered in the first step sort.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.quick_mode:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Quick mode, starting of result file: {self.result_file_name}\")\n",
    "                fit_class = self.determine_model(self.result_file_name)\n",
    "                fit_class.run_quick_mode(optimize_metric, nr_candidates)\n",
    "                \n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        else:\n",
    "            if self.continue_from_result_file:\n",
    "                print(f\"Exhaustive mode, starting of result file: {result_file_name}\")\n",
    "\n",
    "                fit_class = self.determine_model(result_file_name)\n",
    "                fit_class.run_exhaustive_mode(optimize_metric, nr_candidates)\n",
    "            else:\n",
    "                raise Exception(\"Not continuing training from result file is not yet implemented. Suspected to be unused.\")\n",
    "        return\n",
    "\n",
    "    def determine_model(self, result_file_name):\n",
    "         name_list = result_file_name.split('_')\n",
    "         if \"InceptionTime\" in name_list:\n",
    "             return IncepTimeNarrowOptimizer(self.loadData, self.detrend, self.use_scaler, self.use_time_augmentor,\n",
    "                         self.use_noise_augmentor, self.use_minmax, self.use_highpass, self.use_tensorboard,\n",
    "                         self.use_liveplots, self.use_custom_callback, self.use_early_stopping, \n",
    "                         self.highpass_freq, self.use_reduced_lr, self.num_channels, self.depth, \n",
    "                         self.quick_mode, self.continue_from_result_file, self.result_file_name, self.start_grid)\n",
    "         else:\n",
    "            raise Excpetion(\"Other models have not yet been implemented in this class\")\n",
    "    \n",
    "    def quick_mode(self, result_file_name, num_classes, optimize_metric):\n",
    "        pass\n",
    "\n",
    "    def get_best_model(self, result_file_name, num_classes, optimize_metric, nr_candidates):\n",
    "        # Clear nan values\n",
    "        self.clear_nans(result_file_name)\n",
    "        results_df = self.get_results_df_by_name(result_file_name)\n",
    "        df_f1 = results_df.copy()\n",
    "        # Add f1 stats\n",
    "        df_f1 = self.add_f1_stats(df_f1)\n",
    "        # Sort by sort conditions\n",
    "        sorted_df = self.sort_df(df_f1, optimize_metric)\n",
    "        # Get the top nr_candidates\n",
    "        best_initial_candidates = sorted_df.copy().head(nr_candidates)\n",
    "        # Attempt to only select models which have the best f1 score, and first part of the sort conditions\n",
    "        # This is due to (likely) bug that has some models perform really well in one metric, but terrible in other metrics. The working assumption is that models with high f1, are good.\n",
    "        # TODO: Consider just switching the optimizer metrics here. Without the current BUG with strange training metrics (and inconsistent metrics wrt. the confusion matrix) this is a good opportunity to optimize with two metrics.\n",
    "        best_initial_sorted_by_f1 = self.sort_df(best_initial_candidates, ['val_f1', optimize_metric[0]])\n",
    "        # Select nr_candidates//2 of these models, and then resort them by their primary condition.\n",
    "        reduced_sorted_by_f1 = best_initial_sorted_by_f1.head(nr_candidates//2)\n",
    "        best_secondary_sorted_by_conditions = self.sort_df(reduced_sorted_by_f1, optimize_metric)\n",
    "        # At this point we should have filtered out bad outlier models, and be left with good candidates. \n",
    "        # We now select the best model according to the sort condidtions.\n",
    "        best_model = best_secondary_sorted_by_conditions.head(1)\n",
    "\n",
    "        return best_model\n",
    "\n",
    "    \n",
    "    def add_f1_stats(self, df_f1):\n",
    "        df_f1.columns=df_f1.columns.str.strip()\n",
    "        all_train_precision = df_f1['train_precision']\n",
    "        all_train_recall = df_f1['train_recall']\n",
    "        all_val_precision = df_f1['val_precision']\n",
    "        all_val_recall = df_f1['val_recall']\n",
    "        f1_train = self.create_f1_list(all_train_precision, all_train_recall)\n",
    "        f1_val = self.create_f1_list(all_val_precision, all_val_recall)\n",
    "        df_f1['train_f1'] = f1_train\n",
    "        df_f1['val_f1'] = f1_val\n",
    "        return df_f1\n",
    "\n",
    "    \n",
    "\n",
    "    def f1_score(self, precision, recall):\n",
    "        f1 = 2*((precision*recall)/(precision + recall))\n",
    "        return f1\n",
    "\n",
    "    def create_f1_list(self, precision_df, recall_df):\n",
    "        f1 = []\n",
    "        for i in range(len(precision_df)):\n",
    "            f1.append(self.f1_score(precision_df.loc[i], recall_df.loc[i]))\n",
    "        return f1\n",
    "\n",
    "        \n",
    "    def sort_df(self, df, sort_conditions):\n",
    "        ascending = False\n",
    "        if sort_conditions == ['val_loss', 'train_loss'] or sort_conditions == ['train_loss', 'val_loss']:\n",
    "            ascending = True\n",
    "        if 'val_loss' in sort_conditions and 'train_loss' not in sort_conditions:\n",
    "            raise Exception(\"Problematic sorting criteria. Cannot determine if sorting should be ascending or descending. A solution for this needs to be implemented in order for this to work\")\n",
    "        return df.sort_values(by=sort_conditions, axis = 0, ascending = ascending)\n",
    "\n",
    "    \"\"\"\n",
    "    def convert_best_model_to_main_grid(self, best_model):\n",
    "        model_dict = self.row_to_dict(best_model)\n",
    "\n",
    "\n",
    "    def row_to_dict(self, model_df):\n",
    "        keys = list(model_df.keys())\n",
    "        # Assumes 10 columns dedicated to results and the rest to hyperparams\n",
    "        hyper_keys = keys[:len(keys) - 10]\n",
    "        model_dict = model_df[:len(hyper_keys)].to_dict()\n",
    "        #del model_dict['index']\n",
    "        return model_dict\n",
    "    \"\"\"\n",
    "\n",
    "    def delete_metrics(self, best_model_df):\n",
    "        best_model_df = best_model_df[best_model_df.columns[:len(best_model_df.columns) - 10]]\n",
    "        return best_model_df\n",
    "\n",
    "    def adapt_best_model_dict(self, best_model_dict):\n",
    "        print(best_model_dict)\n",
    "        return {key:[value] for (key,value) in best_model_dict.items()}\n",
    "\n",
    "    def create_search_grid(self, main_model_grid):\n",
    "        # Handle hyperparameters that are the same for all models\n",
    "        param_grid = main_model_grid.copy()\n",
    "        scaler = range(-4, 4, 2)\n",
    "    \n",
    "    \n",
    "    def create_batch_params(self, batch_center):\n",
    "        max_batch_size = 4096\n",
    "        new_params = [batch_center//4, batch_center//2, batch_center*2, batch_center*4]\n",
    "        for i, batch_size in enumerate(new_params):\n",
    "            new_params[i] = min(batch_size, max_batch_size)\n",
    "        return list(set(new_params))\n",
    "    \n",
    "    def create_learning_rate_params(self, learning_rate_center):\n",
    "        min_learning_rate = 0.00001\n",
    "        new_learning_params = [learning_rate_center*10**2, learning_rate_center*10**1, (learning_rate_center*10)/2, learning_rate_center / 2, learning_rate_center*10**(-1), learning_rate_center*10**(-2)]\n",
    "        for i, rate in enumerate(new_learning_params):\n",
    "            new_learning_params[i] = max(rate, min_learning_rate)\n",
    "        return list(set(new_learning_params))\n",
    "\n",
    "    def create_epochs_params(self, epoch_center):\n",
    "        max_epochs = 150\n",
    "        new_epochs = [epoch_center - 20, epoch_center -10, epoch_center + 10, epoch_center +20]\n",
    "        for i in range(len(new_epochs)):\n",
    "            new_epochs[i] = min(max(new_epochs[i], 10), max_epochs)\n",
    "        return list(set(new_epochs))\n",
    "    \n",
    "    def create_optimizer_params(self, current_optimizer):\n",
    "        options = [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "        del options[options.index(current_optimizer)]\n",
    "        return options\n",
    "\n",
    "    def create_activation_params(self, current_activation, include_linear):\n",
    "        if include_linear:\n",
    "            options = [\"linear\", \"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        else: \n",
    "            options = [\"relu\", \"softmax\", \"tanh\", \"sigmoid\"]\n",
    "        del options[options.index(current_activation)]\n",
    "        return options\n",
    "\n",
    "    def create_reg_params(self, current_reg):\n",
    "        max_reg = 0.3\n",
    "        if current_reg == 0.0:\n",
    "            current_reg = 0.01\n",
    "        new_reg = [current_reg*10**2, current_reg*10, (current_reg*10)/2, current_reg/2, current_reg*10**(-1), current_reg*10**(-2)]\n",
    "        for i in range(len(new_reg)):\n",
    "            new_reg[i] = min(new_reg[i], max_reg)\n",
    "        return list(set(new_reg))\n",
    "\n",
    "    def create_boolean_params(self, current_bool):\n",
    "        if current_bool:\n",
    "            return [False, False]\n",
    "        else:\n",
    "            return [True, True]\n",
    "\n",
    "    def create_output_activation(self, current):\n",
    "        return [current]\n",
    "\n",
    "    def get_metrics(self, model, optimize_metric):\n",
    "        print(model)\n",
    "        print(optimize_metric)\n",
    "        return model[optimize_metric[0]].iloc[0], model[optimize_metric[1]].iloc[0]\n",
    "\n",
    "    def create_search_space(self, main_grid, search_grid):\n",
    "        key_list = list(main_grid.keys())\n",
    "        np.random.shuffle(key_list)\n",
    "        search_list = []\n",
    "        for key in key_list:\n",
    "            if len(search_grid[key]) > 1:\n",
    "                one_model = main_grid.copy()\n",
    "                one_model[key] = search_grid[key]\n",
    "                key_grid = list(ParameterGrid(one_model))\n",
    "                search_list.append(key_grid)\n",
    "            else:\n",
    "                continue\n",
    "        search_list = list(chain.from_iterable(search_list))\n",
    "        pprint.pprint(search_list)\n",
    "        return search_list\n",
    "\n",
    "    def is_new_model_better(self, previous_best_metrics, current_best_metrics):\n",
    "        if previous_best_metrics == current_best_metrics:\n",
    "            print(\"The models have the same metrics. Assumed to be the same models.\")\n",
    "            return False\n",
    "        if previous_best_metrics[0] > current_best_metrics[0]:\n",
    "            print(\"The first optimizer metric is worse on the new model by \", current_best_metrics[0] - previous_best_metric[0])\n",
    "            if previous_best_metrics[1] < current_best_metrics[1]:\n",
    "                print(\"The second optimizer metric is better on the new model by \", current_best_metrics[1] - previous_best_metrics[1])\n",
    "                # TODO: Consider a better way to handle this. There could possibly be a solution where if the second metric is better, but not the first, the first metric cannot be worse by some condition, e.g. 0.05.\n",
    "                # An attempt is implemented below, but by now means perfect. \n",
    "                if previous_best_metric[0] - current_best_metrics[0] > 0.05:\n",
    "                    return False\n",
    "                return True\n",
    "        else:\n",
    "            if current_best_metrics[0] < 0.5 or current_best_metrics[1] < 0.5:\n",
    "                return False\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "class IncepTimeNarrowOptimizer(NarrowOptimizer):\n",
    "\n",
    "    def __init__(self, loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, use_highpass, use_tensorboard, use_liveplots, use_custom_callback, use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, quick_mode = False, continue_from_result_file = False, \n",
    "                result_file_name = \"\", start_grid = []):\n",
    "        super().__init__(loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, \n",
    "                         use_highpass, use_tensorboard, use_liveplots, use_custom_callback, \n",
    "                         use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, \n",
    "                         quick_mode, continue_from_result_file, result_file_name, start_grid)\n",
    "        self.model_nr_type = \"InceptionTime\"\n",
    "        \n",
    "    \n",
    "    def run_exhaustive_mode(self, optimize_metric, nr_candidates):\n",
    "        \"\"\"\n",
    "        This function starts of by chosing the best model from the current results file, based on the user defined metrics.\n",
    "\n",
    "        The model will then create a search space that is near what the current best model is.\n",
    "\n",
    "        Then the model will train till completion on this search space. One of two things will happen:\n",
    "\n",
    "        1. If none of the newly trained models are better than the current, then we can do one of three things:\n",
    "            1.1: End training. Assume we have reached some kind of minima\n",
    "            1.2 Select the second best model and train on this. This means that the heuristic will never naturally end.\n",
    "        2. If any of the models are better than the current best model, start this process again.\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        # Due to rewrite of best_model, we need to store the best model in the last iteration\n",
    "        previous_best_model = None\n",
    "        previous_best_metrics = None\n",
    "        if self.current_best_model != None and self.current_best_metrics != None:\n",
    "            previous_best_model = self.current_best_model\n",
    "            previous_best_metrics = self.current_best_metrics\n",
    "\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        # To start of I will only implement what to do when we are continuing off existing file.\n",
    "        self.current_best_model = self.get_best_model(self.result_file_name, self.num_classes, optimize_metric, nr_candidates)\n",
    "        self.current_best_metrics = self.get_metrics(self.current_best_model, optimize_metric)\n",
    "\n",
    "        if previous_best_model != None:\n",
    "            # This is where we determine whether the new model is better than the previous model.\n",
    "            if not self.is_new_model_better(previous_best_metrics, self.current_best_metrics):\n",
    "                print(\"The new model is not better\")\n",
    "                return\n",
    "\n",
    "\n",
    "        print(f\"Current best metrics: {optimize_metric[0]} = {self.current_best_metrics[0]}, {optimize_metric[1]} = {self.current_best_metrics[1]}\")\n",
    "        best_model_dict = self.delete_metrics(self.current_best_model).iloc[0].to_dict()\n",
    "        print(\"Gained with this model:\")\n",
    "        pp.pprint(best_model_dict)\n",
    "        search_grid = self.create_search_grid(best_model_dict)\n",
    "        print(\"Which will be explored with this search space:\")\n",
    "        pp.pprint(search_grid)\n",
    "        search_space = self.create_search_space(self.adapt_best_model_dict(best_model_dict), search_grid)\n",
    "        print(\"Current search space is of length: \", len(search_space))\n",
    "\n",
    "        assert self.get_results_file_name(narrow = True) != self.result_file_name, f\"{self.get_results_file_name(narrow = True)} != {self.result_file_name}\"\n",
    "        \n",
    "        ramLoader = RamLoader(self.loadData, \n",
    "                              self.handler, \n",
    "                              use_time_augmentor = self.use_time_augmentor, \n",
    "                              use_noise_augmentor = self.use_noise_augmentor, \n",
    "                              use_scaler = self.use_scaler,\n",
    "                              use_minmax = self.use_minmax, \n",
    "                              use_highpass = self.use_highpass, \n",
    "                              highpass_freq = self.highpass_freq, \n",
    "                              detrend = self.detrend, \n",
    "                              load_test_set = False)\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val, self.timeAug, self.scaler, self.noiseAug = ramLoader.load_to_ram(False, self.num_channels)\n",
    "        \n",
    "        self.results_df = self.initiate_results_df_opti(self.result_file_name, \n",
    "                                                        self.num_classes, \n",
    "                                                        False, \n",
    "                                                        best_model_dict)\n",
    "\n",
    "\n",
    "\n",
    "        # Everything prior to the for loop should be general enough to work for any model\n",
    "        for i in range(len(search_space)):\n",
    "            # Housekeeping\n",
    "            tf.keras.backend.clear_session()\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "            print(f\"Model nr {i + 1} of {len(search_space)}\")\n",
    "\n",
    "            batch_size = search_space[i][\"batch_size\"]\n",
    "            epochs = search_space[i][\"epochs\"]\n",
    "            learning_rate = search_space[i][\"learning_rate\"]\n",
    "            opt = self.helper.get_optimizer(search_space[i][\"optimizer\"], learning_rate)\n",
    "            \n",
    "            use_residuals = search_space[i][\"use_residuals\"]\n",
    "            use_bottleneck = search_space[i][\"use_bottleneck\"]\n",
    "            nr_modules = search_space[i][\"nr_modules\"]\n",
    "            kernel_size = search_space[i][\"kernel_size\"]\n",
    "            bottleneck_size = search_space[i][\"bottleneck_size\"]\n",
    "            num_filters =  search_space[i][\"num_filters\"]\n",
    "            shortcut_activation = search_space[i][\"shortcut_activation\"]\n",
    "            module_activation = search_space[i][\"module_activation\"]\n",
    "            module_output_activation = search_space[i][\"module_output_activation\"]\n",
    "            output_activation = search_space[i][\"output_activation\"]\n",
    "\n",
    "            reg_shortcut = search_space[i][\"reg_shortcut\"]\n",
    "            reg_module = search_space[i][\"reg_module\"]\n",
    "            l1_r = search_space[i][\"l1_r\"]\n",
    "            l2_r = search_space[i][\"l2_r\"]\n",
    "\n",
    "            self.results_df = self.store_params_before_fit_opti(search_space[i], self.results_df, self.result_file_name)\n",
    "            \n",
    "            # Generate build model args using the picks from above.\n",
    "            _, channels, timesteps = self.handler.get_trace_shape_no_cast(self.loadData.train, self.use_time_augmentor)\n",
    "            input_shape = (channels, timesteps)\n",
    "\n",
    "            model_args = self.helper.generate_inceptionTime_build_args(input_shape, self.num_classes, opt,\n",
    "                                                                      use_residuals, use_bottleneck, nr_modules,\n",
    "                                                                      kernel_size, num_filters, bottleneck_size,\n",
    "                                                                      shortcut_activation, module_activation,\n",
    "                                                                      module_output_activation, output_activation,\n",
    "                                                                      reg_shortcut, reg_module, l1_r, l2_r)\n",
    "            # Build model using args generated above\n",
    "            inceptionTime = InceptionTimeModel(**model_args)\n",
    "            model = inceptionTime.build_model(input_shape, self.num_classes)\n",
    "\n",
    "            # Initializing generators:\n",
    "            gen = RamGenerator(self.loadData, self.handler, self.noiseAug)\n",
    "            train_gen = gen.data_generator(self.x_train, self.y_train, batch_size)\n",
    "            val_gen = gen.data_generator(self.x_val, self.y_val, batch_size)\n",
    "\n",
    "            # Generate fit args using picks.\n",
    "            fit_args = self.helper.generate_fit_args(self.loadData.train, self.loadData.val_ds, batch_size, \n",
    "                                                     epochs, val_gen, use_tensorboard = self.use_tensorboard, \n",
    "                                                     use_liveplots = self.use_liveplots, \n",
    "                                                     use_custom_callback = self.use_custom_callback,\n",
    "                                                     use_early_stopping = self.use_early_stopping,\n",
    "                                                     use_reduced_lr = self.use_reduced_lr)\n",
    "\n",
    "            # Fit the model using the generated args\n",
    "            try:\n",
    "                model.fit(train_gen, **fit_args)\n",
    "                \n",
    "                # Evaluate the fitted model on the validation set\n",
    "                loss, accuracy, precision, recall = model.evaluate_generator(generator=val_gen,\n",
    "                                                                        steps=self.helper.get_steps_per_epoch(self.loadData.val, batch_size))\n",
    "                # Record metrics for train\n",
    "                metrics = {}\n",
    "                metrics['val'] = {  \"val_loss\" : loss,\n",
    "                                    \"val_accuracy\" : accuracy,\n",
    "                                    \"val_precision\": precision,\n",
    "                                    \"val_recall\" : recall}\n",
    "                \n",
    "                # Evaluate the fitted model on the train set\n",
    "                # Likely very redundant\n",
    "                train_loss, train_accuracy, train_precision, train_recall = model.evaluate_generator(generator=train_gen,\n",
    "                                                                                            steps=self.helper.get_steps_per_epoch(self.loadData.train,\n",
    "                                                                                                                                batch_size))\n",
    "                metrics['train'] = { \"train_loss\" : train_loss,\n",
    "                                    \"train_accuracy\" : train_accuracy,\n",
    "                                    \"train_precision\": train_precision,\n",
    "                                    \"train_recall\" : train_recall}\n",
    "                self.results_df = self.store_metrics_after_fit(metrics, self.results_df, self.result_file_name)\n",
    "\n",
    "            except Exception:\n",
    "                print(\"Error (hopefully) occured during training.\")\n",
    "                continue\n",
    "        self.run_exhaustive_mode(optimize_metric, nr_candidates)\n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def run_quick_mode(self, optimize_metric, nr_candidates):\n",
    "        raise Exception(\"Quick mode has not yet been implemented\")\n",
    "\n",
    "\n",
    "    def create_search_grid(self, main_grid):\n",
    "        # This is the least robust function in this class. \n",
    "        return {'batch_size' : self.create_batch_params(main_grid['batch_size']),\n",
    "                     'epochs' : self.create_epochs_params(main_grid['epochs']),\n",
    "                     'learning_rate' : self.create_learning_rate_params(main_grid['learning_rate']),\n",
    "                     'optimizer' : self.create_optimizer_params(main_grid['optimizer']),\n",
    "                     'bottleneck_size' : self.create_bottleneck_size(main_grid['bottleneck_size']),\n",
    "                     'kernel_size' : self.create_kernel_and_filter_params(main_grid['kernel_size']),\n",
    "                     'l1_r' : self.create_reg_params(main_grid['l1_r']),\n",
    "                     'l2_r' : self.create_reg_params(main_grid['l2_r']),\n",
    "                     'module_activation' : self.create_activation_params(main_grid['module_activation'], include_linear = True),\n",
    "                     'module_output_activation' : self.create_activation_params(main_grid['module_output_activation'], include_linear = True),\n",
    "                     'nr_modules' : self.create_nr_modules_params(main_grid['nr_modules']),\n",
    "                     'num_filters' : self.create_kernel_and_filter_params(main_grid['num_filters']),\n",
    "                     'output_activation' : self.create_output_activation(main_grid['output_activation']),\n",
    "                     'reg_module' : self.create_boolean_params(main_grid['reg_module']),\n",
    "                     'reg_shortcut' : self.create_boolean_params(main_grid['reg_shortcut']),\n",
    "                     'shortcut_activation' : self.create_activation_params(main_grid['shortcut_activation'], include_linear = False),\n",
    "                     'use_bottleneck' : self.create_boolean_params(main_grid['use_bottleneck']),\n",
    "                     'use_residuals' : self.create_boolean_params(main_grid['use_residuals'])}\n",
    "\n",
    "    def create_nr_modules_params(self, center):\n",
    "        max_modules = 30\n",
    "        new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "        for i in range(len(new_nr_modules)):\n",
    "            new_nr_modules[i] = min(max(new_nr_modules[i], 1), max_modules)\n",
    "        return list(set(new_nr_modules))\n",
    "    \n",
    "    def create_kernel_and_filter_params(self, current):\n",
    "        max_size = 120\n",
    "        new_kernels = [current - 20, current - 10, current - 2, current + 2, current + 10, current + 20]\n",
    "        for i, kern in enumerate(new_kernels):\n",
    "            new_kernels[i] = min(max(kern, 2), max_size)\n",
    "        return list(set(new_kernels))\n",
    "\n",
    "    def create_bottleneck_size(self, current_nr):\n",
    "        max_nr = 100\n",
    "        new_bottleneck = [current_nr - 4, current_nr - 2, current_nr + 2, current_nr + 4]\n",
    "        for i, neck in enumerate(new_bottleneck):\n",
    "            new_bottleneck[i] = min(max(neck, 2), max_nr)\n",
    "        return list(set(new_bottleneck))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exhaustive mode, starting of result file: results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv\nEmpty DataFrame\nColumns: [batch_size, epochs, learning_rate, optimizer, bottleneck_size, kernel_size, l1_r, l2_r, module_activation, module_output_activation, nr_modules, num_filters, output_activation, reg_module, reg_shortcut, shortcut_activation, use_bottleneck, use_residuals, train_loss, train_accuracy, train_precision, train_recall, val_loss, val_accuracy, val_precision, val_recall, train_f1, val_f1]\nIndex: []\n\n[0 rows x 28 columns]\n['val_accuracy', 'val_f1']\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fb6f43956141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#top_10 = narrowOpt.get_best_model(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mbest_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnarrowOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-5020ce49e09b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, optimize_metric, nr_candidates)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mfit_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetermine_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mfit_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_exhaustive_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Not continuing training from result file is not yet implemented. Suspected to be unused.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5020ce49e09b>\u001b[0m in \u001b[0;36mrun_exhaustive_mode\u001b[0;34m(self, optimize_metric, nr_candidates)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# To start of I will only implement what to do when we are continuing off existing file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_best_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_best_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_best_model\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-5020ce49e09b>\u001b[0m in \u001b[0;36mget_metrics\u001b[0;34m(self, model, optimize_metric)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize_metric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimize_metric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_search_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "num_channels = 3\n",
    "\n",
    "use_time_augmentor = True\n",
    "use_scaler = True\n",
    "use_noise_augmentor = True\n",
    "detrend = True\n",
    "use_minmax = False\n",
    "use_highpass = True\n",
    "highpass_freq = 0.1\n",
    "\n",
    "use_tensorboard = True\n",
    "use_liveplots = False\n",
    "use_custom_callback = False\n",
    "use_early_stopping = True\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = True\n",
    "\n",
    "result_file_name = 'results_InceptionTime_NARROW_noiseNotNoise_detrend_timeAug_sscale_noiseAug_earlyS_highpass-0.1.csv'\n",
    "quick_mode = False\n",
    "continue_from_result_file = True\n",
    "start_grid = None\n",
    "\n",
    "depth = 5\n",
    "\n",
    "\n",
    "\n",
    "narrowOpt = NarrowOptimizer(loadData, detrend, use_scaler, use_time_augmentor, use_noise_augmentor, use_minmax, \n",
    "                            use_highpass, use_tensorboard, use_liveplots, use_custom_callback, \n",
    "                            use_early_stopping, highpass_freq, use_reduced_lr, num_channels, depth, \n",
    "                            quick_mode, continue_from_result_file, result_file_name, start_grid)\n",
    "\n",
    "#top_10 = narrowOpt.get_best_model(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\n",
    "best_model_dict = narrowOpt.run(['val_accuracy', 'val_f1'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 14 required positional arguments: 'detrend', 'use_scaler', 'use_time_augmentor', 'use_noise_augmentor', 'use_minmax', 'use_highpass', 'use_tensorboard', 'use_liveplots', 'use_custom_callback', 'use_early_stopping', 'highpass_freq', 'use_reduced_lr', 'num_channels', and 'depth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-f616878cd507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mincepTimeOpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIncepTimeNarrowOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquick_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from_result_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincepTimeOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mincepTimeOpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_search_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 14 required positional arguments: 'detrend', 'use_scaler', 'use_time_augmentor', 'use_noise_augmentor', 'use_minmax', 'use_highpass', 'use_tensorboard', 'use_liveplots', 'use_custom_callback', 'use_early_stopping', 'highpass_freq', 'use_reduced_lr', 'num_channels', and 'depth'"
     ]
    }
   ],
   "source": [
    "incepTimeOpt = IncepTimeNarrowOptimizer(0, quick_mode = True, continue_from_result_file = True)\n",
    "best_model_dict = incepTimeOpt.run(result_file_name, 2, optimize_metric = ['val_accuracy', 'val_f1'], nr_candidates = 10)\n",
    "incepTimeOpt.create_search_grid(best_model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(narrowOpt.current_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{21, 24, 30}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "def dab(center):\n",
    "    max_modules = 30\n",
    "    new_nr_modules = [center - 6, center - 3, center + 3, center + 6]\n",
    "    for i in range(len(new_nr_modules)):\n",
    "        new_nr_modules[i] = min(new_nr_modules[i], max_modules)\n",
    "    return set(new_nr_modules)\n",
    "dab(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}