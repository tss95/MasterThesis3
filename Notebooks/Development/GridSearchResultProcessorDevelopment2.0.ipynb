{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "os.chdir(utils.base_dir)\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "gen_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/Classes/DataProcessing'\n",
    "os.chdir(gen_dir)\n",
    "import ts_RamGenerator\n",
    "os.chdir(base_dir)\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "divided-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Mapping redundancy: [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded explosion and earthquake dataset:\n",
      "Evenly balanced among classes in the train set.\n",
      "As well as non train sets.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Full ds:\n",
      "earthquake: (10554, 0.5066)  |  explosion: (10279, 0.4934)  \n",
      "Train ds:\n",
      "earthquake: (7962, 0.5096)  |  explosion: (7662, 0.4904)  \n",
      "Val ds:\n",
      "earthquake: (1567, 0.5014)  |  explosion: (1558, 0.4986)  \n",
      "Test ds:\n",
      "earthquake: (1025, 0.4918)  |  explosion: (1059, 0.5082)  \n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.1,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "altered-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import os\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "\n",
    "class GridSearchResultProcessor():\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    \n",
    "    def create_results_df(self, hyper_picks, model_picks):\n",
    "        hyper_keys = list(hyper_picks.keys())\n",
    "        model_keys = list(model_picks.keys())\n",
    "        metrics_train_keys = [\"train_loss\", \"train_accuracy\", \"train_precision\", \"train_recall\"]\n",
    "        metrics_val_keys = [\"val_loss\", \"val_accuracy\", \"val_precision\", \"val_recall\"]\n",
    "        header = np.concatenate((hyper_keys, model_keys, metrics_train_keys, metrics_val_keys))\n",
    "        results_df = pd.DataFrame(columns = header)\n",
    "        return results_df\n",
    "\n",
    "    def create_results_df_opti(self, current_picks):\n",
    "        keys = list(current_picks.keys())\n",
    "        metrics_train_keys = [\"train_loss\", \"train_accuracy\", \"train_precision\", \"train_recall\"]\n",
    "        metrics_val_keys = [\"val_loss\", \"val_accuracy\", \"val_precision\", \"val_recall\"]\n",
    "        confusion_matrix_key = [\"confusion_matrix\"]\n",
    "        header = np.concatenate((keys, metrics_train_keys, metrics_val_keys, confusion_matrix_key))\n",
    "        results_df = pd.DataFrame(columns = header)\n",
    "        return results_df\n",
    "    \n",
    "    def initiate_results_df(self, file_name, num_classes, start_from_scratch, hyper_picks, model_picks):\n",
    "        if start_from_scratch:\n",
    "            self.clear_results_df(file_name)\n",
    "            return self.create_results_df(hyper_picks, model_picks)\n",
    "        else:\n",
    "            if self.does_result_exist(file_name):\n",
    "                file_name = file_name.split('/')[-1]\n",
    "                results_df = self.get_results_df_by_name(file_name)\n",
    "                return results_df\n",
    "            else:\n",
    "                return self.create_results_df(hyper_picks, model_picks)\n",
    "\n",
    "    def initiate_results_df_opti(self, file_name, num_classes, start_from_scratch, search_picks):\n",
    "        if start_from_scratch:\n",
    "            self.clear_results_df(file_name)\n",
    "            return self.create_results_df_opti(search_picks)\n",
    "        else:\n",
    "            if self.does_result_exist(file_name) or self.does_result_exist(f\"{self.get_results_file_path()}/{file_name}\"):\n",
    "                #file_name = file_name.split('/')[-1]\n",
    "                results_df = self.get_results_df_by_name(file_name)\n",
    "                return results_df\n",
    "            else:\n",
    "                return self.create_results_df_opti(search_picks)\n",
    "\n",
    "        \n",
    "    def does_result_exist(self, file_name):\n",
    "        if isfile(f\"{self.get_results_file_path()}/{file_name}\"):\n",
    "            return True\n",
    "        return isfile(file_name)\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    def save_results_df(self, results_df, file_name):\n",
    "        results_df.to_csv(file_name, mode = 'w', index=False)\n",
    "    \"\"\"\n",
    "    def clear_results_df(self, file_name):\n",
    "        path = self.get_results_file_path()\n",
    "        file = f\"{path}/{file_name}\"\n",
    "        if os.path.isfile(file):\n",
    "            f = open(file, \"w+\")\n",
    "            f.close()\n",
    "        \n",
    "    \n",
    "    def get_results_file_name(self, narrow = False, narrowOpt = False):\n",
    "        file_name = f\"results_{self.model_nr_type}\"\n",
    "        if narrow:\n",
    "            file_name = f\"{file_name}_NARROW\"\n",
    "        if narrowOpt:\n",
    "            file_name = f\"{file_name}_NarrowOpt\"\n",
    "        if self.loadData.earth_explo_only:\n",
    "            file_name = f\"{file_name}_earthExplo\"\n",
    "        if self.loadData.noise_earth_only:\n",
    "            file_name = f\"{file_name}_noiseEarth\"\n",
    "        if self.loadData.noise_not_noise:\n",
    "            file_name = f\"{file_name}_noiseNotNoise\"\n",
    "        if self.filter_name != None:\n",
    "            file_name = f\"{file_name}_{self.filter_name}\"\n",
    "            if self.filter_name == \"bandpass\":\n",
    "                file_name = f\"{file_name}-{self.band_min}-{self.band_max}\"\n",
    "            if self.filter_name == \"highpass\":\n",
    "                file_name = f\"{file_name}-{self.highpass_freq}\"\n",
    "        if self.use_time_augmentor:\n",
    "            file_name = f\"{file_name}_timeAug\"\n",
    "        if self.scaler_name:\n",
    "            file_name == file_name + f\"{self.scaler_name}\"\n",
    "        if self.use_noise_augmentor:\n",
    "            file_name = f\"{file_name}_noiseAug\"\n",
    "        if self.use_early_stopping:\n",
    "            file_name = f\"{file_name}_earlyS\"\n",
    "        file_name = file_name + f\"_numChannels-{self.num_channels}\"\n",
    "        file_name = file_name + \".csv\"\n",
    "        return file_name\n",
    "    \n",
    "    def get_results_file_path(self):\n",
    "        file_path = f'{utils.base_dir}/GridSearchResults/{self.num_classes}_classes'\n",
    "        return file_path\n",
    "    \n",
    "    def store_params_before_fit(self, current_picks, results_df, file_name):\n",
    "\n",
    "        hyper_params = current_picks[1]\n",
    "        model_params = current_picks[2]\n",
    "        picks = []\n",
    "        for key in list(hyper_params.keys()):\n",
    "            picks.append(hyper_params[key])\n",
    "        for key in list(model_params.keys()):\n",
    "            picks.append(model_params[key])\n",
    "        nr_fillers = len(results_df.columns) - len(picks)\n",
    "        for i in range(nr_fillers):\n",
    "            picks.append(np.nan)\n",
    "        temp_df = pd.DataFrame(np.array(picks, dtype = object).reshape(1,len(results_df.columns)), columns = results_df.columns)\n",
    "        results_df = results_df.append(temp_df, ignore_index = True)\n",
    "        for idx, column in enumerate(results_df.columns):\n",
    "            if idx >= len(picks):\n",
    "                results_df[column] = results_df[column].astype('float')\n",
    "        self.save_results_df(results_df, file_name)\n",
    "        return results_df\n",
    "\n",
    "    def store_params_before_fit_opti(self, current_picks, results_df, file_name):\n",
    "        columns = results_df.columns\n",
    "        filled_dict = {}\n",
    "        for column in columns:\n",
    "            if column not in list(current_picks.keys()):\n",
    "                current_picks[column] = np.nan\n",
    "            filled_dict[column] = [current_picks[column]]\n",
    "        for idx, column in enumerate(columns):\n",
    "            assert column == list(filled_dict.keys())[idx], print(f\"True order: {columns}. Created order: {list(filled_dict.keys())}\")\n",
    "        temp = pd.DataFrame.from_dict(filled_dict, orient = \"columns\")\n",
    "        temp = temp.reindex(results_df.columns, axis = 1)\n",
    "        results_df = results_df.append(temp, ignore_index = True)\n",
    "        self.save_results_df(results_df, file_name)\n",
    "        return results_df\n",
    "\n",
    "\n",
    "\n",
    "    def store_metrics_after_fit(self, metrics, confusion_matrix, results_df, file_name):\n",
    "        results_df = results_df.replace('nan', np.nan)\n",
    "        unfinished_columns = results_df.columns[results_df.isna().any()].tolist()\n",
    "        for column in unfinished_columns:\n",
    "            if column == \"confusion_matrix\":\n",
    "                results_df.iloc[-1, results_df.columns.get_loc(column)] = str(np.array(confusion_matrix))\n",
    "            else:\n",
    "                results_df.iloc[-1, results_df.columns.get_loc(column)] = metrics[column.split('_')[0]][column]\n",
    "        self.save_results_df(results_df, file_name)\n",
    "        return results_df\n",
    "\n",
    "    \n",
    "    def find_best_performers(self, results_df):\n",
    "        train_loss_index = results_df.columns.get_loc('train_loss')\n",
    "        metrics_df = results_df[results_df.columns[train_loss_index:]]\n",
    "        min_loss = {'train_loss' : min(metrics_df['train_loss']), 'val_loss' : min(metrics_df['val_loss']), \n",
    "                    'train_index' : metrics_df[metrics_df['train_loss'] == min(metrics_df['train_loss'])].index[0], \n",
    "                    'val_index' : metrics_df[metrics_df['val_loss'] == min(metrics_df['val_loss'])].index[0]}\n",
    "\n",
    "        max_accuracy = {'train_accuracy' : max(metrics_df['train_accuracy']), 'val_accuracy' : max(metrics_df['val_accuracy']), \n",
    "                        'train_index' : metrics_df[metrics_df['train_accuracy'] == max(metrics_df['train_accuracy'])].index[0], \n",
    "                        'val_index' : metrics_df[metrics_df['val_accuracy'] == max(metrics_df['val_accuracy'])].index[0]}\n",
    "\n",
    "        max_precision = {'train_precision' : max(metrics_df['train_precision']), 'val_precision' : max(metrics_df['val_precision']), \n",
    "                         'train_index' : metrics_df[metrics_df['train_precision'] == max(metrics_df['train_precision'])].index[0], \n",
    "                         'val_index' : metrics_df[metrics_df['val_precision'] == max(metrics_df['val_precision'])].index[0]}\n",
    "\n",
    "        max_recall = {'train_recall' : max(metrics_df['train_recall']), 'val_recall' : max(metrics_df['train_recall']), \n",
    "                      'train_index' : metrics_df[metrics_df['train_recall'] == max(metrics_df['train_recall'])].index[0], \n",
    "                      'val_index' : metrics_df[metrics_df['val_recall'] == max(metrics_df['val_recall'])].index[0]}\n",
    "\n",
    "        return min_loss, max_accuracy, max_precision, max_recall\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: The functions below are largely redundant, and should be streamlined in the future\n",
    "\n",
    "    They are included as is in order to speed up the development of NarrowOptimizer\n",
    "    \"\"\"\n",
    "\n",
    "    def get_results_df_by_name(self, file_name):\n",
    "        file_path = f\"{utils.base_dir}/GridSearchResults/{self.num_classes}_classes\"\n",
    "        loaded_df = pd.read_csv(file_path+'/'+file_name)\n",
    "        return loaded_df\n",
    "\n",
    "\n",
    "    def clear_nans(self, result_file_name):\n",
    "        df = self.get_results_df_by_name(result_file_name)\n",
    "        df_copy = df.copy()\n",
    "        no_nans = df_copy.dropna()\n",
    "        self.clear_results_df(result_file_name)\n",
    "        self.save_results_df(no_nans, result_file_name)\n",
    "\n",
    "    def save_results_df(self, results_df, file_name):\n",
    "        #results_df.to_csv(file_name, mode = 'w', index=False)\n",
    "        print(f\"Saving file. {len(results_df)} rows.\")\n",
    "        print(f\"{file_name} saved to path:   {self.get_results_file_path()}\")\n",
    "        results_df.to_csv(f\"{self.get_results_file_path()}/{file_name}\", mode = 'w', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "stunning-uncertainty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "\n",
    "import os\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "#from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.DataProcessing.ts_RamGenerator import data_generator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "import re\n",
    "import json\n",
    "\n",
    "class RGS(GridSearchResultProcessor):\n",
    "    \n",
    "    def __init__(self, loadData, train_ds, val_ds, test_ds, model_type, scaler_name, use_time_augmentor, use_noise_augmentor,\n",
    "                 filter_name, n_picks, hyper_grid, use_tensorboard = False, \n",
    "                 use_liveplots = True, use_custom_callback = False, use_early_stopping = False, use_reduced_lr = False,\n",
    "                 band_min = 2.0, band_max = 4.0, highpass_freq = 0.1, start_from_scratch = True, is_lstm = False, \n",
    "                 log_data = True, num_channels = 3):\n",
    "        \n",
    "        self.loadData = loadData\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.model_nr_type = model_type\n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "\n",
    "        self.scaler_name = scaler_name\n",
    "        self.use_noise_augmentor = use_noise_augmentor\n",
    "        self.use_time_augmentor = use_time_augmentor\n",
    "        self.filter_name = filter_name\n",
    "        self.n_picks = n_picks\n",
    "        self.hyper_grid = hyper_grid\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "\n",
    "        self.band_min = band_min\n",
    "        self.band_max = band_max\n",
    "        self.highpass_freq = highpass_freq\n",
    "        self.start_from_scratch = start_from_scratch\n",
    "        self.is_lstm = is_lstm\n",
    "        self.log_data = log_data\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.helper = HelperFunctions()\n",
    "        self.handler = DataHandler(self.loadData)\n",
    "        self.is_dynamic = False\n",
    "        if type(self.model_nr_type) == str:\n",
    "            self.is_dynamic = True\n",
    "\n",
    "            \n",
    "\n",
    "    def fit(self):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        # Creating grid:\n",
    "        self.p = ParameterGrid(self.hyper_grid)\n",
    "        if len(self.p) < self.n_picks:\n",
    "            self.n_picks = len(self.p)\n",
    "            print(f\"Picks higher than max. Reducing picks to {self.n_picks} picks\")\n",
    "        self.p = self.get_n_params_from_list(self.p, self.n_picks)\n",
    "        \n",
    "        # Create name of results file, get initiated results df, either brand new or continue old.\n",
    "        self.results_file_name = self.get_results_file_name()\n",
    "        print(self.results_file_name)\n",
    "        if self.start_from_scratch:\n",
    "            confirmation = input(\"Are you sure you want to erase the results file? Y/N \\n\").upper()\n",
    "            if confirmation != \"Y\":\n",
    "                print(\"Terminating process.\")\n",
    "                return\n",
    "        self.results_df = self.initiate_results_df_opti(self.results_file_name, self.num_classes, self.start_from_scratch, self.p[0])\n",
    "        print(self.results_df)\n",
    "        # Preprocessing and loading all data to RAM:\n",
    "        ramLoader = RamLoader(self.loadData, \n",
    "                              self.handler, \n",
    "                              use_time_augmentor = self.use_time_augmentor, \n",
    "                              use_noise_augmentor = self.use_noise_augmentor, \n",
    "                              scaler_name = self.scaler_name,\n",
    "                              filter_name = self.filter_name, \n",
    "                              band_min = self.band_min,\n",
    "                              band_max = self.band_max,\n",
    "                              highpass_freq = self.highpass_freq, \n",
    "                              load_test_set = False)\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val, self.timeAug, self.scaler, self.noiseAug = ramLoader.load_to_ram()\n",
    "\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.p)):\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "            tf.config.optimizer.set_jit(True)\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "            model_info = {\"model_nr_type\" : self.model_nr_type, \"index\" : i}\n",
    "            print(f\"Model nr {i + 1} of {len(self.p)}\")           \n",
    "            # Translate picks to a more readable format:\n",
    "            num_channels = self.num_channels\n",
    "            epoch = self.p[i][\"epochs\"]\n",
    "            batch_size = self.p[i][\"batch_size\"]\n",
    "            \n",
    "            opt = self.helper.get_optimizer(self.p[i][\"optimizer\"], self.p[i][\"learning_rate\"])\n",
    "    \n",
    "\n",
    "            if \"decay_sequence\" in self.p[i]:\n",
    "                if \"num_filters\" in self.p[i]:\n",
    "                    units_or_num_filters = self.p[i][\"num_filters\"]\n",
    "                else:\n",
    "                    units_or_num_filters = self.p[i][\"units\"]\n",
    "                num_layers = self.p[i][\"num_layers\"]\n",
    "                self.p[i][\"decay_sequence\"] = self.helper.get_max_decay_sequence(num_layers, \n",
    "                                                                                units_or_num_filters, \n",
    "                                                                                self.p[i][\"decay_sequence\"], \n",
    "                                                                                self.num_classes)\n",
    "   \n",
    "            if \"first_dense_units\" in self.p[i]:\n",
    "                if self.p[i][\"first_dense_units\"] < self.p[i][\"second_dense_units\"]:\n",
    "                    self.p[i][\"second_dense_units\"] = self.p[i][\"first_dense_units\"]\n",
    "            \n",
    "            current_picks = [model_info, self.p[i]]\n",
    "            pp.pprint(current_picks)\n",
    "            # Store picked parameters:\n",
    "            if self.log_data:\n",
    "                self.results_df = self.store_params_before_fit_opti(self.p[i], self.results_df, self.results_file_name)\n",
    "\n",
    "            _, _, timesteps = self.x_train.shape\n",
    "\n",
    "            input_shape = (timesteps, self.num_channels)\n",
    "            \n",
    "            if self.is_dynamic:  \n",
    "                model = DynamicModels(self.model_nr_type, self.num_classes, input_shape, **self.p[i]).model\n",
    "            else:\n",
    "                raise Exception(\"Static models are not handled by this class yet.\")\n",
    "            \n",
    "            # Initializing generators:\n",
    "            #gen = RamGenerator(self.loadData, self.handler, self.noiseAug)\n",
    "            train_enq = GeneratorEnqueuer(data_generator(self.x_train, self.y_train, batch_size, self.loadData, self.handler, self.noiseAug, num_channels = num_channels, is_lstm  = self.is_lstm), use_multiprocessing = False)\n",
    "            val_enq = GeneratorEnqueuer(data_generator(self.x_val, self.y_val,batch_size, self.loadData, self.handler, self.noiseAug, num_channels = num_channels, is_lstm  = self.is_lstm), use_multiprocessing = False)\n",
    "            train_enq.start(workers = 16, max_queue_size = 15)\n",
    "            val_enq.start(workers = 16, max_queue_size = 15)\n",
    "            train_gen = train_enq.get()\n",
    "            val_gen = train_enq.get()\n",
    "\n",
    "            # Generate compiler args using picks\n",
    "            model_compile_args = self.helper.generate_model_compile_args(opt, self.num_classes)\n",
    "            # Compile model using generated args\n",
    "            model.compile(**model_compile_args)\n",
    "            \n",
    "            print(\"Starting: \")\n",
    "            pp.pprint(self.p[i])\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "            # Generate fit args using picks.\n",
    "            fit_args = self.helper.generate_fit_args(self.train_ds, self.val_ds, batch_size, \n",
    "                                                     epoch, val_gen, use_tensorboard = self.use_tensorboard, \n",
    "                                                     use_liveplots = self.use_liveplots, \n",
    "                                                     use_custom_callback = self.use_custom_callback,\n",
    "                                                     use_early_stopping = self.use_early_stopping,\n",
    "                                                     use_reduced_lr = self.use_reduced_lr)\n",
    "            try:\n",
    "                print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.val, batch_size)*batch_size}/{len(self.loadData.val)} validation points\")\n",
    "                print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.train, batch_size)*batch_size}/{len(self.loadData.train)} training points\")\n",
    "                \n",
    "                \n",
    "                # Fit the model using the generated args\n",
    "                model.fit(train_gen, **fit_args)\n",
    "                \n",
    "                # Evaluate the fitted model on the validation set\n",
    "                val_eval = model.evaluate(x=val_gen,\n",
    "                                          steps=self.helper.get_steps_per_epoch(self.loadData.val, batch_size),\n",
    "                                          return_dict = True)\n",
    "                pp.pprint(val_eval)\n",
    "                \n",
    "                metrics = {}\n",
    "                metrics['val'] = {  \"val_loss\" : val_eval[\"loss\"],\n",
    "                                    \"val_accuracy\" : val_eval[\"binary_accuracy\"],\n",
    "                                    \"val_precision\": val_eval[\"precision\"],\n",
    "                                    \"val_recall\" : val_eval[\"recall\"]}\n",
    "                \n",
    "                # Evaluate the fitted model on the train set\n",
    "                \n",
    "                train_eval = model.evaluate(x=train_gen,\n",
    "                                            steps=self.helper.get_steps_per_epoch(self.loadData.train, batch_size),\n",
    "                                            return_dict = True)\n",
    "                train_enq.stop()\n",
    "                val_enq.stop()\n",
    "                \n",
    "                metrics['train'] = { \"train_loss\" : train_eval[\"loss\"],\n",
    "                                    \"train_accuracy\" : train_eval[\"binary_accuracy\"],\n",
    "                                    \"train_precision\": train_eval[\"precision\"],\n",
    "                                    \"train_recall\" : train_eval[\"recall\"]}\n",
    "                \n",
    "                \n",
    "                conf, _ = self.helper.evaluate_model(model, self.x_val, self.y_val, self.loadData.label_dict, num_channels = self.num_channels, plot = False, run_evaluate = False)\n",
    "                train_enq.stop()\n",
    "                val_enq.stop()\n",
    "                gc.collect()\n",
    "                \n",
    "                tf.keras.backend.clear_session()\n",
    "                tf.compat.v1.reset_default_graph()\n",
    "                del model, train_gen, val_gen, train_enq, val_enq\n",
    "                \n",
    "                if self.log_data:\n",
    "                    self.results_df = self.store_metrics_after_fit(metrics, conf, self.results_df, self.results_file_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                print(\"Something went wrong while training the model (most likely)\")\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "    def print_best_performers(self, min_loss, max_accuracy, max_precision, max_recall):\n",
    "        print(\"----------------------------------------------------LOSS----------------------------------------------------------\")\n",
    "        print(f\"Min val loss: {min_loss['val_loss']}, at index: {min_loss['val_index']}\")\n",
    "        print(f\"Min training loss: {min_loss['train_loss']}, at index: {min_loss['train_index']}\")\n",
    "        print(\"----------------------------------------------------ACCURACY------------------------------------------------------\")\n",
    "        print(f\"Highest val accuracy: {max_accuracy['val_accuracy']}, at index: {max_accuracy['val_index']}\")\n",
    "        print(f\"Highest training accuracy: {max_accuracy['train_accuracy']}, at index: {max_accuracy['train_index']}\")\n",
    "        print(\"----------------------------------------------------PRECISION-----------------------------------------------------\")\n",
    "        print(f\"Highest val precision: {max_precision['val_precision']}, at index: {max_precision['val_index']}\")\n",
    "        print(f\"Highest training precision: {max_precision['train_precision']}, at index: {max_precision['train_index']}\") \n",
    "        print(\"-----------------------------------------------------RECALL-------------------------------------------------------\")\n",
    "        print(f\"Highest val recall: {max_recall['val_recall']}, at index: {max_recall['val_index']}\")\n",
    "        print(f\"Highest training recall: {max_recall['train_recall']}, at index: {max_recall['train_index']}\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "       \n",
    "\n",
    "    \n",
    "    def get_n_params_from_list(self, grid, n_picks):\n",
    "        print(f\"Length of grid: {len(grid)}\")\n",
    "        indexes = random.sample(range(0, len(grid)), n_picks)\n",
    "        picks = [grid[idx] for idx in indexes]\n",
    "        return picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "suited-involvement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of grid: 203212800\n",
      "results_DENSE_earthExplo_timeAug_noiseAug_earlyS_numChannels-3.csv\n",
      "Are you sure you want to erase the results file? Y/N \n",
      "y\n",
      "Empty DataFrame\n",
      "Columns: [use_layerwise_dropout_batchnorm, units, output_layer_activation, optimizer, num_layers, learning_rate, l2_r, l1_r, epochs, dropout_rate, decay_sequence, batch_size, activation, train_loss, train_accuracy, train_precision, train_recall, val_loss, val_accuracy, val_precision, val_recall, confusion_matrix]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 22 columns]\n",
      "Fit process completed after 14.012139081954956 seconds. Total datapoints fitted: 31480.\n",
      "Average time per datapoint: 0.0004451124231878957\n",
      "\n",
      "\n",
      "Stage one loading training set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading validation set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Fitting scaler progress: [------------------->] 100 %\n",
      "\n",
      "Stage two loading training set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading validation set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Completed loading to RAM [--------------------------------------> ] 99 %\n",
      "Model nr 1 of 100\n",
      "[   {'index': 0, 'model_nr_type': 'DENSE'},\n",
      "    {   'activation': 'sigmoid',\n",
      "        'batch_size': 64,\n",
      "        'decay_sequence': [1, 1, 1],\n",
      "        'dropout_rate': 0.1,\n",
      "        'epochs': 1,\n",
      "        'l1_r': 0.001,\n",
      "        'l2_r': 0.3,\n",
      "        'learning_rate': 0.0001,\n",
      "        'num_layers': 3,\n",
      "        'optimizer': 'sgd',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'units': 170,\n",
      "        'use_layerwise_dropout_batchnorm': False}]\n",
      "Saving file. 1 rows.\n",
      "results_DENSE_earthExplo_timeAug_noiseAug_earlyS_numChannels-3.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 6000, 3)]         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 170)               3060170   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 170)               29070     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 171       \n",
      "=================================================================\n",
      "Total params: 3,118,481\n",
      "Trainable params: 3,118,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting: \n",
      "{   'activation': 'sigmoid',\n",
      "    'batch_size': 64,\n",
      "    'confusion_matrix': nan,\n",
      "    'decay_sequence': [1, 1, 1],\n",
      "    'dropout_rate': 0.1,\n",
      "    'epochs': 1,\n",
      "    'l1_r': 0.001,\n",
      "    'l2_r': 0.3,\n",
      "    'learning_rate': 0.0001,\n",
      "    'num_layers': 3,\n",
      "    'optimizer': 'sgd',\n",
      "    'output_layer_activation': 'sigmoid',\n",
      "    'train_accuracy': nan,\n",
      "    'train_loss': nan,\n",
      "    'train_precision': nan,\n",
      "    'train_recall': nan,\n",
      "    'units': 170,\n",
      "    'use_layerwise_dropout_batchnorm': False,\n",
      "    'val_accuracy': nan,\n",
      "    'val_loss': nan,\n",
      "    'val_precision': nan,\n",
      "    'val_recall': nan}\n",
      "---------------------------------------------------------------------------------\n",
      "Utilizes 3072/3125 validation points\n",
      "Utilizes 15616/15624 training points\n",
      "244/244 [==============================] - 10s 33ms/step - loss: 273.6120 - binary_accuracy: 0.5115 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 266.9491 - val_binary_accuracy: 0.5130 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "48/48 [==============================] - 1s 30ms/step - loss: 266.9726 - binary_accuracy: 0.5212 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "{   'binary_accuracy': 0.5211588740348816,\n",
      "    'loss': 266.97259521484375,\n",
      "    'precision': 0.0,\n",
      "    'recall': 0.0}\n",
      "244/244 [==============================] - 6s 26ms/step - loss: 266.9673 - binary_accuracy: 0.5101 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
      "Num samples: 3125, Num predictions: 3125\n",
      "tf.Tensor(\n",
      "[[1567    0]\n",
      " [1558    0]], shape=(2, 2), dtype=int32)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  earthquake       0.50      1.00      0.67      1567\n",
      "   explosion       0.00      0.00      0.00      1558\n",
      "\n",
      "    accuracy                           0.50      3125\n",
      "   macro avg       0.25      0.50      0.33      3125\n",
      "weighted avg       0.25      0.50      0.33      3125\n",
      "\n",
      "Saving file. 1 rows.\n",
      "results_DENSE_earthExplo_timeAug_noiseAug_earlyS_numChannels-3.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model nr 2 of 100\n",
      "[   {'index': 1, 'model_nr_type': 'DENSE'},\n",
      "    {   'activation': 'relu',\n",
      "        'batch_size': 128,\n",
      "        'decay_sequence': [1],\n",
      "        'dropout_rate': 0.2,\n",
      "        'epochs': 1,\n",
      "        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tord/miniconda3/envs/thesis/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'l1_r': 0.3,\n",
      "        'l2_r': 0.2,\n",
      "        'learning_rate': 0.01,\n",
      "        'num_layers': 1,\n",
      "        'optimizer': 'sgd',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'units': 210,\n",
      "        'use_layerwise_dropout_batchnorm': False}]\n",
      "Saving file. 2 rows.\n",
      "results_DENSE_earthExplo_timeAug_noiseAug_earlyS_numChannels-3.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 6000, 3)]         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 18000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 210)               3780210   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 211       \n",
      "=================================================================\n",
      "Total params: 3,780,421\n",
      "Trainable params: 3,780,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting: \n",
      "{   'activation': 'relu',\n",
      "    'batch_size': 128,\n",
      "    'confusion_matrix': nan,\n",
      "    'decay_sequence': [1],\n",
      "    'dropout_rate': 0.2,\n",
      "    'epochs': 1,\n",
      "    'l1_r': 0.3,\n",
      "    'l2_r': 0.2,\n",
      "    'learning_rate': 0.01,\n",
      "    'num_layers': 1,\n",
      "    'optimizer': 'sgd',\n",
      "    'output_layer_activation': 'sigmoid',\n",
      "    'train_accuracy': nan,\n",
      "    'train_loss': nan,\n",
      "    'train_precision': nan,\n",
      "    'train_recall': nan,\n",
      "    'units': 210,\n",
      "    'use_layerwise_dropout_batchnorm': False,\n",
      "    'val_accuracy': nan,\n",
      "    'val_loss': nan,\n",
      "    'val_precision': nan,\n",
      "    'val_recall': nan}\n",
      "---------------------------------------------------------------------------------\n",
      "Utilizes 3072/3125 validation points\n",
      "Utilizes 15616/15624 training points\n",
      "122/122 [==============================] - ETA: 0s - loss: inf - binary_accuracy: 0.4967 - precision: 0.4547 - recall: 0.1837"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-22a6cf43ac0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     84\u001b[0m                                             \u001b[0muse_reduced_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_reduced_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mband_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mband_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighpass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighpass_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                                             start_from_scratch = start_from_scratch, is_lstm = is_lstm, log_data = log_data, num_channels = num_channels)\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomGridSearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-31-1d0f0d9f5a54>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0;31m# Fit the model using the generated args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0;31m# Evaluate the fitted model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1165\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1166\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    851\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2991\u001b[0m       (graph_function,\n\u001b[1;32m   2992\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2993\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2994\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1937\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1939\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1940\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    562\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    565\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# IncepTime grid:\n",
    "\"\"\"\n",
    "hyper_grid = {    \n",
    "    \"batch_size\" : [64, 128, 256],\n",
    "    \"epochs\" : [50, 50, 50, 50, 50, 50, 50, 50, 50],\n",
    "    \"learning_rate\" : [0.1, 0.01, 0.01, 0.001, 0.0001],\n",
    "    \"optimizer\" : [\"adam\", \"adam\", \"adam\", \"adam\", \"rmsprop\", \"sgd\", \"adam\", \"rmsprop\", \"sgd\"],\n",
    "    \"use_residuals\" : [True, True, False],\n",
    "    \"use_bottleneck\" : [True, True, False],\n",
    "    \"num_modules\" : np.concatenate((np.array([1]), np.arange(3, 9, 3))),\n",
    "    \"filter_size\" : np.arange(30, 60, 2),\n",
    "    \"bottleneck_size\" : np.arange(30, 50, 2),\n",
    "    \"num_filters\" : np.arange(20, 40, 2),\n",
    "    \"residual_activation\" : [\"relu\", \"relu\", \"relu\", \"relu\", \"tanh\"],\n",
    "    \"module_activation\" : [\"linear\", \"linear\", \"linear\", \"relu\", \"tanh\"],\n",
    "    \"module_output_activation\" : [\"relu\", \"relu\", \"relu\", \"relu\", \"linear\", \"tanh\"],\n",
    "    \"output_layer_activation\": [\"sigmoid\"],\n",
    "    \"reg_residual\": [True, False, False],\n",
    "    \"reg_module\" : [True, False, False],\n",
    "    \"l1_r\" : [0.1, 0.01, 0.01, 0.001, 0.0001, 0],\n",
    "    \"l2_r\" : [0.1, 0.01, 0.01, 0.001, 0.0001, 0]\n",
    "}\n",
    "\"\"\"\n",
    "# Dense grid:\n",
    "\n",
    "hyper_grid = {\n",
    "        \"batch_size\" : [64, 128, 256],\n",
    "        \"epochs\" : [1],\n",
    "        \"learning_rate\" : [0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001],\n",
    "        \"optimizer\" : [\"sgd\", \"sgd\", \"sgd\", \"sgd\", \"rmsprop\", \"adam\", \"rmsprop\", \"sgd\"],\n",
    "        \"num_layers\" : [1, 2, 3, 4, 5],\n",
    "        \"units\" : np.arange(100, 300, 10),\n",
    "        \"use_layerwise_dropout_batchnorm\" : [False, True],\n",
    "        \"decay_sequence\" : [[1,2,4,4,2,1], [1,4,8,8,4,1], [1,1,1,1,1,1], [1, 2, 4, 6, 8, 10]],\n",
    "        \"dropout_rate\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0],\n",
    "        \"l2_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "        \"l1_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "        \"activation\" : [\"tanh\", \"tanh\", \"relu\", \"relu\", \"relu\", \"sigmoid\", \"softmax\"],\n",
    "        \"output_layer_activation\" : [\"sigmoid\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "model_type = \"DENSE\"\n",
    "is_lstm = True\n",
    "num_channels = 3    \n",
    "\n",
    "use_time_augmentor = True\n",
    "scaler_name = \"standard\"\n",
    "use_noise_augmentor = True\n",
    "filter_name = None\n",
    "band_min = 2.0\n",
    "band_max = 4.0\n",
    "highpass_freq = 15\n",
    "\n",
    "n_picks = 100\n",
    "\n",
    "use_tensorboard = False\n",
    "use_liveplots = False\n",
    "use_custom_callback = True\n",
    "use_early_stopping = True\n",
    "start_from_scratch = True\n",
    "use_reduced_lr = True\n",
    "log_data = True\n",
    "\n",
    "shutdown = False\n",
    "\n",
    "def clear_tensorboard_dir():\n",
    "        import os\n",
    "        import shutil\n",
    "        path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "        files = os.listdir(path)\n",
    "        print(files)\n",
    "        for f in files:\n",
    "            shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "\n",
    "\n",
    "\n",
    "randomGridSearch = RGS(loadData, train_ds, val_ds, test_ds, model_type, scaler_name, use_time_augmentor, use_noise_augmentor,\n",
    "                                            filter_name, n_picks, hyper_grid=hyper_grid, use_tensorboard = use_tensorboard, \n",
    "                                            use_liveplots = use_liveplots, use_custom_callback = use_custom_callback, use_early_stopping = use_early_stopping, \n",
    "                                            use_reduced_lr = use_reduced_lr, band_min = band_min, band_max = band_max, highpass_freq = highpass_freq, \n",
    "                                            start_from_scratch = start_from_scratch, is_lstm = is_lstm, log_data = log_data, num_channels = num_channels)\n",
    "results_df, min_loss, max_accuracy, max_precision, max_recall = randomGridSearch.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((1))\n",
    "x = input_layer\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs = input_layer, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = [[1],[0]], y = [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reliable-found",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2],[4,5]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "vocational-nation",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame([[1,1,np.nan],[2,2,np.nan]], columns = [\"a\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "banner-spray",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b   c\n",
       "0  1  1 NaN\n",
       "1  2  2 NaN"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "enclosed-angola",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a2d1580fb7f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtake_split_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0;31m# We have to operate column-wise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1635\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_split_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1637\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_single_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_split_path\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_2d_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlplane_indexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_2d_value\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0milocs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1733\u001b[0m                 \u001b[0;34m\"Must have equal len keys and value when setting with an ndarray\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: Must have equal len keys and value when setting with an ndarray"
     ]
    }
   ],
   "source": [
    "b.loc[-2, b.columns == \"c\"] = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hungry-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[[1 2]\\n [4 5]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      a    b                c\n",
       " 0  1.0  1.0              NaN\n",
       " 1  2.0  2.0              NaN\n",
       "-2  NaN  NaN  [[1 2]\\n [4 5]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-diving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
