{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "productive-salmon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tord/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/util/module_wrapper.py:49: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "\n",
    "import os\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.DataProcessing.ts_RamGenerator import data_generator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "absent-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainSingleModel(GridSearchResultProcessor):\n",
    "    \n",
    "    def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test, noiseAug, helper, loadData, \n",
    "                 model_type, num_channels, use_tensorboard, use_liveplots, use_custom_callback,\n",
    "                 use_early_stopping, use_reduced_lr, log_data = True, results_df = None, \n",
    "                 results_file_name = None, index = None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.noiseAug = noiseAug\n",
    "        self.helper = helper\n",
    "        self.loadData = loadData\n",
    "        \n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        \n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.results_df = results_df\n",
    "        self.log_data = log_data\n",
    "        \n",
    "        self.index = index\n",
    "        \n",
    "    def create_and_compile_model(self, **p):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        \n",
    "        epoch = p[\"epochs\"]\n",
    "        batch_size = p[\"batch_size\"]\n",
    "        opt = self.helper.get_optimizer(p[\"optimizer\"], p[\"learning_rate\"])\n",
    "        \n",
    "        p = self.handle_hyperparams(**p)\n",
    "        \n",
    "        if self.index != None:\n",
    "            model_info = {\"model_type\" : self.model_type, \"index\" : self.index}\n",
    "        else:\n",
    "            model_info = {\"model_type\" : self.model_type}\n",
    "        current_picks = [model_info, p]\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(current_picks)\n",
    "        if self.log_data and self.results_df != None and self.results_file_name != None:\n",
    "            self.results_df = self.store_params_before_fit_opti(p, self.results_df, self.results_file_name)\n",
    "        \n",
    "        _,_,timesteps = self.x_train.shape\n",
    "        input_shape = (timesteps, self.num_channels)\n",
    "        \n",
    "        model = DynamicModels(self.model_type, self.num_classes, input_shape, **p).model\n",
    "        model_compile_args = self.helper.generate_model_compile_args(opt, self.num_classes)\n",
    "        model.compile(**model_compile_args)\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    \n",
    "    def handle_hyperparams(self, **p):\n",
    "        if \"decay_sequence\" or \"growth_sequence\" in p:\n",
    "            if \"num_filters\" in p:\n",
    "                units_or_num_filters = p[\"num_filters\"]\n",
    "            else:\n",
    "                units_or_num_filters = p[\"units\"]\n",
    "            num_layers = p[\"num_layers\"]\n",
    "            if \"decay_sequence\" in p:\n",
    "                p[\"decay_sequence\"] = self.helper.get_max_decay_sequence(num_layers,\n",
    "                                                                         units_or_num_filters,\n",
    "                                                                         p[\"decay_sequence\"],\n",
    "                                                                         self.num_classes)\n",
    "            else:\n",
    "                p[\"growth_sequence\"] = p[\"growth_sequence\"][:num_layers]\n",
    "        return p\n",
    "    \n",
    "    def create_enqueuer(self, X, y, batch_size, noiseAug, num_channels):\n",
    "        enq = GeneratorEnqueuer(data_generator(X, y, batch_size, noiseAug, num_channels = num_channels, is_lstm  = True), \n",
    "                                use_multiprocessing = False)\n",
    "        return enq\n",
    "        \n",
    "    def fit_model(self, model, workers, max_queue_size, **p):\n",
    "        train_enq = self.create_enqueuer(self.x_train, self.y_train, p[\"batch_size\"], self.noiseAug, self.num_channels)\n",
    "        val_enq = self.create_enqueuer(self.x_val, self.y_val, p[\"batch_size\"], self.noiseAug, self.num_channels)\n",
    "        train_enq.start(workers = workers, max_queue_size = max_queue_size)\n",
    "        val_enq.start(workers = workers, max_queue_size = max_queue_size)\n",
    "        train_gen = train_enq.get()\n",
    "        val_gen = val_enq.get()\n",
    "        \n",
    "        fit_args = self.helper.generate_fit_args(self.loadData.train, self.loadData.val, self.loadData,\n",
    "                                                 p[\"batch_size\"], p[\"epochs\"], val_gen,\n",
    "                                                 use_tensorboard = self.use_tensorboard, \n",
    "                                                 use_liveplots = self.use_liveplots, \n",
    "                                                 use_custom_callback = self.use_custom_callback,\n",
    "                                                 use_early_stopping = self.use_early_stopping,\n",
    "                                                 use_reduced_lr = self.use_reduced_lr)\n",
    "        try:\n",
    "            print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.val, p['batch_size'])*p['batch_size']}/{len(self.loadData.val)} validation points\")\n",
    "            print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.train, p['batch_size'])*p['batch_size']}/{len(self.loadData.train)} training points\")\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "            # Fit the model using the generated args\n",
    "            model.fit(train_gen, **fit_args)\n",
    "            train_enq.stop()\n",
    "            val_enq.stop()\n",
    "            del train_gen, val_gen, train_enq, val_enq\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            print(\"Something went wrong.\")\n",
    "        return model\n",
    "    \n",
    "    def run(self, workers, max_queue_size, **p):\n",
    "        model = self.create_and_compile_model(**p)\n",
    "        model = self.fit_model(model, 16, 15, **p)\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "other-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "Mapping train redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping validation redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping test redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded explosion and earthquake dataset:\n",
      "Evenly balanced among classes in the train set.\n",
      "As well as non train sets.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Train ds:\n",
      "earthquake: (20569, 0.5002)  |  explosion: (20554, 0.4998)  \n",
      "Val ds:\n",
      "earthquake: (3094, 0.5016)  |  explosion: (3074, 0.4984)  \n",
      "Test ds:\n",
      "earthquake: (2069, 0.5032)  |  explosion: (2043, 0.4968)  \n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.25,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "helper = HelperFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attended-count",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting noise set time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 9.679181337356567 seconds. Total datapoints fitted: 21199.\n",
      "Average time per datapoint: 0.00045658669453071217\n",
      "\n",
      "\n",
      "Fitting train time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 13.603031396865845 seconds. Total datapoints fitted: 41123.\n",
      "Average time per datapoint: 0.0003307888869213298\n",
      "\n",
      "\n",
      "Fitting validation time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 2.0126843452453613 seconds. Total datapoints fitted: 6168.\n",
      "Average time per datapoint: 0.0003263106915118939\n",
      "\n",
      "\n",
      "Fitting test time augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process completed after 13.687296390533447 seconds. Total datapoints fitted: 4112.\n",
      "Average time per datapoint: 0.0033286226630674727\n",
      "\n",
      "\n",
      "Stage one loading training set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading validation set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading test set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Fit process of normalizer skipped as unecessary\n",
      "\n",
      "\n",
      "Stage one loading noise set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading noise set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Fitting noise augmentor: [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Stage two loading training set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading validation set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading test set, labels and normalize scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Completed loading to RAM\n",
      "Process took 108 seconds.\n"
     ]
    }
   ],
   "source": [
    "use_time_augmentor = True\n",
    "use_noise_augmentor = True\n",
    "scaler_name = \"normalize\"\n",
    "filter_name = None\n",
    "band_min = 2\n",
    "band_max = 4\n",
    "highpass_freq = 5\n",
    "\n",
    "use_tensorboard = True\n",
    "use_liveplots = False\n",
    "use_custom_callback = True\n",
    "use_early_stopping = True\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = True\n",
    "log_data = False\n",
    "\n",
    "load_test_set = True\n",
    "\n",
    "\n",
    "ramLoader = RamLoader(loadData, \n",
    "                      handler, \n",
    "                      use_time_augmentor = use_time_augmentor, \n",
    "                      use_noise_augmentor = use_noise_augmentor, \n",
    "                      scaler_name = scaler_name,\n",
    "                      filter_name = filter_name, \n",
    "                      band_min = band_min,\n",
    "                      band_max = band_max,\n",
    "                      highpass_freq = highpass_freq, \n",
    "                      load_test_set = load_test_set)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, noiseAug = ramLoader.load_to_ram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-personality",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {'model_type': 'CNN_grow'},\n",
      "    {   'batch_size': 64,\n",
      "        'cnn_activation': 'relu',\n",
      "        'dense_activation': 'relu',\n",
      "        'dropout_rate': 0.001,\n",
      "        'epochs': 25,\n",
      "        'filter_size': 42,\n",
      "        'first_dense_units': 286,\n",
      "        'growth_sequence': [1, 2],\n",
      "        'l1_r': 0.001,\n",
      "        'l2_r': 0.01,\n",
      "        'learning_rate': 0.01,\n",
      "        'num_filters': 68,\n",
      "        'num_layers': 2,\n",
      "        'optimizer': 'sgd',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'padding': 'same',\n",
      "        'use_layerwise_dropout_batchnorm': True}]\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 6000, 3)]         0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 6000, 68)          8636      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 6000, 68)          272       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 6000, 68)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6000, 68)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 3000, 68)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 3000, 136)         388552    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3000, 136)         544       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3000, 136)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 3000, 136)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1500, 136)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 204000)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 286)               58344286  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 287       \n",
      "=================================================================\n",
      "Total params: 58,742,577\n",
      "Trainable params: 58,742,169\n",
      "Non-trainable params: 408\n",
      "_________________________________________________________________\n",
      "Utilizes 6144/6168 validation points\n",
      "Utilizes 41088/41123 training points\n",
      "---------------------------------------------------------------------------------\n",
      "Epoch 1/25\n",
      "642/642 [==============================] - 46s 65ms/step - loss: 6.1395 - binary_accuracy: 0.6954 - precision: 0.6994 - recall: 0.6818 - val_loss: 3.5160 - val_binary_accuracy: 0.6515 - val_precision: 0.6250 - val_recall: 0.7502\n",
      "Epoch 2/25\n",
      "201/642 [========>.....................] - ETA: 26s - loss: 3.1281 - binary_accuracy: 0.8165 - precision: 0.8329 - recall: 0.7944"
     ]
    }
   ],
   "source": [
    "model_type = \"CNN_grow\"\n",
    "num_channels = 3\n",
    "\n",
    "singleModel = TrainSingleModel(x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                               noiseAug, helper, loadData, \n",
    "                               model_type, \n",
    "                               num_channels, \n",
    "                               use_tensorboard, \n",
    "                               use_liveplots, \n",
    "                               use_custom_callback,\n",
    "                               use_early_stopping, \n",
    "                               use_reduced_lr, \n",
    "                               log_data = log_data, \n",
    "                               results_df = None, \n",
    "                               results_file_name = None, \n",
    "                               index = None)\n",
    "\n",
    "workers = 16\n",
    "max_queue_size = 15\n",
    "\n",
    "\n",
    "hyper_params = {\n",
    "    \"num_layers\" : 2,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 25,\n",
    "    \"learning_rate\" : 0.01,\n",
    "    \"optimizer\" : \"sgd\",\n",
    "    \"num_filters\" : 68,\n",
    "    \"filter_size\" : 42,\n",
    "    \"cnn_activation\" : \"relu\",\n",
    "    \"dense_activation\" : \"relu\",\n",
    "    \"padding\" : \"same\",\n",
    "    \"use_layerwise_dropout_batchnorm\" : True,\n",
    "    \"growth_sequence\" : [1,2,4,4,2,1],\n",
    "    \"dropout_rate\" : 0.001,\n",
    "    \"l2_r\" : 0.01,\n",
    "    \"l1_r\" : 0.001,\n",
    "    \"first_dense_units\" : 286,\n",
    "    \"output_layer_activation\" : \"sigmoid\"\n",
    "}\n",
    "model = singleModel.run(workers = workers, max_queue_size = max_queue_size, **hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-sampling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
