{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifteen-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "os.chdir(utils.base_dir)\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "gen_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/Classes/DataProcessing'\n",
    "os.chdir(gen_dir)\n",
    "import ts_RamGenerator\n",
    "os.chdir(base_dir)\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "divided-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "Mapping redundancy: [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded explosion and earthquake dataset:\n",
      "Evenly balanced among classes in the train set.\n",
      "As well as non train sets.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Full ds:\n",
      "earthquake: (105999, 0.5076)  |  explosion: (102808, 0.4924)  \n",
      "Train ds:\n",
      "earthquake: (79455, 0.5074)  |  explosion: (77150, 0.4926)  \n",
      "Val ds:\n",
      "earthquake: (15922, 0.5083)  |  explosion: (15399, 0.4917)  \n",
      "Test ds:\n",
      "earthquake: (10622, 0.5087)  |  explosion: (10259, 0.4913)  \n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 1,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stunning-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "\n",
    "import os\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.DataProcessing.ts_RamGenerator import data_generator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "import re\n",
    "import json\n",
    "\n",
    "class RGS(GridSearchResultProcessor):\n",
    "    \n",
    "    def __init__(self, loadData, train_ds, val_ds, test_ds, model_type, scaler_name, use_time_augmentor, use_noise_augmentor,\n",
    "                 filter_name, n_picks, hyper_grid, use_tensorboard = False, \n",
    "                 use_liveplots = True, use_custom_callback = False, use_early_stopping = False, use_reduced_lr = False,\n",
    "                 band_min = 2.0, band_max = 4.0, highpass_freq = 0.1, start_from_scratch = True, is_lstm = False, \n",
    "                 log_data = True, num_channels = 3):\n",
    "        \n",
    "        self.loadData = loadData\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.test_ds = test_ds\n",
    "        self.model_nr_type = model_type\n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "\n",
    "        self.scaler_name = scaler_name\n",
    "        self.use_noise_augmentor = use_noise_augmentor\n",
    "        self.use_time_augmentor = use_time_augmentor\n",
    "        self.filter_name = filter_name\n",
    "        self.n_picks = n_picks\n",
    "        self.hyper_grid = hyper_grid\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "\n",
    "        self.band_min = band_min\n",
    "        self.band_max = band_max\n",
    "        self.highpass_freq = highpass_freq\n",
    "        self.start_from_scratch = start_from_scratch\n",
    "        self.is_lstm = is_lstm\n",
    "        self.log_data = log_data\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.helper = HelperFunctions()\n",
    "        self.handler = DataHandler(self.loadData)\n",
    "        self.is_dynamic = False\n",
    "        if type(self.model_nr_type) == str:\n",
    "            self.is_dynamic = True\n",
    "        #if self.loadData.earth_explo_only:\n",
    "        #    self.full_ds = np.concatenate((self.loadData.noise_ds, self.loadData.full_ds))\n",
    "        #else:\n",
    "        #    self.full_ds = self.loadData.full_ds\n",
    "            \n",
    "\n",
    "    def fit(self):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        # Creating grid:\n",
    "        self.p = ParameterGrid(self.hyper_grid)\n",
    "        if len(self.p) < self.n_picks:\n",
    "            self.n_picks = len(self.p)\n",
    "            print(f\"Picks higher than max. Reducing picks to {self.n_picks} picks\")\n",
    "        self.p = self.get_n_params_from_list(self.p, self.n_picks)\n",
    "        \n",
    "        # Create name of results file, get initiated results df, either brand new or continue old.\n",
    "        self.results_file_name = self.get_results_file_name()\n",
    "        print(self.results_file_name)\n",
    "        self.results_df = self.initiate_results_df_opti(self.results_file_name, self.num_classes, self.start_from_scratch, self.p[0])\n",
    "        print(self.results_df)\n",
    "        # Preprocessing and loading all data to RAM:\n",
    "        ramLoader = RamLoader(self.loadData, \n",
    "                              self.handler, \n",
    "                              use_time_augmentor = self.use_time_augmentor, \n",
    "                              use_noise_augmentor = self.use_noise_augmentor, \n",
    "                              scaler_name = self.scaler_name,\n",
    "                              filter_name = self.filter_name, \n",
    "                              band_min = self.band_min,\n",
    "                              band_max = self.band_max,\n",
    "                              highpass_freq = self.highpass_freq, \n",
    "                              load_test_set = False)\n",
    "        self.x_train, self.y_train, self.x_val, self.y_val, self.timeAug, self.scaler, self.noiseAug = ramLoader.load_to_ram()\n",
    "\n",
    "        \n",
    "        \n",
    "        for i in range(len(self.p)):\n",
    "            gc.collect()\n",
    "            tf.keras.backend.clear_session()\n",
    "            tf.config.optimizer.set_jit(True)\n",
    "            mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "            model_info = {\"model_nr_type\" : self.model_nr_type, \"index\" : i}\n",
    "            print(f\"Model nr {i + 1} of {len(self.p)}\")           \n",
    "            # Translate picks to a more readable format:\n",
    "            num_channels = self.num_channels\n",
    "            epoch = self.p[i][\"epochs\"]\n",
    "            batch_size = self.p[i][\"batch_size\"]\n",
    "            \n",
    "            opt = self.helper.get_optimizer(self.p[i][\"optimizer\"], self.p[i][\"learning_rate\"])\n",
    "    \n",
    "\n",
    "            if \"decay_sequence\" in self.p[i]:\n",
    "                if \"num_filters\" in self.p[i]:\n",
    "                    units_or_num_filters = self.p[i][\"num_filters\"]\n",
    "                else:\n",
    "                    units_or_num_filters = self.p[i][\"units\"]\n",
    "                num_layers = self.p[i][\"num_layers\"]\n",
    "                self.p[i][\"decay_sequence\"] = self.helper.get_max_decay_sequence(num_layers, \n",
    "                                                                                units_or_num_filters, \n",
    "                                                                                self.p[i][\"decay_sequence\"], \n",
    "                                                                                self.num_classes)\n",
    "   \n",
    "            \n",
    "            \n",
    "            current_picks = [model_info, self.p[i]]\n",
    "            pp.pprint(current_picks)\n",
    "            # Store picked parameters:\n",
    "            if self.log_data:\n",
    "                self.results_df = self.store_params_before_fit_opti(self.p[i], self.results_df, self.results_file_name)\n",
    "\n",
    "            _, _, timesteps = self.x_train.shape\n",
    "\n",
    "            input_shape = (timesteps, self.num_channels)\n",
    "            \n",
    "            if self.is_dynamic:  \n",
    "                model = DynamicModels(self.model_nr_type, self.num_classes, input_shape, **self.p[i]).model\n",
    "            else:\n",
    "                raise Exception(\"Static models are not handled by this class yet.\")\n",
    "            \n",
    "            # Initializing generators:\n",
    "            #gen = RamGenerator(self.loadData, self.handler, self.noiseAug)\n",
    "            train_enq = GeneratorEnqueuer(data_generator(self.x_train, self.y_train, batch_size, self.loadData, self.handler, self.noiseAug, num_channels = num_channels, is_lstm  = self.is_lstm), use_multiprocessing = False)\n",
    "            val_enq = GeneratorEnqueuer(data_generator(self.x_val, self.y_val,batch_size, self.loadData, self.handler, self.noiseAug, num_channels = num_channels, is_lstm  = self.is_lstm), use_multiprocessing = False)\n",
    "            train_enq.start(workers = 16, max_queue_size = 15)\n",
    "            val_enq.start(workers = 16, max_queue_size = 15)\n",
    "            train_gen = train_enq.get()\n",
    "            val_gen = train_enq.get()\n",
    "\n",
    "            # Generate compiler args using picks\n",
    "            model_compile_args = self.helper.generate_model_compile_args(opt, self.num_classes)\n",
    "            # Compile model using generated args\n",
    "            model.compile(**model_compile_args)\n",
    "            \n",
    "            print(\"Starting: \")\n",
    "            pp.pprint(self.p[i])\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "            \n",
    "            # Generate fit args using picks.\n",
    "            fit_args = self.helper.generate_fit_args(self.train_ds, self.val_ds, batch_size, \n",
    "                                                     epoch, val_gen, use_tensorboard = self.use_tensorboard, \n",
    "                                                     use_liveplots = self.use_liveplots, \n",
    "                                                     use_custom_callback = self.use_custom_callback,\n",
    "                                                     use_early_stopping = self.use_early_stopping,\n",
    "                                                     use_reduced_lr = self.use_reduced_lr)\n",
    "            try:\n",
    "                print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.val, batch_size)*batch_size}/{len(self.loadData.val)} validation points\")\n",
    "                print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.train, batch_size)*batch_size}/{len(self.loadData.train)} training points\")\n",
    "                \n",
    "                \n",
    "                # Fit the model using the generated args\n",
    "                model.fit(train_gen, **fit_args)\n",
    "                \n",
    "                # Evaluate the fitted model on the validation set\n",
    "                val_eval = model.evaluate(x=val_gen,\n",
    "                                          steps=self.helper.get_steps_per_epoch(self.loadData.val, batch_size),\n",
    "                                          return_dict = True)\n",
    "                pp.pprint(val_eval)\n",
    "                \n",
    "                metrics = {}\n",
    "                metrics['val'] = {  \"val_loss\" : val_eval[\"loss\"],\n",
    "                                    \"val_accuracy\" : val_eval[\"binary_accuracy\"],\n",
    "                                    \"val_precision\": val_eval[\"precision\"],\n",
    "                                    \"val_recall\" : val_eval[\"recall\"]}\n",
    "                \n",
    "                # Evaluate the fitted model on the train set\n",
    "                \n",
    "                train_eval = model.evaluate(x=train_gen,\n",
    "                                            steps=self.helper.get_steps_per_epoch(self.loadData.train, batch_size),\n",
    "                                            return_dict = True)\n",
    "                train_enq.stop()\n",
    "                val_enq.stop()\n",
    "                \n",
    "                metrics['train'] = { \"train_loss\" : train_eval[\"loss\"],\n",
    "                                    \"train_accuracy\" : train_eval[\"binary_accuracy\"],\n",
    "                                    \"train_precision\": train_eval[\"precision\"],\n",
    "                                    \"train_recall\" : train_eval[\"recall\"]}\n",
    "                \n",
    "                \n",
    "                _ = self.helper.evaluate_model(model, self.x_val, self.y_val, self.loadData.label_dict, num_channels = self.num_channels, plot = False, run_evaluate = False)\n",
    "                train_enq.stop()\n",
    "                val_enq.stop()\n",
    "                gc.collect()\n",
    "                \n",
    "                tf.keras.backend.clear_session()\n",
    "                tf.compat.v1.reset_default_graph()\n",
    "                del model, train_gen, val_gen, train_enq, val_enq\n",
    "                \n",
    "                if self.log_data:\n",
    "                    self.results_df = self.store_metrics_after_fit(metrics, self.results_df, self.results_file_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                print(\"Something went wrong while training the model (most likely)\")\n",
    "\n",
    "                continue\n",
    "\n",
    "\n",
    "    def print_best_performers(self, min_loss, max_accuracy, max_precision, max_recall):\n",
    "        print(\"----------------------------------------------------LOSS----------------------------------------------------------\")\n",
    "        print(f\"Min val loss: {min_loss['val_loss']}, at index: {min_loss['val_index']}\")\n",
    "        print(f\"Min training loss: {min_loss['train_loss']}, at index: {min_loss['train_index']}\")\n",
    "        print(\"----------------------------------------------------ACCURACY------------------------------------------------------\")\n",
    "        print(f\"Highest val accuracy: {max_accuracy['val_accuracy']}, at index: {max_accuracy['val_index']}\")\n",
    "        print(f\"Highest training accuracy: {max_accuracy['train_accuracy']}, at index: {max_accuracy['train_index']}\")\n",
    "        print(\"----------------------------------------------------PRECISION-----------------------------------------------------\")\n",
    "        print(f\"Highest val precision: {max_precision['val_precision']}, at index: {max_precision['val_index']}\")\n",
    "        print(f\"Highest training precision: {max_precision['train_precision']}, at index: {max_precision['train_index']}\") \n",
    "        print(\"-----------------------------------------------------RECALL-------------------------------------------------------\")\n",
    "        print(f\"Highest val recall: {max_recall['val_recall']}, at index: {max_recall['val_index']}\")\n",
    "        print(f\"Highest training recall: {max_recall['train_recall']}, at index: {max_recall['train_index']}\")\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "       \n",
    "\n",
    "    \n",
    "    def get_n_params_from_list(self, grid, n_picks):\n",
    "        print(f\"Length of grid: {len(grid)}\")\n",
    "        indexes = random.sample(range(0, len(grid)), n_picks)\n",
    "        picks = [grid[idx] for idx in indexes]\n",
    "        return picks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "suited-involvement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of grid: 2391484500000\n",
      "results_InceptionTime_noiseNotNoise_timeAug_noiseAug_earlyS_numChannels-2.csv\n",
      "Empty DataFrame\n",
      "Columns: [use_residuals, use_bottleneck, residual_activation, reg_residual, reg_module, output_layer_activation, optimizer, num_modules, num_filters, module_output_activation, module_activation, learning_rate, l2_r, l1_r, filter_size, epochs, bottleneck_size, batch_size, train_loss, train_accuracy, train_precision, train_recall, val_loss, val_accuracy, val_precision, val_recall]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 26 columns]\n",
      "Fit process completed after 78.79526424407959 seconds. Total datapoints fitted: 14475.\n",
      "Average time per datapoint: 0.005443541571266293\n",
      "\n",
      "\n",
      "Stage one loading training set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage one loading validation set, timeAug: [--------------------------------------->] 100 %\n",
      "\n",
      "Fitting scaler progress: [------------------->] 100 %\n",
      "\n",
      "Stage two loading training set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Stage two loading validation set, labels and standard scaler: [--------------------------------------->] 100 %\n",
      "\n",
      "Completed loading to RAM [--------------------------------------> ] 99 %\n",
      "Model nr 1 of 100\n",
      "[   {'index': 0, 'model_nr_type': 'InceptionTime'},\n",
      "    {   'batch_size': 256,\n",
      "        'bottleneck_size': 40,\n",
      "        'epochs': 50,\n",
      "        'filter_size': 38,\n",
      "        'l1_r': 0.1,\n",
      "        'l2_r': 0.0001,\n",
      "        'learning_rate': 0.01,\n",
      "        'module_activation': 'linear',\n",
      "        'module_output_activation': 'relu',\n",
      "        'num_filters': 20,\n",
      "        'num_modules': 6,\n",
      "        'optimizer': 'adam',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'reg_module': False,\n",
      "        'reg_residual': False,\n",
      "        'residual_activation': 'relu',\n",
      "        'use_bottleneck': True,\n",
      "        'use_residuals': True}]\n",
      "Saving file. 1 rows.\n",
      "results_InceptionTime_noiseNotNoise_timeAug_noiseAug_earlyS_numChannels-2.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6000, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 6000, 40)     80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 6000, 2)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 6000, 20)     30400       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 6000, 20)     15200       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 6000, 20)     7200        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6000, 20)     40          max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6000, 80)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 6000, 80)     320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 6000, 80)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 6000, 40)     3200        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 6000, 80)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 6000, 20)     30400       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 6000, 20)     15200       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 6000, 20)     7200        conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 6000, 20)     1600        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6000, 80)     0           conv1d_6[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 6000, 80)     320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 6000, 80)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 6000, 40)     3200        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 6000, 80)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 6000, 20)     30400       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 6000, 20)     15200       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 6000, 20)     7200        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6000, 80)     0           conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 6000, 80)     160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 6000, 80)     320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 6000, 80)     320         conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 6000, 80)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 6000, 80)     0           batch_normalization_3[0][0]      \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 6000, 80)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 6000, 40)     3200        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 6000, 80)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 6000, 20)     30400       conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 6000, 20)     15200       conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 6000, 20)     7200        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 6000, 80)     0           conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6000, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 6000, 80)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 6000, 40)     3200        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 6000, 80)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 6000, 20)     30400       conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 6000, 20)     15200       conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 6000, 20)     7200        conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6000, 80)     0           conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 6000, 80)     320         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 6000, 80)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 6000, 40)     3200        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 6000, 80)     0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 6000, 20)     30400       conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 6000, 20)     15200       conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 6000, 20)     7200        conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 6000, 80)     0           conv1d_27[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 6000, 80)     6400        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 6000, 80)     320         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 6000, 80)     320         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 6000, 80)     0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 6000, 80)     0           batch_normalization_7[0][0]      \n",
      "                                                                 activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 6000, 80)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 80)           0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            81          global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 350,121\n",
      "Trainable params: 348,841\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n",
      "Starting: \n",
      "{   'batch_size': 256,\n",
      "    'bottleneck_size': 40,\n",
      "    'epochs': 50,\n",
      "    'filter_size': 38,\n",
      "    'l1_r': 0.1,\n",
      "    'l2_r': 0.0001,\n",
      "    'learning_rate': 0.01,\n",
      "    'module_activation': 'linear',\n",
      "    'module_output_activation': 'relu',\n",
      "    'num_filters': 20,\n",
      "    'num_modules': 6,\n",
      "    'optimizer': 'adam',\n",
      "    'output_layer_activation': 'sigmoid',\n",
      "    'reg_module': False,\n",
      "    'reg_residual': False,\n",
      "    'residual_activation': 'relu',\n",
      "    'train_accuracy': nan,\n",
      "    'train_loss': nan,\n",
      "    'train_precision': nan,\n",
      "    'train_recall': nan,\n",
      "    'use_bottleneck': True,\n",
      "    'use_residuals': True,\n",
      "    'val_accuracy': nan,\n",
      "    'val_loss': nan,\n",
      "    'val_precision': nan,\n",
      "    'val_recall': nan}\n",
      "---------------------------------------------------------------------------------\n",
      "Utilizes 1536/1632 validation points\n",
      "Utilizes 11520/11755 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45/45 [==============================] - ETA: 0s - loss: 0.5375 - binary_accuracy: 0.7685 - precision: 0.7873 - recall: 0.90842 root error(s) found.\n",
      "  (0) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/dense/Sigmoid:0) = ] [[8.15438179e-05][0.0484126918][0.00147369935]...] [y (Cast_6/x:0) = ] [0]\n",
      "\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n",
      "\t [[assert_less_equal/Assert/AssertGuard/pivot_f/_13/_25]]\n",
      "  (1) Invalid argument:  assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:] [x (model/dense/Sigmoid:0) = ] [[8.15438179e-05][0.0484126918][0.00147369935]...] [y (Cast_6/x:0) = ] [0]\n",
      "\t [[{{node assert_greater_equal/Assert/AssertGuard/else/_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_test_function_8731]\n",
      "\n",
      "Function call stack:\n",
      "test_function -> test_function\n",
      "\n",
      "Something went wrong while training the model (most likely)\n",
      "Model nr 2 of 100\n",
      "[   {'index': 1, 'model_nr_type': 'InceptionTime'},\n",
      "    {   'batch_size': 64,\n",
      "        'bottleneck_size': 44,\n",
      "        'epochs': 50,\n",
      "        'filter_size': 56,\n",
      "        'l1_r': 0,\n",
      "        'l2_r': 0.1,\n",
      "        'learning_rate': 0.001,\n",
      "        'module_activation': 'linear',\n",
      "        'module_output_activation': 'relu',\n",
      "        'num_filters': 20,\n",
      "        'num_modules': 6,\n",
      "        'optimizer': 'adam',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'reg_module': True,\n",
      "        'reg_residual': False,\n",
      "        'residual_activation': 'tanh',\n",
      "        'use_bottleneck': True,\n",
      "        'use_residuals': False}]\n",
      "Saving file. 2 rows.\n",
      "results_InceptionTime_noiseNotNoise_timeAug_noiseAug_earlyS_numChannels-2.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6000, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 6000, 44)     88          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 6000, 2)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 6000, 20)     49280       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 6000, 20)     24640       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 6000, 20)     12320       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6000, 20)     40          max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6000, 80)     0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 6000, 80)     320         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 6000, 80)     0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 6000, 44)     3520        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 6000, 80)     0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 6000, 20)     49280       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 6000, 20)     24640       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 6000, 20)     12320       conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 6000, 20)     1600        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 6000, 80)     0           conv1d_6[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 6000, 80)     320         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 6000, 80)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 6000, 44)     3520        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 6000, 80)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 6000, 20)     49280       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 6000, 20)     24640       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 6000, 20)     12320       conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6000, 80)     0           conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 6000, 80)     320         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 6000, 80)     0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 6000, 44)     3520        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 6000, 80)     0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 6000, 20)     49280       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 6000, 20)     24640       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 6000, 20)     12320       conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 6000, 80)     0           conv1d_16[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 6000, 80)     320         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 6000, 80)     0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 6000, 44)     3520        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 6000, 80)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 6000, 20)     49280       conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 6000, 20)     24640       conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 6000, 20)     12320       conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 6000, 80)     0           conv1d_21[0][0]                  \n",
      "                                                                 conv1d_22[0][0]                  \n",
      "                                                                 conv1d_23[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 6000, 80)     320         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 6000, 80)     0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 6000, 44)     3520        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 6000, 80)     0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 6000, 20)     49280       conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 6000, 20)     24640       conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 6000, 20)     12320       conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 6000, 20)     1600        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 6000, 80)     0           conv1d_26[0][0]                  \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 6000, 80)     320         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 6000, 80)     0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 80)           0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            81          global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 545,169\n",
      "Trainable params: 544,209\n",
      "Non-trainable params: 960\n",
      "__________________________________________________________________________________________________\n",
      "Starting: \n",
      "{   'batch_size': 64,\n",
      "    'bottleneck_size': 44,\n",
      "    'epochs': 50,\n",
      "    'filter_size': 56,\n",
      "    'l1_r': 0,\n",
      "    'l2_r': 0.1,\n",
      "    'learning_rate': 0.001,\n",
      "    'module_activation': 'linear',\n",
      "    'module_output_activation': 'relu',\n",
      "    'num_filters': 20,\n",
      "    'num_modules': 6,\n",
      "    'optimizer': 'adam',\n",
      "    'output_layer_activation': 'sigmoid',\n",
      "    'reg_module': True,\n",
      "    'reg_residual': False,\n",
      "    'residual_activation': 'tanh',\n",
      "    'train_accuracy': nan,\n",
      "    'train_loss': nan,\n",
      "    'train_precision': nan,\n",
      "    'train_recall': nan,\n",
      "    'use_bottleneck': True,\n",
      "    'use_residuals': False,\n",
      "    'val_accuracy': nan,\n",
      "    'val_loss': nan,\n",
      "    'val_precision': nan,\n",
      "    'val_recall': nan}\n",
      "---------------------------------------------------------------------------------\n",
      "Utilizes 1600/1632 validation points\n",
      "Utilizes 11712/11755 training points\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 83s 407ms/step - loss: 24.7425 - binary_accuracy: 0.8221 - precision: 0.8807 - recall: 0.8553 - val_loss: 2.0917 - val_binary_accuracy: 0.6894 - val_precision: 0.6894 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "183/183 [==============================] - 78s 425ms/step - loss: 1.3622 - binary_accuracy: 0.8563 - precision: 0.9304 - recall: 0.8466 - val_loss: 1.0956 - val_binary_accuracy: 0.6769 - val_precision: 0.6769 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "183/183 [==============================] - 75s 410ms/step - loss: 0.6186 - binary_accuracy: 0.8681 - precision: 0.9355 - recall: 0.8578 - val_loss: 0.6941 - val_binary_accuracy: 0.8769 - val_precision: 0.9200 - val_recall: 0.8829\n",
      "Epoch 4/50\n",
      "183/183 [==============================] - 75s 408ms/step - loss: 0.6413 - binary_accuracy: 0.8798 - precision: 0.9369 - recall: 0.8787 - val_loss: 0.7200 - val_binary_accuracy: 0.8887 - val_precision: 0.9482 - val_recall: 0.8830\n",
      "Epoch 5/50\n",
      "183/183 [==============================] - 76s 417ms/step - loss: 0.5674 - binary_accuracy: 0.8736 - precision: 0.9337 - recall: 0.8720 - val_loss: 0.6664 - val_binary_accuracy: 0.9031 - val_precision: 0.9346 - val_recall: 0.9178\n",
      "Epoch 6/50\n",
      "183/183 [==============================] - 76s 414ms/step - loss: 0.5371 - binary_accuracy: 0.8757 - precision: 0.9348 - recall: 0.8722 - val_loss: 0.6208 - val_binary_accuracy: 0.8775 - val_precision: 0.9504 - val_recall: 0.8545\n",
      "Epoch 7/50\n",
      "183/183 [==============================] - 76s 414ms/step - loss: 0.5622 - binary_accuracy: 0.8794 - precision: 0.9333 - recall: 0.8841 - val_loss: 0.7397 - val_binary_accuracy: 0.6325 - val_precision: 0.9898 - val_recall: 0.4546\n",
      "Epoch 8/50\n",
      "183/183 [==============================] - 76s 413ms/step - loss: 0.4985 - binary_accuracy: 0.8698 - precision: 0.9293 - recall: 0.8720 - val_loss: 0.8948 - val_binary_accuracy: 0.6827 - val_precision: 0.6831 - val_recall: 0.9991\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.000500000023749.\n",
      "Epoch 9/50\n",
      "183/183 [==============================] - 76s 417ms/step - loss: 0.4080 - binary_accuracy: 0.8796 - precision: 0.9325 - recall: 0.8835 - val_loss: 0.8722 - val_binary_accuracy: 0.6825 - val_precision: 0.6825 - val_recall: 1.0000\n",
      "Epoch 10/50\n",
      "183/183 [==============================] - 76s 415ms/step - loss: 0.4002 - binary_accuracy: 0.8808 - precision: 0.9347 - recall: 0.8803 - val_loss: 0.8959 - val_binary_accuracy: 0.9056 - val_precision: 0.9383 - val_recall: 0.9116\n",
      "Epoch 11/50\n",
      "183/183 [==============================] - 76s 416ms/step - loss: 0.4124 - binary_accuracy: 0.8805 - precision: 0.9307 - recall: 0.8867 - val_loss: 0.6235 - val_binary_accuracy: 0.7450 - val_precision: 0.7284 - val_recall: 0.9860\n",
      "Epoch 12/50\n",
      "183/183 [==============================] - 77s 420ms/step - loss: 0.3673 - binary_accuracy: 0.8884 - precision: 0.9401 - recall: 0.8888 - val_loss: 0.4620 - val_binary_accuracy: 0.7044 - val_precision: 0.6949 - val_recall: 0.9953\n",
      "Epoch 13/50\n",
      "183/183 [==============================] - 76s 417ms/step - loss: 0.4022 - binary_accuracy: 0.8931 - precision: 0.9351 - recall: 0.9012 - val_loss: 0.6160 - val_binary_accuracy: 0.6662 - val_precision: 0.6642 - val_recall: 1.0000\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.000250000011874.\n",
      "Epoch 14/50\n",
      "183/183 [==============================] - 76s 416ms/step - loss: 0.3363 - binary_accuracy: 0.8876 - precision: 0.9292 - recall: 0.8983 - val_loss: 0.4891 - val_binary_accuracy: 0.8150 - val_precision: 0.7854 - val_recall: 0.9846\n",
      "Epoch 15/50\n",
      "183/183 [==============================] - 78s 425ms/step - loss: 0.3136 - binary_accuracy: 0.8932 - precision: 0.9324 - recall: 0.9065 - val_loss: 0.5886 - val_binary_accuracy: 0.6909 - val_precision: 0.6884 - val_recall: 1.0000\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00015: early stopping\n",
      "25/25 [==============================] - 1s 50ms/step - loss: 0.4582 - binary_accuracy: 0.8950 - precision: 0.9499 - recall: 0.8946\n",
      "{   'binary_accuracy': 0.8949999809265137,\n",
      "    'loss': 0.45821940898895264,\n",
      "    'precision': 0.9498553276062012,\n",
      "    'recall': 0.8946412205696106}\n",
      "183/183 [==============================] - 9s 47ms/step - loss: 0.5267 - binary_accuracy: 0.9036 - precision: 0.9470 - recall: 0.9057\n",
      "Num samples: 1632, Num predictions: 1632\n",
      "tf.Tensor(\n",
      "[[696  66]\n",
      " [ 32 838]], shape=(2, 2), dtype=int32)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       noise       0.96      0.91      0.93       762\n",
      "   not_noise       0.93      0.96      0.94       870\n",
      "\n",
      "    accuracy                           0.94      1632\n",
      "   macro avg       0.94      0.94      0.94      1632\n",
      "weighted avg       0.94      0.94      0.94      1632\n",
      "\n",
      "Saving file. 2 rows.\n",
      "results_InceptionTime_noiseNotNoise_timeAug_noiseAug_earlyS_numChannels-2.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model nr 3 of 100\n",
      "[   {'index': 2, 'model_nr_type': 'InceptionTime'},\n",
      "    {   'batch_size': 64,\n",
      "        'bottleneck_size': 42,\n",
      "        'epochs': 50,\n",
      "        'filter_size': 30,\n",
      "        'l1_r': 0.0001,\n",
      "        'l2_r': 0.01,\n",
      "        'learning_rate': 0.1,\n",
      "        'module_activation': 'relu',\n",
      "        'module_output_activation': 'linear',\n",
      "        'num_filters': 28,\n",
      "        'num_modules': 1,\n",
      "        'optimizer': 'sgd',\n",
      "        'output_layer_activation': 'sigmoid',\n",
      "        'reg_module': False,\n",
      "        'reg_residual': True,\n",
      "        'residual_activation': 'relu',\n",
      "        'use_bottleneck': True,\n",
      "        'use_residuals': False}]\n",
      "Saving file. 3 rows.\n",
      "results_InceptionTime_noiseNotNoise_timeAug_noiseAug_earlyS_numChannels-2.csv saved to path:   /media/tord/T7/Thesis_ssd/MasterThesis3/GridSearchResults/2_classes\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 6000, 2)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 6000, 42)     84          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 6000, 2)      0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 6000, 28)     35280       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 6000, 28)     17640       conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 6000, 28)     8232        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 6000, 28)     56          max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 6000, 112)    0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "                                                                 conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 6000, 112)    448         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 6000, 112)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 112)          0           activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            113         global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 61,853\n",
      "Trainable params: 61,629\n",
      "Non-trainable params: 224\n",
      "__________________________________________________________________________________________________\n",
      "Starting: \n",
      "{   'batch_size': 64,\n",
      "    'bottleneck_size': 42,\n",
      "    'epochs': 50,\n",
      "    'filter_size': 30,\n",
      "    'l1_r': 0.0001,\n",
      "    'l2_r': 0.01,\n",
      "    'learning_rate': 0.1,\n",
      "    'module_activation': 'relu',\n",
      "    'module_output_activation': 'linear',\n",
      "    'num_filters': 28,\n",
      "    'num_modules': 1,\n",
      "    'optimizer': 'sgd',\n",
      "    'output_layer_activation': 'sigmoid',\n",
      "    'reg_module': False,\n",
      "    'reg_residual': True,\n",
      "    'residual_activation': 'relu',\n",
      "    'train_accuracy': nan,\n",
      "    'train_loss': nan,\n",
      "    'train_precision': nan,\n",
      "    'train_recall': nan,\n",
      "    'use_bottleneck': True,\n",
      "    'use_residuals': False,\n",
      "    'val_accuracy': nan,\n",
      "    'val_loss': nan,\n",
      "    'val_precision': nan,\n",
      "    'val_recall': nan}\n",
      "---------------------------------------------------------------------------------\n",
      "Utilizes 1600/1632 validation points\n",
      "Utilizes 11712/11755 training points\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183/183 [==============================] - 12s 51ms/step - loss: 0.5990 - binary_accuracy: 0.6759 - precision: 0.6790 - recall: 0.9854 - val_loss: 0.5936 - val_binary_accuracy: 0.6894 - val_precision: 0.6894 - val_recall: 1.0000\n",
      "Epoch 2/50\n",
      "183/183 [==============================] - 10s 54ms/step - loss: 0.5280 - binary_accuracy: 0.7474 - precision: 0.7403 - recall: 0.9549 - val_loss: 0.6851 - val_binary_accuracy: 0.6769 - val_precision: 0.6769 - val_recall: 1.0000\n",
      "Epoch 3/50\n",
      "183/183 [==============================] - 9s 50ms/step - loss: 0.4830 - binary_accuracy: 0.7819 - precision: 0.7849 - recall: 0.9193 - val_loss: 0.5764 - val_binary_accuracy: 0.6556 - val_precision: 0.6488 - val_recall: 0.9941\n",
      "Epoch 4/50\n",
      "183/183 [==============================] - 9s 48ms/step - loss: 0.4665 - binary_accuracy: 0.7940 - precision: 0.8036 - recall: 0.9146 - val_loss: 0.5096 - val_binary_accuracy: 0.7237 - val_precision: 0.7219 - val_recall: 0.9591\n",
      "Epoch 5/50\n",
      "183/183 [==============================] - 9s 48ms/step - loss: 0.4655 - binary_accuracy: 0.7971 - precision: 0.8069 - recall: 0.9136 - val_loss: 0.5136 - val_binary_accuracy: 0.7319 - val_precision: 0.7247 - val_recall: 0.9594\n",
      "Epoch 6/50\n",
      "183/183 [==============================] - 10s 52ms/step - loss: 0.4568 - binary_accuracy: 0.7977 - precision: 0.8098 - recall: 0.9055 - val_loss: 0.4859 - val_binary_accuracy: 0.7381 - val_precision: 0.7300 - val_recall: 0.9471\n",
      "Epoch 7/50\n",
      "183/183 [==============================] - 9s 48ms/step - loss: 0.4546 - binary_accuracy: 0.8030 - precision: 0.8190 - recall: 0.9071 - val_loss: 0.4708 - val_binary_accuracy: 0.7825 - val_precision: 0.7788 - val_recall: 0.9420\n",
      "Epoch 8/50\n",
      "183/183 [==============================] - 9s 47ms/step - loss: 0.4483 - binary_accuracy: 0.8063 - precision: 0.8211 - recall: 0.9089 - val_loss: 0.4628 - val_binary_accuracy: 0.7688 - val_precision: 0.7625 - val_recall: 0.9611\n",
      "Epoch 9/50\n",
      "183/183 [==============================] - 9s 47ms/step - loss: 0.4403 - binary_accuracy: 0.8128 - precision: 0.8244 - recall: 0.9139 - val_loss: 0.4637 - val_binary_accuracy: 0.7800 - val_precision: 0.7729 - val_recall: 0.9597\n",
      "Epoch 10/50\n",
      "183/183 [==============================] - 9s 48ms/step - loss: 0.4442 - binary_accuracy: 0.8095 - precision: 0.8217 - recall: 0.9074 - val_loss: 0.5177 - val_binary_accuracy: 0.7856 - val_precision: 0.7668 - val_recall: 0.9528\n",
      "Epoch 11/50\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.4406 - binary_accuracy: 0.8093 - precision: 0.8234 - recall: 0.9085"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-521629dc4aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                             \u001b[0muse_reduced_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_reduced_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mband_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mband_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mband_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighpass_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhighpass_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                                             start_from_scratch = start_from_scratch, is_lstm = is_lstm, log_data = log_data, num_channels = num_channels)\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomGridSearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-3ce8538338dd>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Fit the model using the generated args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Evaluate the fitted model on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1163\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1165\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1166\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1421\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1423\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1424\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    851\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2991\u001b[0m       (graph_function,\n\u001b[1;32m   2992\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2993\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2994\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1937\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1939\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1940\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    562\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    565\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# IncepTime grid:\n",
    "hyper_grid = {    \n",
    "    \"batch_size\" : [64, 128, 256],\n",
    "    \"epochs\" : [50, 50, 50, 50, 50, 50, 50, 50, 50],\n",
    "    \"learning_rate\" : [0.1, 0.01, 0.01, 0.001, 0.0001],\n",
    "    \"optimizer\" : [\"adam\", \"adam\", \"adam\", \"adam\", \"rmsprop\", \"sgd\", \"adam\", \"rmsprop\", \"sgd\"],\n",
    "    \"use_residuals\" : [True, True, False],\n",
    "    \"use_bottleneck\" : [True, True, False],\n",
    "    \"num_modules\" : np.concatenate((np.array([1]), np.arange(3, 9, 3))),\n",
    "    \"filter_size\" : np.arange(30, 60, 2),\n",
    "    \"bottleneck_size\" : np.arange(30, 50, 2),\n",
    "    \"num_filters\" : np.arange(20, 40, 2),\n",
    "    \"residual_activation\" : [\"relu\", \"relu\", \"relu\", \"relu\", \"tanh\"],\n",
    "    \"module_activation\" : [\"linear\", \"linear\", \"linear\", \"relu\", \"tanh\"],\n",
    "    \"module_output_activation\" : [\"relu\", \"relu\", \"relu\", \"relu\", \"linear\", \"tanh\"],\n",
    "    \"output_layer_activation\": [\"sigmoid\"],\n",
    "    \"reg_residual\": [True, False, False],\n",
    "    \"reg_module\" : [True, False, False],\n",
    "    \"l1_r\" : [0.1, 0.01, 0.01, 0.001, 0.0001, 0],\n",
    "    \"l2_r\" : [0.1, 0.01, 0.01, 0.001, 0.0001, 0]\n",
    "}\n",
    "\n",
    "# Dense grid:\n",
    "\"\"\"\n",
    "hyper_grid = {\n",
    "        \"batch_size\" : [64, 128, 256],\n",
    "        \"epochs\" : [50, 50, 50, 50, 50, 50, 50, 50, 50],\n",
    "        \"learning_rate\" : [0.1, 0.01, 0.01, 0.001, 0.001, 0.0001, 0.0001],\n",
    "        \"optimizer\" : [\"sgd\", \"sgd\", \"sgd\", \"sgd\", \"rmsprop\", \"adam\", \"rmsprop\", \"sgd\"],\n",
    "        \"num_layers\" : [1, 2, 3, 4, 5],\n",
    "        \"units\" : np.arange(100, 300, 10),\n",
    "        \"use_layerwise_dropout_batchnorm\" : [False, True],\n",
    "        \"decay_sequence\" : [[1,2,4,4,2,1], [1,4,8,8,4,1], [1,1,1,1,1,1], [1, 2, 4, 6, 8, 10]],\n",
    "        \"dropout_rate\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0],\n",
    "        \"l2_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "        \"l1_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "        \"activation\" : [\"tanh\", \"tanh\", \"relu\", \"relu\", \"relu\", \"sigmoid\", \"softmax\"],\n",
    "        \"output_layer_activation\" : [\"sigmoid\"]\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model_type = \"InceptionTime\"\n",
    "is_lstm = True\n",
    "num_channels = 2    \n",
    "\n",
    "use_time_augmentor = True\n",
    "scaler_name = \"standard\"\n",
    "use_noise_augmentor = True\n",
    "filter_name = None\n",
    "band_min = 2.0\n",
    "band_max = 4.0\n",
    "highpass_freq = 15\n",
    "\n",
    "n_picks = 100\n",
    "\n",
    "use_tensorboard = False\n",
    "use_liveplots = False\n",
    "use_custom_callback = True\n",
    "use_early_stopping = True\n",
    "start_from_scratch = True\n",
    "use_reduced_lr = True\n",
    "log_data = True\n",
    "\n",
    "shutdown = False\n",
    "\n",
    "def clear_tensorboard_dir():\n",
    "        import os\n",
    "        import shutil\n",
    "        path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "        files = os.listdir(path)\n",
    "        print(files)\n",
    "        for f in files:\n",
    "            shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "\n",
    "\n",
    "\n",
    "randomGridSearch = RandomGridSearchDynamic(loadData, train_ds, val_ds, test_ds, model_type, scaler_name, use_time_augmentor, use_noise_augmentor,\n",
    "                                            filter_name, n_picks, hyper_grid=hyper_grid, use_tensorboard = use_tensorboard, \n",
    "                                            use_liveplots = use_liveplots, use_custom_callback = use_custom_callback, use_early_stopping = use_early_stopping, \n",
    "                                            use_reduced_lr = use_reduced_lr, band_min = band_min, band_max = band_max, highpass_freq = highpass_freq, \n",
    "                                            start_from_scratch = start_from_scratch, is_lstm = is_lstm, log_data = log_data, num_channels = num_channels)\n",
    "results_df, min_loss, max_accuracy, max_precision, max_recall = randomGridSearch.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = tf.keras.layers.Input((1))\n",
    "x = input_layer\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "model = tf.keras.Model(inputs = input_layer, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "positive-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x = [[1],[0]], y = [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-found",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
