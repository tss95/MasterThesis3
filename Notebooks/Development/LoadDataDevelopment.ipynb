{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "os.chdir(utils.base_dir)\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/media/tord/T7/Thesis_ssd/MasterThesis3'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-a1b88c6a4374>, line 98)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a1b88c6a4374>\"\u001b[0;36m, line \u001b[0;32m98\u001b[0m\n\u001b[0;31m    zero_column = np.zeros((len(self.train), 1), , dtype = np.int8)\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3.0/'\n",
    "os.chdir(base_dir)\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "\n",
    "        \n",
    "        \n",
    "class LoadData():\n",
    "    \n",
    "    def __init__(self, earth_explo_only = False, noise_earth_only = False, noise_not_noise = False, \n",
    "                 downsample = False, upsample = False, frac_diff = 1, seed = None, subsample_size = 1,\n",
    "                 balance_non_train_set = False, use_true_test_set = False, load_everything = False, \n",
    "                 load_first_batch = False, even_balance = False):\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.earth_explo_only = earth_explo_only\n",
    "        self.noise_earth_only = noise_earth_only\n",
    "        self.noise_not_noise = noise_not_noise\n",
    "        self.downsample = downsample\n",
    "        self.upsample = upsample\n",
    "        self.frac_diff = frac_diff\n",
    "        self.subsample_size = subsample_size\n",
    "        self.balance_non_train_set = balance_non_train_set\n",
    "        self.use_true_test_set = use_true_test_set\n",
    "        # If true, then the class distribution will be equal to 1/num_classes.\n",
    "        self.even_balance = even_balance\n",
    "        \n",
    "        \n",
    "        self.csv_folder = utils.csv_folder\n",
    "        self.data_csv_name = utils.data_csv_name\n",
    "        if load_first_batch:\n",
    "            self.data_csv_name = utils.batch_1_csv_name\n",
    "            assert not load_everything, \"Load everything should be False when using the first batch. A test set has not been generated for this dataset\"\n",
    "        if load_everything:\n",
    "            self.data_csv_name = utils.no_nans_no_induced_csv_name\n",
    "            self.full_ds = self.csv_to_numpy(self.data_csv_name, self.csv_folder)\n",
    "            self.create_label_dict()\n",
    "        else:\n",
    "            self.test_csv_name = 'DO_NOT_TOUCH_test_set.csv'\n",
    "            self.full_ds = self.csv_to_numpy(self.data_csv_name, self.csv_folder)\n",
    "            self.create_label_dict()\n",
    "            self.load_data()\n",
    "            if self.use_true_test_set:\n",
    "                self.true_test_ds = self.csv_to_numpy(self.test_csv_name, self.csv_folder)\n",
    "                print(\"WARNING!\")\n",
    "                print(\"You are using the true test set.\")\n",
    "                print(\"THIS SHOULD ONLY BE USED ONCE\")\n",
    "                print(\"If this is an error, please set use_true_test_set = False and reload the kernel\")\n",
    "\n",
    "            if sum([self.earth_explo_only, self.noise_earth_only, self.noise_not_noise]) > 1:\n",
    "                raise Exception(\"Invalid load data arguments.\")\n",
    "\n",
    "    def load_data(self):\n",
    "        if not self.use_true_test_set:\n",
    "            if self.balance_non_train_set:\n",
    "                self.full_ds = self.balance_ds(self.full_ds, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "                if self.even_balance:\n",
    "                    self.full_ds = self.even_label_occurances(self.full_ds)\n",
    "                self.full_ds = self.full_ds[np.random.choice(self.full_ds.shape[0], int(len(self.full_ds)*self.subsample_size), replace = False)]\n",
    "                self.refine_full_ds()\n",
    "                self.train, val_test = train_test_split(self.full_ds, test_size = 0.25, random_state = self.seed)\n",
    "                self.val, self.test = train_test_split(val_test, test_size = 0.4, random_state = self.seed)\n",
    "                if not self.earth_explo_only:\n",
    "                    self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "            else:\n",
    "                \n",
    "                self.full_ds = self.balance_ds(self.full_ds, False, False, frac_diff = self.frac_diff)\n",
    "                self.full_ds = self.full_ds[np.random.choice(self.full_ds.shape[0], int(len(self.full_ds)*self.subsample_size), replace = False)]\n",
    "                if self.earth_explo_only or self.noise_earth_only:\n",
    "                    if self.earth_explo_only:\n",
    "                        self.noise_ds = np.array(self.full_ds[self.full_ds[:,1] == \"noise\"])\n",
    "                        self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"noise\"])\n",
    "                        # The noise needs to be reduced in order to work properly in noise augmentor\n",
    "                        self.noise_ds, _ = train_test_split(self.noise_ds, test_size = 0.25, random_state = self.seed)\n",
    "                        zero_column = np.zeros((len(self.noise_ds), 1), dtype = np.int8)\n",
    "                        self.noise_ds = np.hstack((self.noise_ds, zero_column))\n",
    "                    else:\n",
    "                        self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"explosion\"])\n",
    "                self.train, val_test = train_test_split(self.full_ds, test_size = 0.25, random_state = self.seed)\n",
    "                self.val, self.test = train_test_split(val_test, test_size = 0.4, random_state = self.seed)\n",
    "                self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "                if self.even_balance:\n",
    "                    self.train = self.even_label_occurances(self.train)\n",
    "                if self.upsample:\n",
    "                    self.train = self.map_redundancy(self.train)\n",
    "                else:\n",
    "                    zero_column = np.zeros((len(self.train), 1), , dtype = np.int8)\n",
    "                    self.train = np.hstack((self.train, zero_column))\n",
    "                zero_val = np.zeros((len(self.val), 1), dtype = np.int8)\n",
    "                zero_test = np.zeros((len(self.test), 1), dtype = np.int8)\n",
    "                self.val = np.hstack((self.val, zero_val), dtype = np.int8)\n",
    "                self.test = np.hstack((self.test, zero_test), dtype = np.int8)\n",
    "                self.full_ds = np.concatenate((self.train, self.val))\n",
    "                self.full_ds = np.concatenate((self.full_ds, self.test))\n",
    "                if not self.earth_explo_only:\n",
    "                    self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "                \n",
    "        else:\n",
    "            print(\"Write this code when you are ready to use the test set.\")\n",
    "            raise Exception(\"The code has not yet been written for the true test set.\")\n",
    "            \n",
    "                \n",
    "    \n",
    "                \n",
    "    \n",
    "    def refine_full_ds(self):\n",
    "        # Method which removes undesired classes from the dataset. \n",
    "        # If noise is removed, a seperate dataset will be created for use in NoiseAugmentation\n",
    "        # Method also calls for map redundancy, or performs equivalent action for formatting consistancy if unecessary.\n",
    "        if self.earth_explo_only or self.noise_earth_only:\n",
    "            if self.earth_explo_only:\n",
    "                self.noise_ds = np.array(self.full_ds[self.full_ds[:,1] == \"noise\"])\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"noise\"])\n",
    "                zero_column = np.zeros((len(self.noise_ds), 1), dtype = np.int8)\n",
    "                self.noise_ds = np.hstack((self.noise_ds, zero_column))\n",
    "            if self.noise_earth_only:\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"explosion\"])\n",
    "        if self.earth_explo_only and self.noise_earth_only:\n",
    "            raise Exception(\"Cannot have both earth_explo_only = True and noise_earth_only = True\")\n",
    "        # Only need to map redundency if upsampling, as upsampling is the cause of redundancy\n",
    "        if self.upsample:\n",
    "            self.full_ds = self.map_redundancy(self.full_ds)\n",
    "        else:\n",
    "            zero_column = np.zeros((len(self.full_ds), 1), dtype = np.int8)\n",
    "            self.full_ds = np.hstack((self.full_ds, zero_column))\n",
    "    \n",
    "    def create_label_dict(self):\n",
    "        # Method which produces the dictionary for labels. This is used in order to disguise labels during training.\n",
    "        if self.earth_explo_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'explosion' : 1}\n",
    "        elif self.noise_earth_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1}\n",
    "        elif self.noise_not_noise:\n",
    "            self.label_dict = { 'noise': 0, 'earthquake' : 1, 'explosion' : 1}\n",
    "        else:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1, 'explosion' : 2, 'induced' : 3}\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        return self.full_ds, self.train, self.val, self.test  \n",
    "        \n",
    "    def csv_to_numpy(self, data_csv, csv_folder):\n",
    "        with open(csv_folder + '/' + data_csv) as file:\n",
    "            file_list = np.array(list(file))\n",
    "            dataset = np.empty((len(file_list), 2), dtype=object)\n",
    "            for idx, event in enumerate(file_list):\n",
    "                path, label = event.split(',')\n",
    "                dataset[idx][0] = path.rstrip()\n",
    "                dataset[idx][1] = label.rstrip()\n",
    "            file.close()\n",
    "        return dataset\n",
    "    \n",
    "    def downsample_label(self, target_label, ds, n_samples):\n",
    "        # Method which produces n_samples of the targeted label, and returns the dataset unchanged but with n_samples of the targeted label.\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label], dtype = object)\n",
    "        down_ds = np.array([y for y in ds if y[1] != target_label], dtype = object)\n",
    "        np.random.seed(self.seed)\n",
    "        down_ds = np.concatenate((down_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(down_ds)\n",
    "\n",
    "    def upsample_label(self, target_label, ds, n_samples):\n",
    "        # Seemingly equivalent to downsample_label (?)\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label])\n",
    "        up_ds = [y for y in ds if y[1] != target_label]\n",
    "        np.random.seed(self.seed)\n",
    "        up_ds = np.concatenate((up_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(up_ds)\n",
    "\n",
    "    def frac_diff_n_samples(self, frac_diff, min_counts, max_counts):\n",
    "        # Returns the difference between the most occuring label and the least occuring label, multiplied by some fraction, and added to the number of the least occuring label.\n",
    "        # Potentially a really stupid idea which is now implemented. Can be omitted by setting frac diff = 1 \n",
    "        diff = max_counts - min_counts\n",
    "        n_samples = int(min_counts + diff*frac_diff)\n",
    "        return n_samples\n",
    "\n",
    "    def balance_ds(self, ds, downsample, upsample, frac_diff = 0):\n",
    "        # Downsamples the two most occuring labels, and upsamples the most occuring label.\n",
    "        unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "        nr_classes = len(unique_labels)\n",
    "        if downsample:\n",
    "            # Downsamples by first reducing the largest class, then the second class.\n",
    "            for i in range(nr_classes-1):\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                most_occuring_label = unique_labels[np.where(counts == max(counts))]\n",
    "                n_samples_frac_diff = self.frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "                ds = self.downsample_label(most_occuring_label, ds, n_samples_frac_diff)\n",
    "        if upsample:\n",
    "            if frac_diff != 0:\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                least_occuring_label = unique_labels[np.where(counts == min(counts))]\n",
    "                n_samples_for_balance = max(counts)\n",
    "                ds = self.upsample_label(least_occuring_label, ds, n_samples_for_balance)\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(ds)\n",
    "        return ds\n",
    "\n",
    "    def even_label_occurances(self, ds):\n",
    "        # In cases where labels are disguised as something else, this method will even them out so that the label distribution is even. \n",
    "        num_classes = len(set(self.label_dict.values()))\n",
    "        print(num_classes, len(set(self.label_dict.keys())))\n",
    "        if num_classes != len(set(self.label_dict.keys())):\n",
    "            ids = self.label_dict.values()\n",
    "            most_occuring_id = max(ids)\n",
    "            least_occuring_id = min(ids)\n",
    "            label_count_dict = {}\n",
    "            for label, id in self.label_dict.items():\n",
    "                label_count_dict[label] = len(ds[ds[:,1] == label])\n",
    "            print(label_count_dict)\n",
    "            # Want the labels which share id, to combined have the same number of events of the event with the unique id.\n",
    "            for label, id in self.label_dict.items():\n",
    "                if id == most_occuring_id:\n",
    "                    pure_label_ds = ds[ds[:,1] == label]\n",
    "                    ds_without_label = ds[ds[:,1] != label]\n",
    "                    ds = np.concatenate((ds_without_label, pure_label_ds[np.random.choice(pure_label_ds.shape[0], label_count_dict[label]//num_classes, replace = False)]))\n",
    "        return ds\n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return self.label_dict\n",
    "    \n",
    "    def map_redundancy(self, ds):\n",
    "        # Creates a redundancy index which distinguishes events which are sampled multiple times.\n",
    "        # Primarily used in timeAugmentation in order to create unique augmentations of otherwise identical events.\n",
    "        # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "        new_column = np.zeros((len(ds), 1), dtype = np.int8)\n",
    "        mapped_ds = np.hstack((ds, new_column))\n",
    "        earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "        unique_earth_paths = set(earth_ds[:,0])\n",
    "        nr_unique_earth_paths = len(unique_earth_paths)\n",
    "        for idx, path in enumerate(unique_earth_paths):\n",
    "            self.progress_bar(idx + 1, nr_unique_earth_paths)\n",
    "            nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "            label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "            repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "            current_index = 0\n",
    "            if len(repeating_indexes) > 1:\n",
    "                for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                    mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                    mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                    mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                    current_index += 1\n",
    "        return mapped_ds\n",
    "\n",
    "    def progress_bar(self, current, total, barLength = 40):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Mapping redundancy: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": []
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 1,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : False\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "154200 16736 11158\n",
      "Total: 154200, earthquake: 77100, explosion: 77100\n",
      "Total: 16736, earthquake: 1328, explosion: 15408\n",
      "Nr noise samples 79499\n",
      "Non noise prop: 1.0\n",
      "Train non noise prop: 1.0\n",
      "Val non noise prop: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))\n",
    "print(f\"Non noise prop: {len(full_ds[full_ds[:,1] != 'noise'])/len(full_ds)}\")\n",
    "print(f\"Train non noise prop: {len(train_ds[train_ds[:,1] != 'noise'])/len(train_ds)}\")\n",
    "print(f\"Val non noise prop: {len(val_ds[val_ds[:,1] != 'noise'])/len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_correct_classes(ds):\n",
    "    for path, label, red in ds:\n",
    "        path_split = path.split('/')\n",
    "        if label == \"earthquake\" or label == \"explosion\":\n",
    "            label = label + \"s\"\n",
    "        assert label in path_split, f\"Incorrect labeling: {label} {path_split}\"\n",
    "\n",
    "assert_correct_classes(full_ds)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solutions:\n",
    "        # Noise-not-noise:\n",
    "        # Either upsample noise 2 times its current length, or downsample earth and explo so that each contain half of thier current.\n",
    "        # earth-explo:\n",
    "        # non issue with frac diff = 0\n",
    "        # Noise-earth:\n",
    "        # Non issue with frac_diff = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'even_label_occurances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-44b300333eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meven_label_occurances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'even_label_occurances' is not defined"
     ]
    }
   ],
   "source": [
    "new_ds = even_label_occurances(loadData, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,counts = np.unique(new_ds[:,1], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2018-05-26T03.16.35.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2016-03-02T08.15.13.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2003-01-25T06.40.11.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       ..., \n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2005-02-06T20.35.45.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/1999-01-05T12.58.48.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2004-04-28T07.47.53.000000Z.h5',\n",
       "        'noise', 0.0]], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "noise_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2012-09-03T19.17.49.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/1998-10-24T20.08.06.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2002-09-22T23.59.56.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       ..., \n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2010-05-05T14.20.28.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/explosions/2017-02-09T08.04.37.598000Z.h5',\n",
       "        'explosion', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2015-06-05T01.55.06.000000Z.h5',\n",
       "        'noise', 0.0]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}