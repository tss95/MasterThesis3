{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from GlobalUtils import GlobalUtils\n",
    "utils = GlobalUtils()\n",
    "os.chdir(utils.base_dir)\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "\n",
    "\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3/'\n",
    "os.chdir(base_dir)\n",
    "from GlobalUtils import GlobalUtils\n",
    "glob_utils = GlobalUtils()\n",
    "\n",
    "        \n",
    "        \n",
    "class LoadData():\n",
    "    \"\"\"\n",
    "    This class is responsible from loading all the data from the premade csv to numpy arrays.\n",
    "    It also splits into training/validation/test and performs sampling of the data.\n",
    "    This class also contains label_dict, which is the translation between the one-hot encoded labels and the\n",
    "    labels in their text form.\n",
    "    \n",
    "    NOTE: The even_label_occurance function is not robust at all. It is used when doing noise-not-noise, and\n",
    "          its functionality is strictly dependent on label_dict = { 'noise': 0, 'earthquake' : 1, 'explosion' : 1}.\n",
    "    \n",
    "    PARAMETERS:\n",
    "    ----------------------------------\n",
    "    earth_explo_only:(Bool) Determining if only events labeled explosion and earthquake is to be loaded. \n",
    "                            Will also load a training set sized sample of noise events for the noise augmentor.\n",
    "                            Splits into train/val/test\n",
    "    \n",
    "    noise_earth_only:(Bool) Intended to behave much like the one above. Currently not in use.\n",
    "    \n",
    "    noise_not_noise:(Bool)  Much like earth_explo_only, this boolean loads events of all classes, and splits them\n",
    "                            into train/val/test\n",
    "    \n",
    "    downsample:(Bool)       Will reduce the most frequent event so that it matches the second most frequent class in number.\n",
    "    \n",
    "    upsample:(Bool)         Will sample with replacement the least frequent class to match the second most frequent class in number.\n",
    "    \n",
    "    frac_diff:(float)       Ignore this, always set to 1.\n",
    "    \n",
    "    seed: (int)             Seed, makes sure that the shuffling and splits of the data are the same everytime with the same parameters.\n",
    "    \n",
    "    subsample_size:(float)  Fraction, (0,1] which will select a subsample of the sets in order to reduce computational resource demand.\n",
    "    \n",
    "    balance_non_train_set:(Bool) Dictates whether or not upsampling/downsampling and even_balance should be done on the validation and test sets\n",
    "    \n",
    "    use_true_test_set:(Bool) Whether or not the test set should consist of the isolated training set, or if a pseudo test set should be used\n",
    "    \n",
    "    load_everything:(Bool) Whether or not to load all the data from the second batch of data. Will also load the isolated\n",
    "                           test set. Useful when graphing and looking at the whole dataset. NEVER USE FOR MODELING\n",
    "    \n",
    "    even_balance:(Bool)    Whether or not to balance the classes so that each class so that the distribution of each class\n",
    "                           is all_events/nr_clases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, earth_explo_only = False, noise_earth_only = False, noise_not_noise = False, \n",
    "                 downsample = False, upsample = False, frac_diff = 1, seed = None, subsample_size = 1,\n",
    "                 balance_non_train_set = False, use_true_test_set = False, load_everything = False, \n",
    "                 load_first_batch = False, even_balance = False):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.earth_explo_only = earth_explo_only\n",
    "        self.noise_earth_only = noise_earth_only\n",
    "        self.noise_not_noise = noise_not_noise\n",
    "        self.downsample = downsample\n",
    "        self.upsample = upsample\n",
    "        self.frac_diff = frac_diff\n",
    "        self.subsample_size = subsample_size\n",
    "        self.balance_non_train_set = balance_non_train_set\n",
    "        self.use_true_test_set = use_true_test_set\n",
    "        # If true, then the class distribution will be equal to 1/num_classes.\n",
    "        self.even_balance = even_balance\n",
    "        \n",
    "        self.csv_folder = glob_utils.csv_dir\n",
    "        self.data_csv_name = glob_utils.data_csv_name\n",
    "        if load_first_batch:\n",
    "            self.data_csv_name = utils.batch_1_csv_name\n",
    "            assert not load_everything, \"Load everything should be False when using the first batch. A test set has not been generated for this dataset\"\n",
    "        if load_everything:\n",
    "            #self.data_csv_name = 'event_paths_no_nan_no_induced.csv'\n",
    "            print(\"Loading all of second batch. Including the test data.\")\n",
    "            self.data_csv_name = glob_utils.no_nan_no_induced_csv_name\n",
    "        else:\n",
    "            if self.use_true_test_set:\n",
    "                self.test_csv_name = glob_utils.test_csv_name\n",
    "                self.test_ds = self.csv_to_numpy(self.test_csv_name, self.csv_folder)\n",
    "                print(\"WARNING!\")\n",
    "                print(\"You are using the true test set.\")\n",
    "                print(\"If this is an error, please set use_true_test_set = False and reload the kernel\")\n",
    "            if sum([self.earth_explo_only, self.noise_earth_only, self.noise_not_noise]) > 1:\n",
    "                raise Exception(\"Invalid load data arguments.\")\n",
    "        self.full_ds = self.csv_to_numpy(self.data_csv_name, self.csv_folder)\n",
    "        self.create_label_dict()\n",
    "        self.load_data()\n",
    "        print(\"\\n\")\n",
    "        self.print_data_info()\n",
    "        \n",
    "    \"\"\"\n",
    "    Todo:\n",
    "     - Want timeAug to fit to each data set, not the entire thing. Should speed things up.\n",
    "         - Therefore, want to map redundancy to each set, seperately.\n",
    "     - Want to split so that events in training is not in validation/test\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def load_data(self):\n",
    "        if not self.use_true_test_set:\n",
    "            if self.noise_not_noise:\n",
    "                self.load_noise_not_noise()\n",
    "            if self.earth_explo_only:\n",
    "                self.load_earth_explo_only()\n",
    "            if self.noise_earth_only:\n",
    "                raise Exception(\"Not implemented noise earth only. Seems unecessary\")\n",
    "        else:\n",
    "            if self.noise_not_noise:\n",
    "                self.load_noise_not_noise_true_test()\n",
    "            if self.earth_explo_only:\n",
    "                self.load_earth_explo_only_true_test()\n",
    "            if self.noise_earth_only:\n",
    "                raise Exception(\"Not implemented noise earth only with true test. Is unecessary\") \n",
    "                \n",
    "                \n",
    "    def load_noise_not_noise_true_test(self):\n",
    "        # Training and validation:\n",
    "        noise = self.full_ds[self.full_ds[:,1] == \"noise\"]\n",
    "        explosions = self.full_ds[self.full_ds[:,1] == \"explosion\"]\n",
    "        earthquakes = self.full_ds[self.full_ds[:,1] == \"earthquake\"]\n",
    "        \n",
    "        # Creating unique train/val splits for each class:\n",
    "        train_noise, val_noise = train_test_split(noise, test_size = 0.15, random_state = self.seed)\n",
    "        train_explo, val_explo = train_test_split(explosions, test_size = 0.15, random_state = self.seed)\n",
    "        train_earth, val_earth = train_test_split(earthquakes, test_size = 0.15, random_state = self.seed)\n",
    "        \n",
    "        # Combining events. This way we prevent duplicates in other sets.\n",
    "        self.train = np.concatenate((train_noise, train_explo, train_earth))\n",
    "        self.val = np.concatenate((val_noise, val_explo, val_earth))\n",
    "        self.test = self.test_ds\n",
    "        \n",
    "        # Up and down sampling \n",
    "        self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        if self.balance_non_train_set:\n",
    "            self.val = self.balance_ds(self.val, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        else:\n",
    "            # Shuffles the data if not\n",
    "            self.val = self.balance_ds(self.val, False, False, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, False, False, frac_diff = self.frac_diff)\n",
    "\n",
    "        if self.even_balance:\n",
    "            self.train = self.even_label_occurances(self.train)\n",
    "            if self.balance_non_train_set:\n",
    "                self.val = self.even_label_occurances(self.val)\n",
    "                self.test = self.even_label_occurances(self.test)\n",
    "\n",
    "        self.train = self.train[np.random.choice(self.train.shape[0], int(len(self.train)*self.subsample_size), replace = False)]\n",
    "        self.val = self.val[np.random.choice(self.val.shape[0], int(len(self.val)*self.subsample_size), replace = False)]\n",
    "        print(\"NO SUBSAMPLING DONE ON THE TRUE TEST SET\")\n",
    "\n",
    "        # Mapping redundnad samples for time augmentor\n",
    "        self.train = self.map_redundancy(self.train, \"train\")\n",
    "        self.val = self.map_redundancy(self.val, \"validation\")\n",
    "        self.test = self.map_redundancy(self.test, \"test\")\n",
    "        self.full_ds = np.concatenate((self.train, self.val, self.test))\n",
    "        \n",
    "        self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def load_noise_not_noise(self):\n",
    "        noise = self.full_ds[self.full_ds[:,1] == \"noise\"]\n",
    "        explosions = self.full_ds[self.full_ds[:,1] == \"explosion\"]\n",
    "        earthquakes = self.full_ds[self.full_ds[:,1] == \"earthquake\"]\n",
    "        \n",
    "        # Unique noise split\n",
    "        train_noise, val_test_noise = train_test_split(noise, test_size = 0.2, random_state = self.seed)\n",
    "        val_noise, test_noise = train_test_split(val_test_noise, test_size = 0.4, random_state = self.seed)\n",
    "        \n",
    "        #Unique explosion split\n",
    "        train_explosions, val_test_explosions = train_test_split(explosions, test_size = 0.2, random_state = self.seed)\n",
    "        val_explosions, test_explosions = train_test_split(val_test_explosions, test_size = 0.4, random_state = self.seed)\n",
    "        \n",
    "        #Unique earthquake split\n",
    "        train_earthquakes, val_test_earthquakes = train_test_split(earthquakes, test_size = 0.2, random_state = self.seed)\n",
    "        val_earthquakes, test_earthquakes = train_test_split(val_test_earthquakes, test_size = 0.4, random_state = self.seed)\n",
    "        \n",
    "        # Combining so that events are not duplicated in the splits\n",
    "        self.train = np.concatenate((train_noise, train_explosions, train_earthquakes))\n",
    "        self.val = np.concatenate((val_noise, val_explosions, val_earthquakes))\n",
    "        self.test = np.concatenate((test_noise, test_explosions, test_earthquakes))\n",
    "        \n",
    "        # Combining so that events are not duplicated in the splits\n",
    "        self.train = np.concatenate((train_noise, train_explosions, train_earthquakes))\n",
    "        self.val = np.concatenate((val_noise, val_explosions, val_earthquakes))\n",
    "        self.test = np.concatenate((test_noise, test_explosions, test_earthquakes))\n",
    "        # Up and down sampling \n",
    "        self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        if self.balance_non_train_set:\n",
    "            self.val = self.balance_ds(self.val, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        else:\n",
    "            # Shuffles the data if not\n",
    "            self.val = self.balance_ds(self.val, False, False, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, False, False, frac_diff = self.frac_diff)\n",
    "\n",
    "        if self.even_balance:\n",
    "            self.train = self.even_label_occurances(self.train)\n",
    "            if self.balance_non_train_set:\n",
    "                self.val = self.even_label_occurances(self.val)\n",
    "                self.test = self.even_label_occurances(self.test)\n",
    "\n",
    "        self.train = self.train[np.random.choice(self.train.shape[0], int(len(self.train)*self.subsample_size), replace = False)]\n",
    "        self.val = self.val[np.random.choice(self.val.shape[0], int(len(self.val)*self.subsample_size), replace = False)]\n",
    "        self.test = self.test[np.random.choice(self.test.shape[0], int(len(self.test)*self.subsample_size), replace = False)]\n",
    "\n",
    "        # Mapping redundnad samples for time augmentor\n",
    "        self.train = self.map_redundancy(self.train, \"train\")\n",
    "        self.val = self.map_redundancy(self.val, \"validation\")\n",
    "        self.test = self.map_redundancy(self.test, \"test\")\n",
    "        self.full_ds = np.concatenate((self.train, self.val, self.test))\n",
    "        \n",
    "        self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "\n",
    "        \n",
    "    def load_earth_explo_only_true_test(self):\n",
    "        noise = self.full_ds[self.full_ds[:,1] == \"noise\"]\n",
    "        explosions = self.full_ds[self.full_ds[:,1] == \"explosion\"]\n",
    "        earthquakes = self.full_ds[self.full_ds[:,1] == \"earthquake\"]\n",
    "        \n",
    "       # Creating unique train/val splits for each class:\n",
    "        train_noise, _ = train_test_split(noise, test_size = 0.15, random_state = self.seed)\n",
    "        train_explo, val_explo = train_test_split(explosions, test_size = 0.15, random_state = self.seed)\n",
    "        train_earth, val_earth = train_test_split(earthquakes, test_size = 0.15, random_state = self.seed)\n",
    "        \n",
    "        # Combing so that events are not duplciated\n",
    "        self.train = np.concatenate((train_explo, train_earth))\n",
    "        self.val = np.concatenate((val_explo, val_earth))\n",
    "        self.test = self.test_ds[self.test_ds[:,1] != \"noise\"]\n",
    "        \n",
    "        # Up and down sampling \n",
    "        self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        if self.balance_non_train_set:\n",
    "            self.val = self.balance_ds(self.val, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        else:\n",
    "            # Shuffles the data if not\n",
    "            self.val = self.balance_ds(self.val, False, False, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, False, False, frac_diff = self.frac_diff)\n",
    "\n",
    "        if self.even_balance:\n",
    "            self.train = self.even_label_occurances(self.train)\n",
    "            if self.balance_non_train_set:\n",
    "                self.val = self.even_label_occurances(self.val)\n",
    "                self.test = self.even_label_occurances(self.test)\n",
    "\n",
    "        self.train = self.train[np.random.choice(self.train.shape[0], int(len(self.train)*self.subsample_size), replace = False)]\n",
    "        self.val = self.val[np.random.choice(self.val.shape[0], int(len(self.val)*self.subsample_size), replace = False)]\n",
    "        print(\"NOT SUBSAMPLING THE TRUE TEST SET.\")\n",
    "        \n",
    "        # Mapping redundnad samples for time augmentor\n",
    "        self.train = self.map_redundancy(self.train, \"train\")\n",
    "        self.val = self.map_redundancy(self.val, \"validation\")\n",
    "        self.test = self.map_redundancy(self.test, \"test\")\n",
    "        self.full_ds = np.concatenate((self.train, self.val, self.test))\n",
    "        \n",
    "        # Create noise_ds. \n",
    "        # Create zero redundancy column\n",
    "        train_noise = train_noise[np.random.choice(train_noise.shape[0], int(len(train_noise)*self.subsample_size), replace = False)]\n",
    "        zero_column = np.zeros((len(train_noise), 1), dtype = np.int)\n",
    "        self.noise_ds = np.hstack((train_noise, zero_column))\n",
    "        \n",
    "        \n",
    "            \n",
    "    def load_earth_explo_only(self):\n",
    "        noise = self.full_ds[self.full_ds[:,1] == \"noise\"]\n",
    "        \n",
    "        explosions = self.full_ds[self.full_ds[:,1] == \"explosion\"]\n",
    "        earthquakes = self.full_ds[self.full_ds[:,1] == \"earthquake\"]\n",
    "        \n",
    "        \n",
    "        # Unique noise split\n",
    "        train_noise, _ = train_test_split(noise, test_size = 0.2, random_state = self.seed)\n",
    "        \n",
    "        #Unique explosion split\n",
    "        train_explosions, val_test_explosions = train_test_split(explosions, test_size = 0.2, random_state = self.seed)\n",
    "        val_explosions, test_explosions = train_test_split(val_test_explosions, test_size = 0.4, random_state = self.seed)\n",
    "        \n",
    "        #Unique earthquake split\n",
    "        train_earthquakes, val_test_earthquakes = train_test_split(earthquakes, test_size = 0.2, random_state = self.seed)\n",
    "        val_earthquakes, test_earthquakes = train_test_split(val_test_earthquakes, test_size = 0.4, random_state = self.seed)\n",
    "        \n",
    "        # Combining so that events are not duplicated in the splits\n",
    "        self.train = np.concatenate((train_explosions, train_earthquakes))\n",
    "        self.val = np.concatenate((val_explosions, val_earthquakes))\n",
    "        self.test = np.concatenate((test_explosions, test_earthquakes))\n",
    "        \n",
    "        # Up and down sampling \n",
    "        self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        if self.balance_non_train_set:\n",
    "            self.val = self.balance_ds(self.val, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "        else:\n",
    "            # Shuffles the data if not\n",
    "            self.val = self.balance_ds(self.val, False, False, frac_diff = self.frac_diff)\n",
    "            self.test = self.balance_ds(self.test, False, False, frac_diff = self.frac_diff)\n",
    "\n",
    "        if self.even_balance:\n",
    "            self.train = self.even_label_occurances(self.train)\n",
    "            if self.balance_non_train_set:\n",
    "                self.val = self.even_label_occurances(self.val)\n",
    "                self.test = self.even_label_occurances(self.test)\n",
    "\n",
    "        self.train = self.train[np.random.choice(self.train.shape[0], int(len(self.train)*self.subsample_size), replace = False)]\n",
    "        self.val = self.val[np.random.choice(self.val.shape[0], int(len(self.val)*self.subsample_size), replace = False)]\n",
    "        self.test = self.test[np.random.choice(self.test.shape[0], int(len(self.test)*self.subsample_size), replace = False)]\n",
    "\n",
    "        # Mapping redundnad samples for time augmentor\n",
    "        self.train = self.map_redundancy(self.train, \"train\")\n",
    "        self.val = self.map_redundancy(self.val, \"validation\")\n",
    "        self.test = self.map_redundancy(self.test, \"test\")\n",
    "        self.full_ds = np.concatenate((self.train, self.val, self.test))\n",
    "        \n",
    "        # Create noise_ds. \n",
    "        # Create zero redundancy column\n",
    "        train_noise = train_noise[np.random.choice(train_noise.shape[0], int(len(train_noise)*self.subsample_size), replace = False)]\n",
    "        zero_column = np.zeros((len(train_noise), 1), dtype = np.int)\n",
    "        self.noise_ds = np.hstack((train_noise, zero_column))\n",
    "    \n",
    "                \n",
    "    def create_label_dict(self):\n",
    "        # Method which produces the dictionary for labels. This is used in order to disguise labels during training.\n",
    "        if self.earth_explo_only:\n",
    "            self.label_dict = {'explosion' : 0, 'earthquake' : 1}\n",
    "        elif self.noise_earth_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1}\n",
    "        elif self.noise_not_noise:\n",
    "            self.label_dict = { 'noise': 0, 'earthquake' : 1, 'explosion' : 1}\n",
    "        else:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1, 'explosion' : 2, 'induced' : 3}\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        return self.train, self.val, self.test  \n",
    "        \n",
    "    def csv_to_numpy(self, data_csv, csv_folder):\n",
    "        with open(csv_folder + '/' + data_csv) as file:\n",
    "            file_list = np.array(list(file))\n",
    "            dataset = np.empty((len(file_list), 2), dtype=object)\n",
    "            for idx, event in enumerate(file_list):\n",
    "                path, label = event.split(',')\n",
    "                dataset[idx][0] = path.rstrip()\n",
    "                dataset[idx][1] = label.rstrip()\n",
    "            file.close()\n",
    "        return dataset\n",
    "    \n",
    "    def downsample_label(self, target_label, ds, n_samples):\n",
    "        # Method which produces n_samples of the targeted label, and returns the dataset unchanged but with n_samples of the targeted label.\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label], dtype = object)\n",
    "        down_ds = np.array([y for y in ds if y[1] != target_label], dtype = object)\n",
    "        np.random.seed(self.seed)\n",
    "        down_ds = np.concatenate((down_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(down_ds)\n",
    "\n",
    "    def upsample_label(self, target_label, ds, n_samples):\n",
    "        # Seemingly equivalent to downsample_label (?)\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label])\n",
    "        up_ds = [y for y in ds if y[1] != target_label]\n",
    "        np.random.seed(self.seed)\n",
    "        up_ds = np.concatenate((up_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(up_ds)\n",
    "\n",
    "    def frac_diff_n_samples(self, frac_diff, min_counts, max_counts):\n",
    "        # Returns the difference between the most occuring label and the least occuring label, multiplied by some fraction, and added to the number of the least occuring label.\n",
    "        # Potentially a really stupid idea which is now implemented. Can be omitted by setting frac diff = 1 \n",
    "        diff = max_counts - min_counts\n",
    "        n_samples = int(min_counts + diff*frac_diff)\n",
    "        return n_samples\n",
    "\n",
    "    def balance_ds(self, ds, downsample, upsample, frac_diff = 0):\n",
    "        # Downsamples the two most occuring labels, and upsamples the most occuring label.\n",
    "        unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "        nr_classes = len(unique_labels)\n",
    "        if downsample:\n",
    "            # Downsamples by first reducing the largest class, then the second class.\n",
    "            for i in range(nr_classes-1):\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                most_occuring_label = unique_labels[np.where(counts == max(counts))]\n",
    "                n_samples_frac_diff = self.frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "                ds = self.downsample_label(most_occuring_label, ds, n_samples_frac_diff)\n",
    "        if upsample:\n",
    "            if frac_diff != 0:\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                least_occuring_label = unique_labels[np.where(counts == min(counts))]\n",
    "                n_samples_for_balance = max(counts)\n",
    "                ds = self.upsample_label(least_occuring_label, ds, n_samples_for_balance)\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(ds)\n",
    "        return ds\n",
    "\n",
    "    def even_label_occurances(self, ds):\n",
    "        # In cases where labels are disguised as something else, this method will even them out so that the label distribution is even. \n",
    "        num_classes = len(set(self.label_dict.values()))\n",
    "        print(num_classes, len(set(self.label_dict.keys())))\n",
    "        if num_classes != len(set(self.label_dict.keys())):\n",
    "            print(\"Balancing due to disguised labels.\")\n",
    "            print(\"This functions barely works, and is a piece of shit that should not be trusted. Only works because noise has id: 0\")\n",
    "            ids = self.label_dict.values()\n",
    "            most_occuring_id = max(ids)\n",
    "            least_occuring_id = min(ids)\n",
    "            label_count_dict = {}\n",
    "            for label, id in self.label_dict.items():\n",
    "                label_count_dict[label] = len(ds[ds[:,1] == label])\n",
    "            print(label_count_dict)\n",
    "            # Want the labels which share id, to combined have the same number of events of the event with the unique id.\n",
    "            for label, id in self.label_dict.items():\n",
    "                if id == most_occuring_id:\n",
    "                    pure_label_ds = ds[ds[:,1] == label]\n",
    "                    ds_without_label = ds[ds[:,1] != label]\n",
    "                    ds = np.concatenate((ds_without_label, pure_label_ds[np.random.choice(pure_label_ds.shape[0], label_count_dict[label]//num_classes, replace = False)]))\n",
    "        return ds\n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return self.label_dict\n",
    "    \n",
    "    def map_redundancy(self, ds, set_name):\n",
    "        # Creates a redundancy index which distinguishes events which are sampled multiple times.\n",
    "        # Primarily used in timeAugmentation in order to create unique augmentations of otherwise identical events.\n",
    "        # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "        new_column = np.zeros((len(ds), 1), dtype = np.int8)\n",
    "        mapped_ds = np.hstack((ds, new_column))\n",
    "        earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "        unique_earth_paths = set(earth_ds[:,0])\n",
    "        nr_unique_earth_paths = len(unique_earth_paths)\n",
    "        for idx, path in enumerate(unique_earth_paths):\n",
    "            self.progress_bar(idx + 1, nr_unique_earth_paths, f\"Mapping {set_name} redundancy: \")\n",
    "            nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "            label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "            repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "            current_index = 0\n",
    "            if len(repeating_indexes) > 1:\n",
    "                for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                    mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                    mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                    mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                    current_index += 1\n",
    "        print(\"\\n\")\n",
    "        return mapped_ds\n",
    "\n",
    "    def progress_bar(self, current, total, text, barLength = 40):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('%s: [%s%s] %d %%' % (text, arrow, spaces, percent), end='\\r')\n",
    "\n",
    "    def print_data_info(self):\n",
    "        if self.earth_explo_only:\n",
    "            print(\"Loaded explosion and earthquake dataset:\")\n",
    "        if self.noise_not_noise:\n",
    "            print(\"Loaded noise non-noise dataset.\")\n",
    "        if self.even_balance:\n",
    "            print(\"Evenly balanced among classes in the train set.\")\n",
    "        if self.balance_non_train_set:\n",
    "            print(\"As well as non train sets.\")\n",
    "        print(\"Distribution (Label: (counts, proportion)) of\")\n",
    "        print(\"Train ds:\")\n",
    "        labels, counts = np.unique(self.train[:,1], return_counts = True)\n",
    "        print(self.generate_dist_string_EE(labels, counts))\n",
    "        print(\"Val ds:\")\n",
    "        labels, counts = np.unique(self.val[:,1], return_counts = True)\n",
    "        print(self.generate_dist_string_EE(labels, counts))\n",
    "        print(\"Test ds:\")\n",
    "        labels, counts = np.unique(self.test[:,1], return_counts = True)\n",
    "        print(self.generate_dist_string_EE(labels, counts))\n",
    "\n",
    "        \n",
    "        \n",
    "    def generate_dist_string_EE(self, labels, counts):\n",
    "        string = \"\"\n",
    "        for i in range(len(labels)):\n",
    "            string += f\"{labels[i]}: ({counts[i]}, {np.round(counts[i]/np.sum(counts), decimals = 4)})  \"\n",
    "            if i != len(labels) - 1:\n",
    "                string += \"|  \"\n",
    "        return string\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "You are using the true test set.\n",
      "If this is an error, please set use_true_test_set = False and reload the kernel\n",
      "2 3\n",
      "Balancing due to disguised labels.\n",
      "This functions barely works, and is a piece of shit that should not be trusted. Only works because noise has id: 0\n",
      "{'noise': 90099, 'earthquake': 90099, 'explosion': 87386}\n",
      "NO SUBSAMPLING DONE ON THE TRUE TEST SET\n",
      "Mapping train redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping validation redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "Mapping test redundancy: : [--------------------------------------->] 100 %\n",
      "\n",
      "\n",
      "\n",
      "Loaded noise non-noise dataset.\n",
      "Evenly balanced among classes in the train set.\n",
      "Distribution (Label: (counts, proportion)) of\n",
      "Train ds:\n",
      "earthquake: (9111, 0.2547)  |  explosion: (8822, 0.2466)  |  noise: (17835, 0.4986)  \n",
      "Val ds:\n",
      "earthquake: (235, 0.036)  |  explosion: (3140, 0.4811)  |  noise: (3152, 0.4829)  \n",
      "Test ds:\n",
      "earthquake: (698, 0.0396)  |  explosion: (8365, 0.4742)  |  noise: (8579, 0.4863)  \n"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.2,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : True,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154200 16736 11158\n",
      "Total: 154200, earthquake: 77100, explosion: 77100\n",
      "Total: 16736, earthquake: 1328, explosion: 15408\n",
      "Nr noise samples 79499\n",
      "Non noise prop: 1.0\n",
      "Train non noise prop: 1.0\n",
      "Val non noise prop: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))\n",
    "print(f\"Non noise prop: {len(full_ds[full_ds[:,1] != 'noise'])/len(full_ds)}\")\n",
    "print(f\"Train non noise prop: {len(train_ds[train_ds[:,1] != 'noise'])/len(train_ds)}\")\n",
    "print(f\"Val non noise prop: {len(val_ds[val_ds[:,1] != 'noise'])/len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_correct_classes(ds):\n",
    "    for path, label, red in ds:\n",
    "        path_split = path.split('/')\n",
    "        if label == \"earthquake\" or label == \"explosion\":\n",
    "            label = label + \"s\"\n",
    "        assert label in path_split, f\"Incorrect labeling: {label} {path_split}\"\n",
    "\n",
    "assert_correct_classes(full_ds)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Solutions:\n",
    "        # Noise-not-noise:\n",
    "        # Either upsample noise 2 times its current length, or downsample earth and explo so that each contain half of thier current.\n",
    "        # earth-explo:\n",
    "        # non issue with frac diff = 0\n",
    "        # Noise-earth:\n",
    "        # Non issue with frac_diff = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'even_label_occurances' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-44b300333eb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meven_label_occurances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloadData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'even_label_occurances' is not defined"
     ]
    }
   ],
   "source": [
    "new_ds = even_label_occurances(loadData, train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels,counts = np.unique(new_ds[:,1], return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2018-05-26T03.16.35.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2016-03-02T08.15.13.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2003-01-25T06.40.11.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       ..., \n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2005-02-06T20.35.45.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/1999-01-05T12.58.48.000000Z.h5',\n",
       "        'noise', 0.0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2004-04-28T07.47.53.000000Z.h5',\n",
       "        'noise', 0.0]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
