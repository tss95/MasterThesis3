{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "base_dir = 'F:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.Modeling.RandomGridSearchDynamic import RandomGridSearchDynamic\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "\n",
    "helper = HelperFunctions()\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "### Data conditions: ###\n",
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.3,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.25,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "\n",
    "loadData = LoadData(**load_args)\n",
    "\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "dataGen = DataGenerator(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21159 4231 2822\n",
      "Total: 28212, earthquake: 9480, explosion: 9122, noise: 9610\n",
      "Total: 21159, earthquake: 7153, explosion: 6817, noise: 7189\n",
      "Total: 4231, earthquake: 1405, explosion: 1360, noise: 1466\n",
      "Nr noise samples 7189\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(full_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(set(loadData.label_dict.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hyper_grid = {\n",
    "        \"num_channels\" : [1,2,3],\n",
    "        \"num_layers\" : [2,3,4,5,6],\n",
    "        \"batch_size\" : [32, 64, 128, 256],\n",
    "        \"epochs\" : [30],\n",
    "        \"learning_rate\" : [0.1, 0.05, 0.025, 0.005],\n",
    "        \"optimizer\" : [\"sgd\", \"rmsprop\", \"adam\"]\n",
    "    }\n",
    "model_grid = {\n",
    "    \"start_neurons\" : [16, 32, 64, 128, 256, 512, 1024],\n",
    "    \"use_layerwise_dropout_batchnorm\" : [False, True],\n",
    "    \"decay_sequence\" : [[1,2,4,4,2,1], [1,4,8,8,4,1], [1,0.5,0.25,0.25,0.5,1], [1,1,1,1,1,1]],\n",
    "    \"dropout_rate\" : [0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.001, 0],\n",
    "    \"filters\" : [11, 13, 15, 17, 19, 21, 23, 25, 27],\n",
    "    \"kernel_size\" : [3, 7, 15, 21, 33, 41],\n",
    "    \"padding\" : [\"same\"],\n",
    "    \"l2_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"l1_r\" : [0.3, 0.2, 0.1, 0.01, 0.001, 0.0001],\n",
    "    \"activation\" : [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"output_layer_activation\" : [\"sigmoid\"]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_type = \"CNN\"\n",
    "is_lstm = True\n",
    "\n",
    "use_time_augmentor = True\n",
    "use_scaler = True\n",
    "use_noise_augmentor = True\n",
    "detrend = False\n",
    "use_minmax = True\n",
    "use_highpass = False\n",
    "highpass_freq = 0.1\n",
    "\n",
    "n_picks = 30\n",
    "\n",
    "use_tensorboard = False\n",
    "use_liveplots = True\n",
    "use_custom_callback = False\n",
    "use_early_stopping = True\n",
    "start_from_scratch = True\n",
    "\n",
    "randomGridSearch = RandomGridSearchDynamic(loadData, train_ds, val_ds, test_ds, model_type, detrend, use_scaler, use_time_augmentor, \n",
    "                                    use_noise_augmentor, use_minmax, use_highpass, n_picks, hyper_grid = hyper_grid, \n",
    "                                    model_grid = model_grid, use_tensorboard = use_tensorboard,\n",
    "                                    use_liveplots = use_liveplots, use_custom_callback = use_custom_callback,\n",
    "                                    use_early_stopping = use_early_stopping, highpass_freq = highpass_freq,\n",
    "                                    start_from_scratch = start_from_scratch, is_lstm = is_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "    %tensorboard --logdir tensorboard_dir/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time augmentor: [--------------->    ] 80 %\r"
     ]
    }
   ],
   "source": [
    "results_df, min_loss, max_accuracy, max_precision, max_recall = randomGridSearch.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
