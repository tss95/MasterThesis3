{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "administrative-three",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import pylab as pl\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\" \n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.TimeAugmentor import TimeAugmentor\n",
    "from Classes.DataProcessing.NoiseAugmentor import NoiseAugmentor\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.DataProcessing.RamGenerator import RamGenerator\n",
    "from Classes.Modeling.InceptionTimeModel import InceptionTimeModel\n",
    "from Classes.Modeling.InceptionTimeRGS import InceptionTimeRGS\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Modeling.ResultFitter import ResultFitter\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json\n",
    "#from Classes import Tf_shutup\n",
    "#Tf_shutup.Tf_shutupe()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]= (15,15)\n",
    "helper = HelperFunctions()\n",
    "\n",
    "import sys\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "\n",
    "# Equivalent to the two lines above\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dynamic-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.4,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "\n",
    "if load_args['earth_explo_only']:\n",
    "    full_and_noise_ds = np.concatenate((full_ds, noise_ds))\n",
    "    timeAug = TimeAugmentor(handler, full_and_noise_ds, seed = load_args['seed'])\n",
    "else:\n",
    "    timeAug = TimeAugmentor(handler, full_ds, seed = load_args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funny-ferry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2020-05-30T21.47.41.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2001-05-13T02.24.59.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2020-02-21T02.49.34.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       ..., \n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2010-07-22T00.23.43.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2012-05-12T11.04.23.000000Z.h5',\n",
       "        'noise', 0],\n",
       "       [ '/media/tord/T7/Thesis_ssd/norsar_data_nov/noise/2012-05-10T02.53.24.000000Z.h5',\n",
       "        'noise', 0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "athletic-chaos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62648 12529 8354\n",
      "Total: 83531, earthquake: 42458, explosion: 41073\n",
      "Total: 62648, earthquake: 31903, explosion: 30745\n",
      "Total: 12529, earthquake: 6380, explosion: 6149\n",
      "Nr noise samples 42391\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds), len(val_ds), len(test_ds))\n",
    "classes, counts = handler.get_class_distribution_from_ds(full_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(train_ds)\n",
    "classes, counts = handler.get_class_distribution_from_ds(val_ds)\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "incoming-pavilion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note that highpass and detrend have not been implemented yet in this class.\n"
     ]
    }
   ],
   "source": [
    "hyper_grid = {    \n",
    "    \"batch_size\" : [128, 256, 512, 1024],\n",
    "    \"epochs\" : [100, 100],\n",
    "    \"learning_rate\" : [0.1, 0.01, 0.001, 0.0001],\n",
    "    \"optimizer\" : [\"adam\", \"rmsprop\", \"sgd\"]\n",
    "    }\n",
    "model_grid = {\n",
    "    \"use_residuals\" : [True, False],\n",
    "    \"use_bottleneck\" : [True, False],\n",
    "    \"nr_modules\" : [1, 3, 6, 9, 12],\n",
    "    \"kernel_size\" : [20, 30, 40, 50],\n",
    "    \"bottleneck_size\" : [ 26, 28, 30, 32, 34, 36],\n",
    "    \"num_filters\" : [24, 26, 28, 30, 32, 34, 36, 38, 40 ,42],\n",
    "    \"shortcut_activation\" : [\"relu\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"module_activation\" : [\"linear\", \"relu\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"module_output_activation\" : [\"relu\", \"linear\", \"sigmoid\", \"softmax\", \"tanh\"],\n",
    "    \"output_activation\": [\"sigmoid\"],\n",
    "    \"reg_shortcut\": [True, False],\n",
    "    \"reg_module\" : [True, False],\n",
    "    \"l1_r\" : [0.1, 0.001, 0.0001, 0],\n",
    "    \"l2_r\" : [0.1, 0.001, 0.0001, 0]\n",
    "}\n",
    "\n",
    "num_channels = 3\n",
    "\n",
    "use_time_augmentor = True\n",
    "use_scaler = True\n",
    "use_noise_augmentor = True\n",
    "detrend = False\n",
    "use_minmax = False\n",
    "use_highpass = True\n",
    "highpass_freq = 0.1\n",
    "\n",
    "n_picks = 50\n",
    "\n",
    "use_tensorboard = False\n",
    "use_liveplots = False\n",
    "use_custom_callback = False\n",
    "use_early_stopping = True\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = True\n",
    "\n",
    "randomGridSearch = InceptionTimeRGS(loadData, train_ds, val_ds, detrend, use_scaler, use_time_augmentor, \n",
    "                                    use_noise_augmentor, use_minmax, use_highpass, n_picks, hyper_grid = hyper_grid, \n",
    "                                    model_grid = model_grid, use_tensorboard = use_tensorboard,\n",
    "                                    use_liveplots = use_liveplots, use_custom_callback = use_custom_callback,\n",
    "                                    use_early_stopping = use_early_stopping, highpass_freq = highpass_freq,\n",
    "                                    start_from_scratch = start_from_scratch, use_reduced_lr = use_reduced_lr, \n",
    "                                    num_channels = num_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stainless-composer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clear_tensorboard_dir():\n",
    "    import os\n",
    "    import shutil\n",
    "    path = f\"{base_dir}/Tensorboard_dir/fit\"\n",
    "    files = os.listdir(path)\n",
    "    print(files)\n",
    "    for f in files:\n",
    "        shutil.rmtree(os.path.join(path,f))\n",
    "if use_tensorboard:\n",
    "    clear_tensorboard_dir()\n",
    "    %tensorboard --logdir tensorboard_dir/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accurate-authority",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of grid: 96\n",
      "Length of grid: 30720000\n",
      "Fit process completed after 46.224916219711304 seconds. Total datapoints fitted: 125922.\n",
      "Average time per datapoint: 0.0003670916616612769\n",
      "Finished fitting augmentors and scaler\n",
      "Starting loading to RAM\n",
      "Completed loading to RAM\n",
      "Starting loading to RAM\n",
      "Completed loading to RAM\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mixed_precision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ff19472083c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomGridSearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/tord/T7/Thesis_ssd/MasterThesis3.0/Classes/Modeling/InceptionTimeRGS.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyper_picks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mmixed_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_global_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mixed_float16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mmodel_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model_nr_type\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_nr_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"index\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model nr {i + 1} of {len(self.hyper_picks)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mixed_precision' is not defined"
     ]
    }
   ],
   "source": [
    "results_df, min_loss, max_accuracy, max_precision, max_recall = randomGridSearch.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
