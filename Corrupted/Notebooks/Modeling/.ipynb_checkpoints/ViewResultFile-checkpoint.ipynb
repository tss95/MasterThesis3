{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "\n",
    "base_dir = 'C:\\Documents\\Thesis_ssd\\MasterThesis'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.BaselineHelperFunctions import BaselineHelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.DataGenerator import DataGenerator\n",
    "from Classes.Modeling.Models import Models\n",
    "from Classes.Modeling.RandomGridSearch import RandomGridSearch\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.Modeling.CustomCallback import CustomCallback\n",
    "from Classes.Scaling.ScalerFitter import ScalerFitter\n",
    "from Classes.Scaling.MinMaxScalerFitter import MinMaxScalerFitter\n",
    "from Classes.Scaling.StandardScalerFitter import StandardScalerFitter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = 'results_8_test_earlyS_highpass-0.1.csv'\n",
    "df = GridSearchResultProcessor().get_results_df_by_name(result_file_name, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_from_csv_and_index(self, file_name, index, num_classes, use_tensorboard = False, use_liveplots = False, use_custom_callbacks = False):\n",
    "    \n",
    "    # Major parameter parser\n",
    "    df = GridSearchResultProcessor().get_results_df_by_name(file_name, num_classes)\n",
    "    model_nr, detrend, use_scaler, use_minmax, use_noise_augmentor, use_early_stopping, use_highpass, highpass_freq = parse_result_name(file_name)\n",
    "    \n",
    "    if use_scaler:\n",
    "            if use_minmax:\n",
    "                scaler = MinMaxScalerFitter(self.train_ds).fit_scaler(test = False, detrend = detrend)\n",
    "            else:\n",
    "                scaler = StandardScalerFitter(self.train_ds).fit_scaler(test = False, detrend = detrend)\n",
    "        else:\n",
    "            scaler = None\n",
    "    if use_noise_augmentor:\n",
    "        augmentor = NoiseAugmentor(self.train_ds, use_scaler, scaler)\n",
    "    else:\n",
    "        augmentor = None\n",
    "    \n",
    "    values = list(df.iloc[index][0:13])\n",
    "    keys = list(df.columns[0:13])\n",
    "    params = {keys[i]: values[i] for i in range(len(keys))}\n",
    "    \n",
    "    build_model_args = self.helper.generate_build_model_args(model_nr, int(params['batch_size']), \n",
    "                                                             float(params['dropout_rate']), params['activation'], \n",
    "                                                             params['output_layer_activation'], float(params['l2_r']), \n",
    "                                                             float(params['l1_r']), int(params['start_neurons']),\n",
    "                                                             int(params['filters']), int(params['kernel_size']),\n",
    "                                                             params['padding'], num_classes)\n",
    "    # Build model using args generated above\n",
    "    model = Models(**build_model_args).model\n",
    "\n",
    "    # Generate generator args using picks.\n",
    "    gen_args = self.helper.generate_gen_args(int(params['batch_size']), False, self.detrend, \n",
    "                                             use_scaler = use_scaler, scaler = scaler, \n",
    "                                             use_noise_augmentor = use_noise_augmentor, \n",
    "                                             augmentor = augmentor, num_classes = num_classes)\n",
    "\n",
    "    # Initiate generators using the args\n",
    "    train_gen = self.data_gen.data_generator(self.train_ds, **gen_args)\n",
    "    val_gen = self.data_gen.data_generator(self.val_ds, **gen_args)\n",
    "    test_gen = self.data_gen.data_generator(self.test_ds, **gen_args)\n",
    "\n",
    "    # Generate compiler args using picks\n",
    "    opt = self.getOptimizer(params['optimizer'], float(params['learning_rate']))\n",
    "    model_compile_args = self.helper.generate_model_compile_args(opt, num_classes)\n",
    "    # Compile model using generated args\n",
    "    model.compile(**model_compile_args)\n",
    "\n",
    "    # Generate fit args using picks.\n",
    "    fit_args = self.helper.generate_fit_args(self.train_ds, self.val_ds, int(params['batch_size']), False, \n",
    "                                             int(params['epochs']), test_gen, use_tensorboard = use_tensorboard, \n",
    "                                             use_liveplots = use_liveplots, \n",
    "                                             use_custom_callback = use_custom_callback,\n",
    "                                             use_early_stopping = use_early_stopping)\n",
    "    # Fit the model using the generated args\n",
    "    model_fit = model.fit(train_gen, **fit_args)\n",
    "\n",
    "    self.helper.plot_confusion_matrix(model, test_gen, self.test_ds, int(params['batch_size']), num_classes)\n",
    "\n",
    "    # Evaluate the fitted model on the test set\n",
    "    loss, accuracy, precision, recall = model.evaluate_generator(generator=test_gen,\n",
    "                                                               steps=self.helper.get_steps_per_epoch(self.test_ds, \n",
    "                                                                                                     int(params['batch_size']), \n",
    "                                                                                                     False))\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    print(f'Test loss: {loss}')\n",
    "    print(f'Test accuracy: {accuracy}')\n",
    "    print(f'Test precision: {precision}')\n",
    "    print(f'Test recall: {recall}')\n",
    "    return model    \n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "def parse_result_name(self, file_name):\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    major_params = file_name.split('_')[1:]\n",
    "    model_nr = major_params[0]\n",
    "    del major_params[0]\n",
    "    \n",
    "    use_scaler = 'sscale' in major_params\n",
    "    use_noise_augmentor = 'noiseAug' in major_params\n",
    "    detrend = 'detrend' in major_params\n",
    "    use_minmax = 'mmscale' in major_params\n",
    "    use_early_stopping = 'earlyS' in major_params\n",
    "    use_highpass = False\n",
    "    highpass_freq = 0.1\n",
    "    for word in major_params:\n",
    "        if len(word.split('-')) == 2:\n",
    "            use_highpass = True\n",
    "            highpass_freq = float(word.split('-')[1])\n",
    "            \n",
    "    return model_nr, detrend, use_scaler, use_minmax, use_noise_augmentor, use_early_stopping, use_highpass, highpass_freq\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['test', 'earlyS', 'highpass-0.1'], True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_from_csv_and_index(result_file_name, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = fit_from_csv_and_index(result_file_name, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('highpass', '0')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.match(r\"([a-z]+)([0-9]+)\", 'highpass0.1', re.I)\n",
    "if match:\n",
    "    items = match.groups()\n",
    "print(items) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchResultProcessor' object has no attribute 'full_ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-cbd85c27eaa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mGridSearchResultProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_ds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchResultProcessor' object has no attribute 'full_ds'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
