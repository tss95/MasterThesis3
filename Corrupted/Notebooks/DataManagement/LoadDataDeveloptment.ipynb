{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "base_dir = 'F:\\Thesis_ssd\\MasterThesis3.0'\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class LoadData():\n",
    "    \n",
    "    def __init__(self, earth_explo_only = False, noise_earth_only = False, noise_not_noise = False, \n",
    "                 downsample = False, upsample = False, frac_diff = 1, seed = None, subsample_size = 1,\n",
    "                 balance_non_train_set = False, use_true_test_set = False):\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.earth_explo_only = earth_explo_only\n",
    "        self.noise_earth_only = noise_earth_only\n",
    "        self.noise_not_noise = noise_not_noise\n",
    "        self.downsample = downsample\n",
    "        self.upsample = upsample\n",
    "        self.frac_diff = frac_diff\n",
    "        self.subsample_size = subsample_size\n",
    "        self.balance_non_train_set = balance_non_train_set\n",
    "        self.use_true_test_set = use_true_test_set\n",
    "        \n",
    "        \n",
    "        self.csv_folder = os.path.join('F:\\\\', 'Thesis_ssd','MasterThesis3.0','csv_folder')\n",
    "        self.data_csv_name = 'full_no_test.csv'\n",
    "        self.test_csv_name = 'DO_NOT_TOUCH_test_set.csv'\n",
    "        self.full_ds = self.csv_to_numpy(self.data_csv_name, self.csv_folder)\n",
    "        \n",
    "        self.create_label_dict()\n",
    "        self.load_data()\n",
    "        if self.use_true_test_set:\n",
    "            self.true_test_ds = self.csv_to_numpy(self.test_csv_name, self.csv_folder)\n",
    "            print(\"WARNING!\")\n",
    "            print(\"You are using the true test set.\")\n",
    "            print(\"THIS SHOULD ONLY BE USED ONCE\")\n",
    "            print(\"If this is an error, please set use_true_test_set = False and reload the kernel\")\n",
    "            \n",
    "        if sum([self.earth_explo_only, self.noise_earth_only, self.noise_not_noise]) > 1:\n",
    "            raise Exception(\"Invalid load data arguments.\")\n",
    "\n",
    "    def load_data(self):\n",
    "        if not self.use_true_test_set:\n",
    "            if self.balance_non_train_set:\n",
    "                self.full_ds = self.balance_ds(self.full_ds, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "                self.full_ds = self.full_ds[0:int(len(self.full_ds)*self.subsample_size)]\n",
    "                self.refine_full_ds()\n",
    "                self.train, val_test = train_test_split(self.full_ds, test_size = 0.15, random_state = self.seed)\n",
    "                self.val, self.test = train_test_split(val_test, test_size = 0.5, random_state = self.seed)\n",
    "                if not self.earth_explo_only:\n",
    "                    self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "            else:\n",
    "                self.full_ds = self.balance_ds(self.full_ds, False, False, frac_diff = 1)\n",
    "                self.full_ds = self.full_ds[0:int(len(self.full_ds)*self.subsample_size)]\n",
    "                if self.earth_explo_only or self.noise_earth_only:\n",
    "                    if self.earth_explo_only:\n",
    "                        self.noise_ds = np.array(self.full_ds[self.full_ds[:,1] == \"noise\"])\n",
    "                        self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"noise\"])\n",
    "                        # The noise needs to be reduced in order to work properly in noise augmentor\n",
    "                        self.noise_ds, _ = train_test_split(self.noise_ds, test_size = 0.15, random_state = self.seed)\n",
    "                        zero_column = np.zeros((len(self.noise_ds), 1))\n",
    "                        self.noise_ds = np.hstack((self.noise_ds, zero_column))\n",
    "                    else:\n",
    "                        self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"explosion\"])\n",
    "                self.train, val_test = train_test_split(self.full_ds, test_size = 0.15, random_state = self.seed)\n",
    "                self.val, self.test = train_test_split(val_test, test_size = 0.5, random_state = self.seed)\n",
    "                self.train = self.balance_ds(self.train, self.downsample, self.upsample, frac_diff = self.frac_diff)\n",
    "                if self.upsample:\n",
    "                    self.train = self.map_redundancy(self.train)\n",
    "                else:\n",
    "                    zero_column = np.zeros((len(self.train), 1))\n",
    "                    self.train = np.hstack((self.train, zero_column))\n",
    "                zero_val = np.zeros((len(self.val), 1))\n",
    "                zero_test = np.zeros((len(self.test), 1))\n",
    "                self.val = np.hstack((self.val, zero_val))\n",
    "                self.test = np.hstack((self.test, zero_test))\n",
    "                self.full_ds = np.concatenate((self.train, self.val))\n",
    "                self.full_ds = np.concatenate((self.full_ds, self.test))\n",
    "                if not self.earth_explo_only:\n",
    "                    self.noise_ds = self.train[self.train[:,1] == \"noise\"]\n",
    "        else:\n",
    "            print(\"Write this code when you are ready to use the test set.\")\n",
    "            raise Exception(\"The code has not yet been written for the true test set.\")\n",
    "            \n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "    def refine_full_ds(self):\n",
    "        if self.earth_explo_only or self.noise_earth_only:\n",
    "            if self.earth_explo_only:\n",
    "                self.noise_ds = np.array(self.full_ds[self.full_ds[:,1] == \"noise\"])\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"noise\"])\n",
    "            if self.noise_earth_only:\n",
    "                self.full_ds = np.array(self.full_ds[self.full_ds[:,1] != \"explosion\"])\n",
    "        if self.earth_explo_only and self.noise_earth_only:\n",
    "            raise Exception(\"Cannot have both earth_explo_only = True and noise_earth_only = True\")\n",
    "        # Only need to map redundency if upsampling, as upsampling is the cause of redundancy\n",
    "        if self.upsample:\n",
    "            self.full_ds = self.map_redundancy(self.full_ds)\n",
    "        else:\n",
    "            zero_column = np.zeros((len(self.full_ds), 1))\n",
    "            self.full_ds = np.hstack((self.full_ds, zero_column))\n",
    "    \n",
    "    def create_label_dict(self):\n",
    "        if self.earth_explo_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'explosion' : 1}\n",
    "        elif self.noise_earth_only:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1}\n",
    "        elif self.noise_not_noise:\n",
    "            self.label_dict = {'earthquake' : 0, 'explosion' : 0, 'noise': 1}\n",
    "        else:\n",
    "            self.label_dict = {'earthquake' : 0, 'noise' : 1, 'explosion' : 2, 'induced' : 3}\n",
    "    \n",
    "    def get_datasets(self):\n",
    "        return self.full_ds, self.train, self.val, self.test  \n",
    "        \n",
    "    def csv_to_numpy(self, data_csv, csv_folder):\n",
    "        with open(csv_folder + '/' + data_csv) as file:\n",
    "            file_list = np.array(list(file))\n",
    "            dataset = np.empty((len(file_list), 2), dtype=object)\n",
    "            for idx, event in enumerate(file_list):\n",
    "                path, label = event.split(',')\n",
    "                dataset[idx][0] = path.rstrip()\n",
    "                dataset[idx][1] = label.rstrip()\n",
    "            file.close()\n",
    "        return dataset\n",
    "    \n",
    "    def downsample_label(self, target_label, ds, n_samples):\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label], dtype = object)\n",
    "        down_ds = np.array([y for y in ds if y[1] != target_label], dtype = object)\n",
    "        np.random.seed(self.seed)\n",
    "        down_ds = np.concatenate((down_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(down_ds)\n",
    "\n",
    "    def upsample_label(self, target_label, ds, n_samples):\n",
    "        target_array = np.array([x for x in ds if x[1] == target_label])\n",
    "        up_ds = [y for y in ds if y[1] != target_label]\n",
    "        np.random.seed(self.seed)\n",
    "        up_ds = np.concatenate((up_ds, target_array[np.random.choice(target_array.shape[0], n_samples, replace = True)]))\n",
    "        return np.array(up_ds)\n",
    "\n",
    "    def frac_diff_n_samples(self, frac_diff, min_counts, max_counts):\n",
    "        diff = max_counts - min_counts\n",
    "        n_samples = int(min_counts + diff*frac_diff)\n",
    "        return n_samples\n",
    "\n",
    "    def balance_ds(self, ds, downsample, upsample, frac_diff = 0):\n",
    "        unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "        nr_classes = len(unique_labels)\n",
    "        if downsample:\n",
    "            # Downsamples by first reducing the largest class, then the second class.\n",
    "            for i in range(nr_classes-1):\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                most_occuring_label = unique_labels[np.where(counts == max(counts))]\n",
    "                n_samples_frac_diff = self.frac_diff_n_samples(frac_diff, min(counts), max(counts))\n",
    "                ds = self.downsample_label(most_occuring_label, ds, n_samples_frac_diff)\n",
    "        if upsample:\n",
    "            if frac_diff != 0:\n",
    "                unique_labels, counts = np.unique(ds[:,1], return_counts = True)\n",
    "                least_occuring_label = unique_labels[np.where(counts == min(counts))]\n",
    "                n_samples_for_balance = max(counts)\n",
    "                ds = self.upsample_label(least_occuring_label, ds, n_samples_for_balance)\n",
    "        np.random.seed(self.seed)\n",
    "        np.random.shuffle(ds)\n",
    "        return ds\n",
    "    \n",
    "    \n",
    "    def get_label_dict(self):\n",
    "        return self.label_dict\n",
    "    \n",
    "    def map_redundancy(self, ds):\n",
    "        # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "        new_column = np.zeros((len(ds), 1), dtype = int)\n",
    "        mapped_ds = np.hstack((ds, new_column))\n",
    "        earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "        unique_earth_paths = set(earth_ds[:,0])\n",
    "        nr_unique_earth_paths = len(unique_earth_paths)\n",
    "        for idx, path in enumerate(unique_earth_paths):\n",
    "            self.progress_bar(idx + 1, nr_unique_earth_paths)\n",
    "            nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "            label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "            repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "            current_index = 0\n",
    "            if len(repeating_indexes) > 1:\n",
    "                for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                    mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                    mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                    mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                    current_index += 1\n",
    "        return mapped_ds\n",
    "\n",
    "    def progress_bar(self, current, total, barLength = 40):\n",
    "        percent = float(current) * 100 / total\n",
    "        arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "        spaces  = ' ' * (barLength - len(arrow))\n",
    "        print('Mapping redundancy: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : False,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : True,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 0.8,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.1,\n",
    "    'balance_non_train_set' : False,\n",
    "    'use_true_test_set' : False\n",
    "}\n",
    "\n",
    "\n",
    "loadData = LoadData(**load_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0}\n"
     ]
    }
   ],
   "source": [
    "print(set(loadData.val[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25198 21934 1632 1632\n",
      "(array(['earthquake', 'explosion', 'noise'], dtype=object), array([7454, 7026, 7454], dtype=int64))\n",
      "(array(['earthquake', 'explosion', 'noise'], dtype=object), array([ 60, 761, 811], dtype=int64))\n",
      "(array(['earthquake', 'explosion', 'noise'], dtype=object), array([ 55, 778, 799], dtype=int64))\n",
      "Nr noise samples 7454\n"
     ]
    }
   ],
   "source": [
    "full, train, val, test = loadData.get_datasets()\n",
    "noise = loadData.noise_ds\n",
    "print(len(full), len(train), len(val), len(test))\n",
    "print(np.unique(train[:,1], return_counts = True))\n",
    "print(np.unique(val[:,1], return_counts = True))\n",
    "print(np.unique(test[:,1], return_counts = True))\n",
    "print(\"Nr noise samples \" + str(len(loadData.noise_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise == train[train[:,1] == \"noise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[loadData.label_dict.get(x) for x in train[:,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds, train_ds, val_ds, test_ds = loadData.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_redundancy(ds):\n",
    "    # This only works if we are upsampling EARTHQUAKES (NOTHING ELSE)!\n",
    "    new_column = np.zeros((len(ds), 1), dtype = int)\n",
    "    mapped_ds = np.hstack((ds, new_column))\n",
    "    earth_ds = ds[ds[:,1] == \"earthquake\"]\n",
    "    unique_earth_paths = set(earth_ds[:,0])\n",
    "    nr_unique_earth_paths = len(unique_earth_paths)\n",
    "    for idx, path in enumerate(unique_earth_paths):\n",
    "        progress_bar(idx + 1, nr_unique_earth_paths)\n",
    "        nr_repeats = len(earth_ds[earth_ds[:,0] == path])\n",
    "        label = earth_ds[earth_ds[:,0] == path][0][1]\n",
    "        repeating_indexes = np.where(ds[ds[:,0] == path][:,0][0] == ds[:,0])[0]\n",
    "        current_index = 0\n",
    "        if len(repeating_indexes) > 1:\n",
    "            for event in earth_ds[earth_ds[:,0] == path]:\n",
    "                mapped_ds[repeating_indexes[current_index]][0] = path\n",
    "                mapped_ds[repeating_indexes[current_index]][1] = label\n",
    "                mapped_ds[repeating_indexes[current_index]][2] = current_index\n",
    "                current_index += 1\n",
    "    return mapped_ds\n",
    "\n",
    "def progress_bar(current, total, barLength = 40):\n",
    "    percent = float(current) * 100 / total\n",
    "    arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "    spaces  = ' ' * (barLength - len(arrow))\n",
    "    print('Mapping redundancy: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping redundancy: [--------------------------------------->] 100 %\r"
     ]
    }
   ],
   "source": [
    "redundancy_ds = map_redundancy(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2003-08-13T22.26.18.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2017-03-14T22.58.39.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2012-03-02T06.10.23.000000Z.h5',\n",
       "        'noise'],\n",
       "       ...,\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2002-08-17T17.39.15.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2015-07-21T23.44.41.000000Z.h5',\n",
       "        'noise'],\n",
       "       ['F:/Thesis_ssd\\\\norsar_data_nov\\\\noise\\\\2003-09-07T15.57.12.000000Z.h5',\n",
       "        'noise']], dtype=object)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadData.noise_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 3], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "ds = np.array([['a', 1],['a',1],['b',0],['a',1],['c',3],['b',0]])\n",
    "print(np.where(ds[ds[:,0] == 'a'][:,0][0] == ds[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'a'], dtype='<U1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[ds[:,0] == 'a'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'a', 'b', 'a', 'c', 'b'], dtype='<U1')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
