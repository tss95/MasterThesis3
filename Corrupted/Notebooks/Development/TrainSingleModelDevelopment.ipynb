{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fuzzy-seeking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tord/miniconda3/envs/thesis/lib/python3.8/site-packages/tensorflow/python/util/module_wrapper.py:49: The name tf.keras.layers.CuDNNLSTM is deprecated. Please use tf.compat.v1.keras.layers.CuDNNLSTM instead.\n",
      "\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: GeForce RTX 3090, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import gc\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import GeneratorEnqueuer\n",
    "\n",
    "import os\n",
    "base_dir = '/media/tord/T7/Thesis_ssd/MasterThesis3'\n",
    "os.chdir(base_dir)\n",
    "\n",
    "from Classes.Modeling.DynamicModels import DynamicModels\n",
    "from Classes.Modeling.StaticModels import StaticModels\n",
    "from Classes.DataProcessing.LoadData import LoadData\n",
    "from Classes.DataProcessing.HelperFunctions import HelperFunctions\n",
    "from Classes.DataProcessing.DataHandler import DataHandler\n",
    "from Classes.DataProcessing.RamLoader import RamLoader\n",
    "from Classes.Modeling.GridSearchResultProcessor import GridSearchResultProcessor\n",
    "from Classes.DataProcessing.ts_RamGenerator import data_generator\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "import random\n",
    "import pprint\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "christian-appearance",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-25b33e2560aa>, line 104)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-25b33e2560aa>\"\u001b[0;36m, line \u001b[0;32m104\u001b[0m\n\u001b[0;31m    def run(self, workers, max_queue_size, **p, evaluate_train = False, evaluate_val = False, evaluate_test = False):\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TrainSingleModel(GridSearchResultProcessor):\n",
    "    \n",
    "    def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test, noiseAug, helper, loadData, \n",
    "                 model_type, num_channels, use_tensorboard, use_liveplots, use_custom_callback,\n",
    "                 use_early_stopping, use_reduced_lr, log_data = True, results_df = None, \n",
    "                 results_file_name = None, index = None):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.noiseAug = noiseAug\n",
    "        self.helper = helper\n",
    "        self.loadData = loadData\n",
    "        \n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        self.use_liveplots = use_liveplots\n",
    "        self.use_custom_callback = use_custom_callback\n",
    "        self.use_early_stopping = use_early_stopping\n",
    "        self.use_reduced_lr = use_reduced_lr\n",
    "        \n",
    "        self.num_classes = len(set(self.loadData.label_dict.values()))\n",
    "        \n",
    "        self.model_type = model_type\n",
    "        self.results_df = results_df\n",
    "        self.log_data = log_data\n",
    "        \n",
    "        self.index = index\n",
    "        \n",
    "    def create_and_compile_model(self, **p):\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "        \n",
    "        epoch = p[\"epochs\"]\n",
    "        batch_size = p[\"batch_size\"]\n",
    "        opt = self.helper.get_optimizer(p[\"optimizer\"], p[\"learning_rate\"])\n",
    "        \n",
    "        p = self.helper.handle_hyperparams(self.num_classes, **p)\n",
    "        \n",
    "        if self.index != None:\n",
    "            model_info = {\"model_type\" : self.model_type, \"index\" : self.index}\n",
    "        else:\n",
    "            model_info = {\"model_type\" : self.model_type}\n",
    "        current_picks = [model_info, p]\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(current_picks)\n",
    "        if self.log_data and self.results_df != None and self.results_file_name != None:\n",
    "            self.results_df = self.store_params_before_fit_opti(p, self.results_df, self.results_file_name)\n",
    "        \n",
    "        _,_,timesteps = self.x_train.shape\n",
    "        input_shape = (timesteps, self.num_channels)\n",
    "        \n",
    "        model = DynamicModels(self.model_type, self.num_classes, input_shape, **p).model\n",
    "        model_compile_args = self.helper.generate_model_compile_args(opt, self.num_classes)\n",
    "        model.compile(**model_compile_args)\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def create_enqueuer(self, X, y, batch_size, noiseAug, num_channels):\n",
    "        enq = GeneratorEnqueuer(data_generator(X, y, batch_size, noiseAug, num_channels = num_channels, is_lstm  = True), \n",
    "                                use_multiprocessing = False)\n",
    "        return enq\n",
    "        \n",
    "    def fit_model(self, model, workers, max_queue_size, **p):\n",
    "        train_enq = self.create_enqueuer(self.x_train, self.y_train, p[\"batch_size\"], self.noiseAug, self.num_channels)\n",
    "        val_enq = self.create_enqueuer(self.x_val, self.y_val, p[\"batch_size\"], self.noiseAug, self.num_channels)\n",
    "        train_enq.start(workers = workers, max_queue_size = max_queue_size)\n",
    "        val_enq.start(workers = workers, max_queue_size = max_queue_size)\n",
    "        train_gen = train_enq.get()\n",
    "        val_gen = val_enq.get()\n",
    "        \n",
    "        fit_args = self.helper.generate_fit_args(self.loadData.train, self.loadData.val, self.loadData,\n",
    "                                                 p[\"batch_size\"], p[\"epochs\"], val_gen,\n",
    "                                                 use_tensorboard = self.use_tensorboard, \n",
    "                                                 use_liveplots = self.use_liveplots, \n",
    "                                                 use_custom_callback = self.use_custom_callback,\n",
    "                                                 use_early_stopping = self.use_early_stopping,\n",
    "                                                 use_reduced_lr = self.use_reduced_lr)\n",
    "        try:\n",
    "            print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.val, p['batch_size'])*p['batch_size']}/{len(self.loadData.val)} validation points\")\n",
    "            print(f\"Utilizes {self.helper.get_steps_per_epoch(self.loadData.train, p['batch_size'])*p['batch_size']}/{len(self.loadData.train)} training points\")\n",
    "            print(\"---------------------------------------------------------------------------------\")\n",
    "\n",
    "            # Fit the model using the generated args\n",
    "            model.fit(train_gen, **fit_args)\n",
    "            train_enq.stop()\n",
    "            val_enq.stop()\n",
    "            del train_gen, val_gen, train_enq, val_enq\n",
    "\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            print(\"Something went wrong.\")\n",
    "        return model\n",
    "    \n",
    "    def run(self, workers, max_queue_size, **p, evaluate_train = False, evaluate_val = False, evaluate_test = False):\n",
    "        model = self.create_and_compile_model(**p)\n",
    "        model = self.fit_model(model, 16, 15, **p)\n",
    "        self.model = model\n",
    "        if evaluate_train:\n",
    "            self.helper.evaluate_model(model, self.x_train, self.y_train, self.loadData.label_dict, self.num_channels, self.noiseAug)\n",
    "        if evaluate_val:\n",
    "            self.helper.evaluate_model(model, self.x_val, self.y_val, self.loadData.label_dict, self.num_channels, self.noiseAug)\n",
    "        if evaluate_test:\n",
    "            self.helper.evaluate_model(model, self.x_test, self.y_test, self.loadData.label_dict, self.num_channels, self.noiseAug)\n",
    "        \n",
    "        return model\n",
    "\n",
    "        \n",
    "            \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_args = {\n",
    "    'earth_explo_only' : True,\n",
    "    'noise_earth_only' : False,\n",
    "    'noise_not_noise' : False,\n",
    "    'downsample' : True,\n",
    "    'upsample' : True,\n",
    "    'frac_diff' : 1,\n",
    "    'seed' : 1,\n",
    "    'subsample_size' : 0.05,\n",
    "    'balance_non_train_set' : True,\n",
    "    'use_true_test_set' : False,\n",
    "    'even_balance' : True\n",
    "}\n",
    "loadData = LoadData(**load_args)\n",
    "train_ds, val_ds, test_ds = loadData.get_datasets()\n",
    "noise_ds = loadData.noise_ds\n",
    "handler = DataHandler(loadData)\n",
    "helper = HelperFunctions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-findings",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_time_augmentor = True\n",
    "use_noise_augmentor = True\n",
    "scaler_name = \"minmax\"\n",
    "filter_name = None\n",
    "band_min = 2\n",
    "band_max = 4\n",
    "highpass_freq = 5\n",
    "\n",
    "use_tensorboard = True\n",
    "use_liveplots = False\n",
    "use_custom_callback = True\n",
    "use_early_stopping = True\n",
    "start_from_scratch = False\n",
    "use_reduced_lr = True\n",
    "log_data = False\n",
    "\n",
    "load_test_set = True\n",
    "\n",
    "\n",
    "ramLoader = RamLoader(loadData, \n",
    "                      handler, \n",
    "                      use_time_augmentor = use_time_augmentor, \n",
    "                      use_noise_augmentor = use_noise_augmentor, \n",
    "                      scaler_name = scaler_name,\n",
    "                      filter_name = filter_name, \n",
    "                      band_min = band_min,\n",
    "                      band_max = band_max,\n",
    "                      highpass_freq = highpass_freq, \n",
    "                      load_test_set = load_test_set)\n",
    "x_train, y_train, x_val, y_val, x_test, y_test, noiseAug = ramLoader.load_to_ram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"CNN_grow\"\n",
    "num_channels = 3\n",
    "\n",
    "singleModel = TrainSingleModel(x_train, y_train, x_val, y_val, x_test, y_test, \n",
    "                               noiseAug, helper, loadData, \n",
    "                               model_type, \n",
    "                               num_channels, \n",
    "                               use_tensorboard, \n",
    "                               use_liveplots, \n",
    "                               use_custom_callback,\n",
    "                               use_early_stopping, \n",
    "                               use_reduced_lr, \n",
    "                               log_data = log_data, \n",
    "                               results_df = None, \n",
    "                               results_file_name = None, \n",
    "                               index = None)\n",
    "\n",
    "workers = 16\n",
    "max_queue_size = 15\n",
    "\n",
    "\n",
    "hyper_params = {\n",
    "    \"num_layers\" : 2,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 40,\n",
    "    \"learning_rate\" : 0.001,\n",
    "    \"optimizer\" : \"sgd\",\n",
    "    \"num_filters\" : 68,\n",
    "    \"filter_size\" : 42,\n",
    "    \"cnn_activation\" : \"relu\",\n",
    "    \"dense_activation\" : \"relu\",\n",
    "    \"padding\" : \"same\",\n",
    "    \"use_layerwise_dropout_batchnorm\" : True,\n",
    "    \"growth_sequence\" : [1,2, 4],\n",
    "    \"dropout_rate\" : 0.01,\n",
    "    \"l2_r\" : 0.01,\n",
    "    \"l1_r\" : 0.01,\n",
    "    \"first_dense_units\" : 286,\n",
    "    \"output_layer_activation\" : \"sigmoid\"\n",
    "}\n",
    "model = singleModel.run(workers = workers, max_queue_size = max_queue_size, **hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.utils import OrderedEnqueuer\n",
    "\n",
    "def evaluate_generator(model, x_test, y_test, batch_size, label_dict, num_channels, noiseAug, helper):\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    steps = helper.get_steps_per_epoch(x_test, batch_size)\n",
    "    test_enq = OrderedEnqueuer(data_generator(x_test, y_test, batch_size, noiseAug, num_channels = num_channels, is_lstm  = True), use_multiprocessing = False)\n",
    "    test_enq.start(workers = 1, max_queue_size = 15)\n",
    "    test_gen = test_enq.get()\n",
    "    predictions = predict_generator(model, test_gen, steps, label_dict)\n",
    "    predictions = np.rint(predictions)\n",
    "    model.evaluate(x = test_gen, steps = steps)\n",
    "    y_test = np.rint(y_test)\n",
    "    y_test = y_test[0:len(predictions)]\n",
    "    num_classes = len(set(label_dict.values()))\n",
    "    predictions = np.reshape(predictions,(predictions.shape[0]))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0]))\n",
    "    assert predictions.shape == y_test.shape\n",
    "    print(y_test.shape)\n",
    "    print(predictions.shape)\n",
    "    conf = tf.math.confusion_matrix(y_test, predictions, num_classes=num_classes)\n",
    "    class_report = classification_report(y_test, predictions, target_names = helper.handle_non_noise_dict(label_dict))\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(conf)\n",
    "    \n",
    "    print(class_report)\n",
    "    return conf, class_report\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def predict_generator(model, gen, steps, label_dict):\n",
    "    predictions = model.predict(x = gen, steps = steps)\n",
    "    return predictions\n",
    "\n",
    "def round_predictions(predictions):\n",
    "    predictions = np.rint(predictions)\n",
    "    return predictions\n",
    "\n",
    "def evaluate_model(model, x_test, y_test, label_dict, num_channels, noiseAug, helper, plot_confusion_matrix = True):\n",
    "    pp = pprint.PrettyPrinter(indent = 4)\n",
    "    if noiseAug != None:\n",
    "        x_test = noiseAug.batch_augment_noise(x_test, 0, noiseAug.noise_std/10)\n",
    "    x_test[:][:,:num_channels]\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[2], x_test.shape[1]))\n",
    "    model.evaluate(x = x_test, y = y_test)\n",
    "    predictions = model.predict(x_test)\n",
    "    predictions = np.rint(predictions)\n",
    "    y_test = np.rint(y_test)\n",
    "    y_test = y_test[:len(predictions)]\n",
    "    predictions = np.reshape(predictions, (predictions.shape[0]))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0]))\n",
    "    num_classes = len(set(label_dict.values()))\n",
    "    assert predictions.shape == y_test.shape\n",
    "    print(y_test.shape)\n",
    "    print(predictions.shape)\n",
    "    conf = tf.math.confusion_matrix(y_test, predictions, num_classes=num_classes)\n",
    "    class_report = classification_report(y_test, predictions, target_names = helper.handle_non_noise_dict(label_dict))\n",
    "    if plot_confusion_matrix:\n",
    "        helper.plot_confusion_matrix(conf, helper.handle_non_noise_dict(label_dict))\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(conf)\n",
    "    print(class_report)\n",
    "    return conf, class_report, predictions, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, predictions, y_test= evaluate_model(model, x_val, y_val, loadData.label_dict, num_channels, noiseAug, helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.evaluate_generator(model, x_test, y_test, hyper_params[\"batch_size\"], loadData.label_dict, num_channels, noiseAug, plot_confusion_matrix = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-stability",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
